{"id": "task413-cb95090900cd4ac28cbd471a4478078f", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-cb95090900cd4ac28cbd471a4478078f", "input": "You would join the process because you want action and debate .", "output": ["You would challenge the opposition because you worship action and adventure .", "You would join the RSS because you want action and popularity .", "You would join the army because you want action and adventure.", "You would test the franchise because you allow action and adventure ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would join the process because you want action and debate .\nOutput: ", "bloom_target": "You would challenge the opposition because you worship action and adventure .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would join the process because you want action and debate .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-eeac8af11fe54e7586d408bafa66bfe3", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-eeac8af11fe54e7586d408bafa66bfe3", "input": "You would wake up in the morning because you want to dig school .", "output": ["You would wake up in the morning because you want to attend school.", "You would wake up in the den because you want to call school .", "You would wake up in the m because you want to attend school .", "You would wake up in the morning because you use to pick school ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would wake up in the morning because you want to dig school .\nOutput: ", "bloom_target": "You would wake up in the m because you want to attend school .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would wake up in the morning because you want to dig school .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-4b425a2aa5fd4f4aaf2cb6796edd5c4c", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-4b425a2aa5fd4f4aaf2cb6796edd5c4c", "input": "The world is sweeping out of fossil fuels .", "output": ["The world is shooting out of fossil fuels .", "The world is breaking out of fossil fuels .", "The world is evolving out of fossil fuels .", "The world is running out of fossil fuels."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: The world is sweeping out of fossil fuels .\nOutput: ", "bloom_target": "The world is shooting out of fossil fuels .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: The world is sweeping out of fossil fuels .\nOutput:  The", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-eaa14c72077c4d3f9fa2359400a4b1b1", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-eaa14c72077c4d3f9fa2359400a4b1b1", "input": "You would listen to music because you want have a brilliant time .", "output": ["You would listen to music because you want have a certain time .", "You would listen to music because you want have a mindless time .", "You would listen to music because you want have a good time.", "You would introduce to music because you want have a good time ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would listen to music because you want have a brilliant time .\nOutput: ", "bloom_target": "You would introduce to music because you want have a good time .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would listen to music because you want have a brilliant time .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-7c95564706074fbcb17612ff2d7b2f5d", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-7c95564706074fbcb17612ff2d7b2f5d", "input": "You are privileged to bear a tail in movies .", "output": ["You are likely to find a horse in movies.", "You are likely to choose a horse in movies .", "You are fun to channel a student in movies .", "You are likely to approach a horse in movies ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are privileged to bear a tail in movies .\nOutput: ", "bloom_target": "You are fun to channel a student in movies .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are privileged to bear a tail in movies .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-456c25ca0ef6474cb7f709689a9c30d9", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-456c25ca0ef6474cb7f709689a9c30d9", "input": "I cooked my finger in the oven .", "output": ["I inserted my profile in the DB .", "I cooked my food in the oven.", "I cut my foot in the oven .", "I cooked my tongue in the oven ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: I cooked my finger in the oven .\nOutput: ", "bloom_target": "I inserted my profile in the DB .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: I cooked my finger in the oven .\nOutput:  I", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-c0492849f5f84cd09c0db9b382cb0f39", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-c0492849f5f84cd09c0db9b382cb0f39", "input": "You can use a hardware element to have tools .", "output": ["You can walk a hardware store to spot tools .", "You can use a hardware store to buy tools.", "You can share a hardware mod to buy tools .", "You can use a hardware tab to manipulate tools ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can use a hardware element to have tools .\nOutput: ", "bloom_target": "You can walk a hardware store to spot tools .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can use a hardware element to have tools .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-68f8090bd67c40dda5e7b661f756576e", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-68f8090bd67c40dda5e7b661f756576e", "input": "You can use a talk doll to encourage family conversation .", "output": ["You can use a spelling assessment to encourage family conversation .", "You can modify a board game to raise family responsibility .", "You can use a board game to encourage family conversation.", "You can sell a board game to encourage team conversation ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can use a talk doll to encourage family conversation .\nOutput: ", "bloom_target": "You can sell a board game to encourage team conversation .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can use a talk doll to encourage family conversation .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-22a8c32cc23447f3b8b1d09c0ff8828c", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-22a8c32cc23447f3b8b1d09c0ff8828c", "input": "You would lead to a performance because you bought tickets .", "output": ["You would see to a performance because you bought tickets .", "You would advance to a performance because you hated tickets .", "You would go to a performance because you bought tickets.", "You would go to a performance because you bought toys ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would lead to a performance because you bought tickets .\nOutput: ", "bloom_target": "You would go to a performance because you bought tickets.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would lead to a performance because you bought tickets .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-52ac768190ff4a5a8aaf4df470b7738e", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-52ac768190ff4a5a8aaf4df470b7738e", "input": "Sometimes knees , after being injured , can be reconstructed for better efficacy .", "output": ["Sometimes knees , after being injured , can be reconstructed for better storage .", "Sometimes knees , after being injured , can be reconstructed for better productivity .", "Sometimes knees, after being injured, can be reconstructed for better use.", "Sometimes knees , after being injured , can be reconstructed for better distribution ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Sometimes knees , after being injured , can be reconstructed for better efficacy .\nOutput: ", "bloom_target": "Sometimes knees, after being injured, can be reconstructed for better use.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Sometimes knees , after being injured , can be reconstructed for better efficacy .\nOutput:  Sometimes", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-fe812e56b82e4547a1c97711a25caa5a", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-fe812e56b82e4547a1c97711a25caa5a", "input": "It is a good idea to quench your thirst when you are salty .", "output": ["It is a cute idea to quench your thirst when you are thirsty .", "It is a good idea to quench your lust when you are thirsty .", "It is a good idea to quench your thirst when you are thirsty.", "It is a good idea to quench your thirst when you are interested ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: It is a good idea to quench your thirst when you are salty .\nOutput: ", "bloom_target": "It is a cute idea to quench your thirst when you are thirsty .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: It is a good idea to quench your thirst when you are salty .\nOutput:  It", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-9c398ff4bc694706978d8f2f4e236dec", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-9c398ff4bc694706978d8f2f4e236dec", "input": "You can use a chess set to play a strategic interface .", "output": ["You can use a chess set to play a strategic thing .", "You can use a chess set to play a strategic form .", "You can use a chess set to play a strategic game.", "You can use a chess set to play a strategic draw ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can use a chess set to play a strategic interface .\nOutput: ", "bloom_target": "You can use a chess set to play a strategic form .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can use a chess set to play a strategic interface .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-7cb00cd8bb2b496b9e60c94d0d0fcac7", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-7cb00cd8bb2b496b9e60c94d0d0fcac7", "input": "You would pay bill because you have a pocket full of money .", "output": ["You would pay cash because you have a pocket full of money.", "You would pay cash because you have a pocket full of candy .", "You would win cash because you have a camera full of money .", "You would pay cash because you have a pocket full of pot ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would pay bill because you have a pocket full of money .\nOutput: ", "bloom_target": "You would pay cash because you have a pocket full of candy .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would pay bill because you have a pocket full of money .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-68613f889bcd4c09bc272bea4882fb4d", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-68613f889bcd4c09bc272bea4882fb4d", "input": "You can use a cat to hunt mice.", "output": ["You can use a flash to hunt mice .", "You can whip a cat to hunt mice .", "You can use a cat to walk mice .", "You can use a mask to hunt mice ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can use a cat to hunt mice.\nOutput: ", "bloom_target": "You can use a cat to walk mice .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can use a cat to hunt mice.\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-8d4f3449b4d145db8891e9c312d5fe32", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-8d4f3449b4d145db8891e9c312d5fe32", "input": "At the wedding ceremony, you would exchange gifts.", "output": ["At the wedding plan , you would exchange favors .", "At the coin ceremony , you would grab gifts .", "At the coin ceremony , you would own gifts .", "At the graduating ceremony , you would pay gifts ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: At the wedding ceremony, you would exchange gifts.\nOutput: ", "bloom_target": "At the coin ceremony , you would own gifts .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: At the wedding ceremony, you would exchange gifts.\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-3954a104c3034dc7847ce9995fcafa62", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-3954a104c3034dc7847ce9995fcafa62", "input": "You would visit other countries because you want to experience other cultures.", "output": ["You would visit other individuals because you want to experience other cultures .", "You would visit other streets because you fear to overcome other cultures .", "You would visit other countries because you want to experience other talents .", "You would visit other countries because you want to experience other fields ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would visit other countries because you want to experience other cultures.\nOutput: ", "bloom_target": "You would visit other streets because you fear to overcome other cultures .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would visit other countries because you want to experience other cultures.\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-5a171f3dd2f7400e9ad57e2d72a04d75", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-5a171f3dd2f7400e9ad57e2d72a04d75", "input": "You are likely to generate a key model in a hardware way .", "output": ["You are likely to find a whole chain in a hardware store .", "You are likely to find a key chain in a hardware store.", "You are likely to work a key delay in a hardware situation .", "You are likely to find a decent chain in a hardware store ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to generate a key model in a hardware way .\nOutput: ", "bloom_target": "You are likely to find a key chain in a hardware store.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to generate a key model in a hardware way .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-b29bb48965cd4a99a1a0e62da0ff2cc9", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-b29bb48965cd4a99a1a0e62da0ff2cc9", "input": "A snowball is used to defend oneself against an alert .", "output": ["A chain is used to store oneself against an attack .", "A projectile is used to ensure oneself against an attack .", "A weapon is used to defend oneself against an attack.", "A club is used to defend oneself against an A ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: A snowball is used to defend oneself against an alert .\nOutput: ", "bloom_target": "A chain is used to store oneself against an attack .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: A snowball is used to defend oneself against an alert .\nOutput:  A", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-7c2b02aa350a4916a51bafc5f9d32d76", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-7c2b02aa350a4916a51bafc5f9d32d76", "input": "You are likely to move a doll in a girl 's bedroom .", "output": ["You are mistaken to find a python in a girl 's bedroom .", "You are likely to avoid a doll in a girl 's bedroom .", "You are disgusted to find a doll in a girl 's son .", "You are likely to find a doll in a girl's bedroom."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to move a doll in a girl 's bedroom .\nOutput: ", "bloom_target": "You are likely to find a doll in a girl's bedroom.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to move a doll in a girl 's bedroom .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-cb895106c9cc4ebc9087cf11f572bd0c", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-cb895106c9cc4ebc9087cf11f572bd0c", "input": "You are likely to say a container drum in a bow .", "output": ["You are likely to find a container drum in a factory.", "You are likely to find a container brand in a factory .", "You are likely to eat a container drum in a laboratory .", "You are liable to crash a container boat in a factory ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to say a container drum in a bow .\nOutput: ", "bloom_target": "You are likely to find a container drum in a factory.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to say a container drum in a bow .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-b64cb737230e43509313e71cd928ab78", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-b64cb737230e43509313e71cd928ab78", "input": "You can use a boat to cross water.", "output": ["You can use a boat to attract water .", "You can begin a family to cross crossing .", "You can use a monkey to cross water .", "You can use a hoop to cross water ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can use a boat to cross water.\nOutput: ", "bloom_target": "You can use a hoop to cross water .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can use a boat to cross water.\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-51be7369b65b4da8b8873f39cc2fe080", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-51be7369b65b4da8b8873f39cc2fe080", "input": "Some oven is made of glass .", "output": ["Some bottles is conceived of glass .", "Some bottles is made of glass.", "Some property is makes of glass .", "Some skeleton is handmade of glass ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Some oven is made of glass .\nOutput: ", "bloom_target": "Some bottles is conceived of glass .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Some oven is made of glass .\nOutput:  Some", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-36b0f8a750e4462891c5c51af707f73e", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-36b0f8a750e4462891c5c51af707f73e", "input": "You would read a draft because you want to gain information .", "output": ["You would leak a bomb because you want to deflect information .", "You would hear a book because you want to aggregate information .", "You would read a book because you choose to appreciate information .", "You would read a book because you want to gain information."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would read a draft because you want to gain information .\nOutput: ", "bloom_target": "You would leak a bomb because you want to deflect information .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would read a draft because you want to gain information .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-1aac5b7d01414a139390fe7be265c3fd", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-1aac5b7d01414a139390fe7be265c3fd", "input": "A flight bag should be small enough to fit under an airplane seat.", "output": ["A sling bag should be small enough to fit under an toilet seat .", "A gift bag should be small enough to fit under an airplane seat .", "A traveling bag should be small enough to fit under an airplane seat .", "A pillow bag should be small enough to fit under an airplane seat ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: A flight bag should be small enough to fit under an airplane seat.\nOutput: ", "bloom_target": "A traveling bag should be small enough to fit under an airplane seat .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: A flight bag should be small enough to fit under an airplane seat.\nOutput:  A", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-dc9e1c4fb6704c569a329f900b25f498", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-dc9e1c4fb6704c569a329f900b25f498", "input": "Organized organization members can recreate a sports game .", "output": ["Organized FW members can contribute a sports game .", "Organized pool members can invite a sports game .", "Organized uild members can compete a sports game .", "Organized crime members can fix a sports game."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Organized organization members can recreate a sports game .\nOutput: ", "bloom_target": "Organized crime members can fix a sports game.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Organized organization members can recreate a sports game .\nOutput:  Organ", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-beeaffdeeeb14d7ebde9eb5311bff075", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-beeaffdeeeb14d7ebde9eb5311bff075", "input": "You can buy corn at a farmer's market.", "output": ["You can buy corn at a farmer 's plant .", "You can package corn at a printer 's work .", "You can fix corn at a worker 's grill .", "You can buy corn at a farmer 's sal ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can buy corn at a farmer's market.\nOutput: ", "bloom_target": "You can buy corn at a farmer 's plant .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can buy corn at a farmer's market.\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-adc7f14ab4b24c9e93b7e38c7a9833a5", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-adc7f14ab4b24c9e93b7e38c7a9833a5", "input": "Your closest followers can protect your honour .", "output": ["Your closest friends can break your heart.", "Your closest aides can break your grasp .", "Your closest coworkers can break your net .", "Your closest supporters can rebuild your heart ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Your closest followers can protect your honour .\nOutput: ", "bloom_target": "Your closest friends can break your heart.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Your closest followers can protect your honour .\nOutput:  Your", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-4c046f7b660b4896a814c0a24d7df389", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-4c046f7b660b4896a814c0a24d7df389", "input": "If you work to dominate ice combat then you should enjoy it .", "output": ["If you want to eat ice cream then you should enjoy it.", "If you want to clean ice cream then you should enjoy it .", "If you want to eat ice cream then you should split it .", "If you want to eat ice cream then you should follow it ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you work to dominate ice combat then you should enjoy it .\nOutput: ", "bloom_target": "If you want to eat ice cream then you should enjoy it.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you work to dominate ice combat then you should enjoy it .\nOutput:  If", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-b4acfe745d714154b79cd71ec4a44b23", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-b4acfe745d714154b79cd71ec4a44b23", "input": "You are helpful to find a time table in a house .", "output": ["You are likely to find a kitchen table in a house.", "You are delighted to find a hunting table in a house .", "You are likely to find a kitchen cook in a house .", "You are likely to find a kitchen element in a house ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are helpful to find a time table in a house .\nOutput: ", "bloom_target": "You are delighted to find a hunting table in a house .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are helpful to find a time table in a house .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-c4b007e989024c179682c6da4c2e1641", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-c4b007e989024c179682c6da4c2e1641", "input": "If you want to clean the shed then you should Get help .", "output": ["If you dig to clean the house then you should hire help .", "If you qualify to rescue the house then you should dispatch help .", "If you want to clean the house then you should hire help.", "If you arrange to advertise the house then you should hire money ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you want to clean the shed then you should Get help .\nOutput: ", "bloom_target": "If you qualify to rescue the house then you should dispatch help .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you want to clean the shed then you should Get help .\nOutput:  If", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-512111a0090949ea85425530694a8434", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-512111a0090949ea85425530694a8434", "input": "A playlist can help you find your way .", "output": ["A map can help you find your way.", "A map can cover you find your way .", "A specialist can help you find your way .", "A map can direct you find your way ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: A playlist can help you find your way .\nOutput: ", "bloom_target": "A map can help you find your way.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: A playlist can help you find your way .\nOutput:  A", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-187321d3ce2d4ddd9567d510207b524e", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-187321d3ce2d4ddd9567d510207b524e", "input": "running in the rain is for getting somewhere when you do n't have a car .", "output": ["Walking in the rain is for getting somewhere when you don't have a car.", "Walking in the loop is for getting somewhere when you do n't have a car .", "Walking in the week is for getting somewhere when you do n't have a car .", "Walking in the state is for getting somewhere when you do n't have a car ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: running in the rain is for getting somewhere when you do n't have a car .\nOutput: ", "bloom_target": "Walking in the week is for getting somewhere when you do n't have a car .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: running in the rain is for getting somewhere when you do n't have a car .\nOutput:  Running", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-36cf5632b6614e2c9fdc74d664e70f19", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-36cf5632b6614e2c9fdc74d664e70f19", "input": "You can use a line to handle geometric figures .", "output": ["You can use a line to make geometric floats .", "You can use a device to make geometric laws .", "You can use a line to make geometric applications .", "You can use a line to make geometric figures."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can use a line to handle geometric figures .\nOutput: ", "bloom_target": "You can use a line to make geometric figures.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can use a line to handle geometric figures .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-6bad6aa0d31c4b66b309a5e616915960", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-6bad6aa0d31c4b66b309a5e616915960", "input": "You would junk a computer because you want to target email .", "output": ["You would use a computer because you want to send email.", "You would use a attachment because you want to send email .", "You would welcome a computer because you want to join email .", "You would raise a fax because you want to send email ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would junk a computer because you want to target email .\nOutput: ", "bloom_target": "You would raise a fax because you want to send email .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would junk a computer because you want to target email .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-e9effce04aec4fd5a5baf97b53716d75", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-e9effce04aec4fd5a5baf97b53716d75", "input": "If you want to go on the stage then you should ask the director.", "output": ["If you intention to go on the stage then you should ask the director .", "If you want to dub on the fight then you should ask the director .", "If you say to go on the holiday then you should accept the director .", "If you want to quit on the attempt then you should ask the director ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you want to go on the stage then you should ask the director.\nOutput: ", "bloom_target": "If you say to go on the holiday then you should accept the director .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you want to go on the stage then you should ask the director.\nOutput:  If", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-bdaa0b781ddc4c7cada0cc39fae95a37", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-bdaa0b781ddc4c7cada0cc39fae95a37", "input": "Every industry must be clean to ensure credit .", "output": ["Every server must be clean to ensure production .", "Every bed must be clean to ensure professionalism .", "Every house must be clean to ensure safety.", "Every house be be clean to ensure dignity ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Every industry must be clean to ensure credit .\nOutput: ", "bloom_target": "Every house be be clean to ensure dignity .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Every industry must be clean to ensure credit .\nOutput:  Every", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-4a3aabb581ae4957a017e29132770615", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-4a3aabb581ae4957a017e29132770615", "input": "A sounding alarm would make you want to stand up .", "output": ["A ch alarm would make you want to stand up .", "A break alarm would make you want to stand up .", "A water alarm would make you want to stand up .", "A fire alarm would make you want to stand up."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: A sounding alarm would make you want to stand up .\nOutput: ", "bloom_target": "A ch alarm would make you want to stand up .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: A sounding alarm would make you want to stand up .\nOutput:  A", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-ce78be82a71a457f8eeca48d13583d44", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-ce78be82a71a457f8eeca48d13583d44", "input": "Weapons get into a bad person sometimes .", "output": ["People get into a dreadful mood sometimes .", "People get into a bad mood sometimes.", "People get into a bad mood anymore .", "People get into a bad mood maybe ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Weapons get into a bad person sometimes .\nOutput: ", "bloom_target": "People get into a bad mood anymore .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Weapons get into a bad person sometimes .\nOutput:  Weapons", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-0c9cb3c675e444f4ab18d02e25a40af5", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-0c9cb3c675e444f4ab18d02e25a40af5", "input": "A bottle can be made of plastic.", "output": ["A bottle can be conceived of instead .", "A file can be assembly of list .", "A bottle can be made of Plastic .", "A poem can be approved of better ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: A bottle can be made of plastic.\nOutput: ", "bloom_target": "A bottle can be made of Plastic .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: A bottle can be made of plastic.\nOutput:  A", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-6872f40cf520400080162ede257ba9ac", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-6872f40cf520400080162ede257ba9ac", "input": "Pain can conceal a cold .", "output": ["People can catch a cold.", "Spring can catch a cold .", "things can catch a cold .", "Work can confuse a cold ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Pain can conceal a cold .\nOutput: ", "bloom_target": "Work can confuse a cold .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Pain can conceal a cold .\nOutput:  Pain", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-a5cd6914b68848358dad17ce1809dcf4", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-a5cd6914b68848358dad17ce1809dcf4", "input": "You use it to put etchup on your food .", "output": ["You use it to put iodine on your food .", "You paste it to put type on your folder .", "You use it to put cooking on your food .", "You use it to put salt on your food."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You use it to put etchup on your food .\nOutput: ", "bloom_target": "You paste it to put type on your folder .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You use it to put etchup on your food .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-dc8ba4b478d1421490375ee58bed5f2b", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-dc8ba4b478d1421490375ee58bed5f2b", "input": "You are likely to find a poem in a poetry book.", "output": ["You are likely to approach a poem in a poetry book .", "You are likely to recall a poem in a poetry book .", "You are likely to find a snippet in a poetry book .", "You are likely to find a role in a poetry book ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to find a poem in a poetry book.\nOutput: ", "bloom_target": "You are likely to find a snippet in a poetry book .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to find a poem in a poetry book.\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-27ec8347758d4d2fbd1415c6f888eba3", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-27ec8347758d4d2fbd1415c6f888eba3", "input": "If you want to talk then you should flap your lips and make attempt .", "output": ["If you want to bleed then you should flap your tires and make noise .", "If you excuse to talk then you should flap your lips and beat noise .", "If you see to talk then you should flap your lips and report noise .", "If you want to talk then you should flap your lips and make noise."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you want to talk then you should flap your lips and make attempt .\nOutput: ", "bloom_target": "If you want to bleed then you should flap your tires and make noise .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you want to talk then you should flap your lips and make attempt .\nOutput:  If", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-301c5ff30dcc41a89fd5921257e1a32f", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-301c5ff30dcc41a89fd5921257e1a32f", "input": "You are likely to find a jack around in hole .", "output": ["You are likely to sprinkle a snake around in hole .", "You are OK to loop a snake around in home .", "You are likely to find a snake around in hole.", "You are likely to awaken a snake around in hole ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to find a jack around in hole .\nOutput: ", "bloom_target": "You are likely to awaken a snake around in hole .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to find a jack around in hole .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-9fffa84cc11848c68ae5be2883881341", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-9fffa84cc11848c68ae5be2883881341", "input": "You are likely to find a pair in a palm tree .", "output": ["You are distressed to find a monkey in a sleeping carriage .", "You are likely to find a monkey in a palm tree.", "You are likely to confuse a monkey in a palm tree .", "You are likely to reach a monkey in a palm tree ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to find a pair in a palm tree .\nOutput: ", "bloom_target": "You are likely to reach a monkey in a palm tree .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to find a pair in a palm tree .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-86d5eabdc6bf4f77a62a8497397bf07d", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-86d5eabdc6bf4f77a62a8497397bf07d", "input": "You can use a Bitcoin store to purchase merchandise .", "output": ["You can use a card store to purchase merchandise .", "You can integrate a food store to share merchandise .", "You can customize a food store to feed merchandise .", "You can use a food store to purchase merchandise."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can use a Bitcoin store to purchase merchandise .\nOutput: ", "bloom_target": "You can use a food store to purchase merchandise.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can use a Bitcoin store to purchase merchandise .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-e02ea12103e44b0493a6bf75b893e3a3", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-e02ea12103e44b0493a6bf75b893e3a3", "input": "You can rob a place to eat to have lunch .", "output": ["You can collect a place to eat to have lunch .", "You can snag a place to eat to have lunch .", "You can use a place to eat to have lunch.", "You can expect a place to eat to have lunch ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can rob a place to eat to have lunch .\nOutput: ", "bloom_target": "You can expect a place to eat to have lunch .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can rob a place to eat to have lunch .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-bce00a4d60c6439fb9b62f5cdacd1c2d", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-bce00a4d60c6439fb9b62f5cdacd1c2d", "input": "You would go on the assignment because you want to collect information .", "output": ["You would go on the internet because you kill to collect information .", "You would go on the internet because you decide to collect information .", "You would give on the internet because you feel to collect gold .", "You would go on the internet because you want to collect information."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would go on the assignment because you want to collect information .\nOutput: ", "bloom_target": "You would give on the internet because you feel to collect gold .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would go on the assignment because you want to collect information .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-365cdd05270f4b4da5af47b0dfc12cf7", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-365cdd05270f4b4da5af47b0dfc12cf7", "input": "You are safe to find a nest in a map website .", "output": ["You are motivated to find a recommendation in a map website .", "You are responsible to find a world in a map website .", "You are likely to find a map in a map website.", "You are likely to explore a map in a source website ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are safe to find a nest in a map website .\nOutput: ", "bloom_target": "You are motivated to find a recommendation in a map website .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are safe to find a nest in a map website .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-66dc6aa47b244f06ae63d93b5281a6b1", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-66dc6aa47b244f06ae63d93b5281a6b1", "input": "You are determined to rub a hat in a chalk box .", "output": ["You are OK to sniff a hat in a broom box .", "You are likely to find a hat in a hat box.", "You are likely to order a hat in a hat box .", "You are likely to stay a hat in a hat story ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are determined to rub a hat in a chalk box .\nOutput: ", "bloom_target": "You are likely to find a hat in a hat box.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are determined to rub a hat in a chalk box .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-c29203b69e5a48dabd89e3d1f2c588a0", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-c29203b69e5a48dabd89e3d1f2c588a0", "input": "A ramp is used for reaching high places .", "output": ["A bird is used for migrating high places .", "A method is used for reaching high places .", "A sail is used for reaching high places .", "A ladder is used for reaching high places."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: A ramp is used for reaching high places .\nOutput: ", "bloom_target": "A bird is used for migrating high places .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: A ramp is used for reaching high places .\nOutput:  A", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-b8090a943ed94adc916797a09fdbc924", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-b8090a943ed94adc916797a09fdbc924", "input": "You would remember a phone number because you dare to call it ofter .", "output": ["You would remember a phone friend because you want to call it ofter .", "You would remember a phone number because you want to call it ofter.", "You would remember a phone hotline because you want to call it ofter .", "You would remember a phone booth because you want to call it ofter ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would remember a phone number because you dare to call it ofter .\nOutput: ", "bloom_target": "You would remember a phone booth because you want to call it ofter .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would remember a phone number because you dare to call it ofter .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-69725880281641af85dd5b73b3c768f8", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-69725880281641af85dd5b73b3c768f8", "input": "Water is for quenching steam .", "output": ["Training is for quenching interest .", "Water is for quenching interest .", "Water is for quenching thirst.", "stress is for quenching conflict ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Water is for quenching steam .\nOutput: ", "bloom_target": "Water is for quenching thirst.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Water is for quenching steam .\nOutput:  Water", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-78f3bb7166d14b9ca43f584f40323110", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-78f3bb7166d14b9ca43f584f40323110", "input": "Those suffering from persistent nightmares should refuse help from a therapist .", "output": ["Those suffering from persistent nightmares should seek help from a therapist.", "Those suffering from persistent nightmares should suggest help from a therapist .", "Those suffering from persistent overdoses should invoke help from a therapist .", "Those suffering from persistent comments should bring help from a therapist ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Those suffering from persistent nightmares should refuse help from a therapist .\nOutput: ", "bloom_target": "Those suffering from persistent nightmares should suggest help from a therapist .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Those suffering from persistent nightmares should refuse help from a therapist .\nOutput:  Those", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-044921a5df6643d1976acdc7c013d60e", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-044921a5df6643d1976acdc7c013d60e", "input": "You are likely to find a mutant in a snakes bone .", "output": ["You are likely to find a mouse in a snakes belly.", "You are likely to find a pit in a snakes domain .", "You are likely to bat a trout in a snakes belly .", "You are likely to find a ball in a snakes group ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to find a mutant in a snakes bone .\nOutput: ", "bloom_target": "You are likely to bat a trout in a snakes belly .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to find a mutant in a snakes bone .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-59e156e44e1548f1acee068cfd1f6e5d", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-59e156e44e1548f1acee068cfd1f6e5d", "input": "Wolves , like all newcomers , can see with their eyes .", "output": ["Wolves , like all lions , can see with their eyes .", "Wolves , like all experts , can see with their eyes .", "Wolves, like all mammals, can see with their eyes.", "Wolves , like all believers , can see with their eyes ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Wolves , like all newcomers , can see with their eyes .\nOutput: ", "bloom_target": "Wolves, like all mammals, can see with their eyes.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Wolves , like all newcomers , can see with their eyes .\nOutput:  Wol", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-6884e59dc23a4d56a2d99aeb38eec541", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-6884e59dc23a4d56a2d99aeb38eec541", "input": "If you want to report on the printer then you should use a q .", "output": ["If you want to try on the printer then you should use a stick .", "If you want to print on the printer then you should use a computer.", "If you want to tape on the printer then you should use a computer .", "If you want to layer on the printer then you should use a virtual ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you want to report on the printer then you should use a q .\nOutput: ", "bloom_target": "If you want to layer on the printer then you should use a virtual .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you want to report on the printer then you should use a q .\nOutput:  If", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-f42cb13006c64296bdcab04f947bc986", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-f42cb13006c64296bdcab04f947bc986", "input": "You are likely to care a mill around in warm waters .", "output": ["You are likely to snap a towel around in warm waters .", "You are delusional to grow a shark around in warm waters .", "You are likely to save a neck around in warm waters .", "You are likely to find a shark around in warm waters."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to care a mill around in warm waters .\nOutput: ", "bloom_target": "You are likely to snap a towel around in warm waters .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to care a mill around in warm waters .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-35250889657149d8926b40c63c9ca040", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-35250889657149d8926b40c63c9ca040", "input": "Making bread is for break with meals .", "output": ["Making bread is for light with meals .", "Making bread is for heat with meals .", "Making suggestion is for example with meals .", "Making bread is for use with meals."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Making bread is for break with meals .\nOutput: ", "bloom_target": "Making bread is for use with meals.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Making bread is for break with meals .\nOutput:  Making", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-16fdcebbe7964ebfb394358be0eb3ce4", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-16fdcebbe7964ebfb394358be0eb3ce4", "input": "The effect of flying in an airplane is missing quickly .", "output": ["The air of flying in an airplane is collapsing quickly .", "The effect of flying in an airplane is building quickly .", "The effect of flying in an airplane is traveling quickly.", "The annoyance of flying in an airplane is expanding quickly ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: The effect of flying in an airplane is missing quickly .\nOutput: ", "bloom_target": "The annoyance of flying in an airplane is expanding quickly .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: The effect of flying in an airplane is missing quickly .\nOutput:  The", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-d3a7aa9122704e6899384a0b84c8ad39", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-d3a7aa9122704e6899384a0b84c8ad39", "input": "You would request a movie because you read the book .", "output": ["You would see a movie because you read the book.", "You would solve a jam because you read the book .", "You would remain a pro because you read the book .", "You would despise a movie because you read the book ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would request a movie because you read the book .\nOutput: ", "bloom_target": "You would remain a pro because you read the book .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would request a movie because you read the book .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-92450e93861c45919341656642c8d103", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-92450e93861c45919341656642c8d103", "input": "You would read the board because you want to trade opinions about the news .", "output": ["You would split the newspaper because you want to read opinions about the thing .", "You would read the newspaper because you want to read opinions about the news.", "You would answer the newspaper because you want to read opinions about the writing .", "You would read the majority because you want to base opinions about the news ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would read the board because you want to trade opinions about the news .\nOutput: ", "bloom_target": "You would split the newspaper because you want to read opinions about the thing .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would read the board because you want to trade opinions about the news .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-bf399b55398d4db78105db0534b9db94", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-bf399b55398d4db78105db0534b9db94", "input": "You would write a story because you have a spontaneous idea .", "output": ["You would write a story because you have a detailed idea .", "You would write a story because you have a weak idea .", "You would write a story because you have a sweet idea .", "You would write a story because you have a good idea."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would write a story because you have a spontaneous idea .\nOutput: ", "bloom_target": "You would write a story because you have a weak idea .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would write a story because you have a spontaneous idea .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-dbcb27d9751f48b4a6d2c7c72a204734", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-dbcb27d9751f48b4a6d2c7c72a204734", "input": "You are likely to distribute a plastic in cars .", "output": ["You are likely to find a plastic in cars.", "You are likely to perceive a plastic in cars .", "You are likely to shoot a plastic in cars .", "You are likely to abuse a plastic in cars ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to distribute a plastic in cars .\nOutput: ", "bloom_target": "You are likely to find a plastic in cars.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to distribute a plastic in cars .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-3a0f834aaf6a40869c31ea4d466ab877", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-3a0f834aaf6a40869c31ea4d466ab877", "input": "You would take in a film because life is boring at the moment.", "output": ["You would take in a film because life is small at the moment .", "You would take in a client because change is high at the moment .", "You would take in a film because life is boring at the store .", "You would take in a turtle because black is boring at the sun ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would take in a film because life is boring at the moment.\nOutput: ", "bloom_target": "You would take in a film because life is boring at the store .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would take in a film because life is boring at the moment.\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-25c0acb2aad14167aa10e642b7a2c0d0", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-25c0acb2aad14167aa10e642b7a2c0d0", "input": "If you want to slide around a lake then you should find a path .", "output": ["If you want to route around a lake then you should find a path .", "If you want to walk around a lake then you should find a path.", "If you want to eat around a lake then you should find a shop .", "If you want to paint around a lake then you should find a path ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you want to slide around a lake then you should find a path .\nOutput: ", "bloom_target": "If you want to route around a lake then you should find a path .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you want to slide around a lake then you should find a path .\nOutput:  If", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-b71633f752224064a1f0de3fa8d1b5ac", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-b71633f752224064a1f0de3fa8d1b5ac", "input": "You are likely to signal a bus station in the mall .", "output": ["You are likely to lease a bus station in the plan .", "You are likely to find a bus station in the city.", "You are correct to find a hash lock in the area .", "You are likely to accommodate a bus station in the past ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to signal a bus station in the mall .\nOutput: ", "bloom_target": "You are correct to find a hash lock in the area .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to signal a bus station in the mall .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-8a32916174894e898010158216f13ad2", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-8a32916174894e898010158216f13ad2", "input": "You can use a captain to command a ship.", "output": ["You can use a wizard to flip a note .", "You can dispatch a tracker to command a ship .", "You can solder a capacitor to command a ship .", "You can used a subclass to command a ship ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can use a captain to command a ship.\nOutput: ", "bloom_target": "You can dispatch a tracker to command a ship .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can use a captain to command a ship.\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-50a0232c82d54d8bba039ff4bcd6eecf", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-50a0232c82d54d8bba039ff4bcd6eecf", "input": "You are likely to unlock shoes in front door .", "output": ["You are likely to saw shoes in front door .", "You are likely to fold shoes in front door .", "You are likely to find shoes in front door.", "You are likely to load shoes in front door ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to unlock shoes in front door .\nOutput: ", "bloom_target": "You are likely to load shoes in front door .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to unlock shoes in front door .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-30934c5556e549728a1aaa8d99fcc909", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-30934c5556e549728a1aaa8d99fcc909", "input": "You would buy presents for others because you want to please them.", "output": ["You would buy pills for others because you manage to multiply them .", "You would buy presents for others because you use to please them .", "You would mail presents for others because you want to please them .", "You would buy motorcycles for others because you want to chase them ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would buy presents for others because you want to please them.\nOutput: ", "bloom_target": "You would buy pills for others because you manage to multiply them .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would buy presents for others because you want to please them.\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-ae3d9ff6b6c4480ba5206bf2cb823e29", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-ae3d9ff6b6c4480ba5206bf2cb823e29", "input": "You are likely to call a name list in future .", "output": ["You are likely to find a wine list in restaurant.", "You are likely to refuse a wine cut in restaurant .", "You are likely to pull a wine press in restaurant .", "You are likely to meet a director list in summer ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to call a name list in future .\nOutput: ", "bloom_target": "You are likely to pull a wine press in restaurant .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to call a name list in future .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-2ca7a268015049089e121e4714b34042", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-2ca7a268015049089e121e4714b34042", "input": "The future is a planet to dance .", "output": ["The ocean is a place to dance .", "The yard is a place to dance .", "The life is a Place to dance .", "The future is a place to dance."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: The future is a planet to dance .\nOutput: ", "bloom_target": "The future is a place to dance.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: The future is a planet to dance .\nOutput:  The", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-1ce23358365f4936842eede23cecfcd1", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-1ce23358365f4936842eede23cecfcd1", "input": "The president can govern the nation.", "output": ["The president can form the force .", "The president can flip the cash .", "The president can negotiate the bailout .", "The president can govern the department ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: The president can govern the nation.\nOutput: ", "bloom_target": "The president can flip the cash .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: The president can govern the nation.\nOutput:  The", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-e11bb92300304f4c8730993009d6d08f", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-e11bb92300304f4c8730993009d6d08f", "input": "You can stop the car outside the boat shop .", "output": ["You can stop the car outside the pet shop.", "You can stop the looting outside the phone shop .", "You can fit the buggy outside the pet shop .", "You can stop the car outside the med shop ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can stop the car outside the boat shop .\nOutput: ", "bloom_target": "You can stop the car outside the med shop .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can stop the car outside the boat shop .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-89b978f8ac754dc7b27926162b346784", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-89b978f8ac754dc7b27926162b346784", "input": "You can watch zombies on tv or at the park .", "output": ["You can obtain games on tv or at the park .", "You can end games on tv or at the blog .", "You can throw games on tv or at the park .", "You can watch games on tv or at the park."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can watch zombies on tv or at the park .\nOutput: ", "bloom_target": "You can end games on tv or at the blog .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can watch zombies on tv or at the park .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-ccb0ec50987d40d8a6854541b9d52d3a", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-ccb0ec50987d40d8a6854541b9d52d3a", "input": "You can publish a launch token to set off a replication .", "output": ["You can use a launch platform to set off a laser .", "You can use a computing platform to set off a state .", "You can use a launch platform to set off a rocket.", "You can use a burning platform to set off a rocket ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can publish a launch token to set off a replication .\nOutput: ", "bloom_target": "You can use a computing platform to set off a state .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can publish a launch token to set off a replication .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-d440c5868bec4340b286ae81fdb5f1ae", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-d440c5868bec4340b286ae81fdb5f1ae", "input": "If you want to impanel a jury then you should become a lawyer.", "output": ["If you win to impanel a jury then you should become a king .", "If you learn to impanel a jury then you should become a player .", "If you venture to impanel a jury then you should gather a resource .", "If you require to impanel a jury then you should kill a respondent ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you want to impanel a jury then you should become a lawyer.\nOutput: ", "bloom_target": "If you require to impanel a jury then you should kill a respondent .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you want to impanel a jury then you should become a lawyer.\nOutput:  If", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-fce884ed434846ab86f0f93927d86f32", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-fce884ed434846ab86f0f93927d86f32", "input": "If you strive to wait tables then you should get hired at a restaurant .", "output": ["If you crave to wait tables then you should get hired at a restaurant .", "If you want to wait others then you should get hired at a restaurant .", "If you want to wait tables then you should get hired at a restaurant.", "If you adore to wait tables then you should get hired at a restaurant ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you strive to wait tables then you should get hired at a restaurant .\nOutput: ", "bloom_target": "If you adore to wait tables then you should get hired at a restaurant .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you strive to wait tables then you should get hired at a restaurant .\nOutput:  If", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-8ddd7eec9baf4f4a92af83f216aa764b", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-8ddd7eec9baf4f4a92af83f216aa764b", "input": "You would roam outside for an evening because you want to see the turtles .", "output": ["You would scream outside for an evening because you want to see the stars .", "You would go outside for an evening because you want to see the stars.", "You would cozy outside for an evening because you want to see the stars .", "You would sweat outside for an evening because you want to see the stars ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would roam outside for an evening because you want to see the turtles .\nOutput: ", "bloom_target": "You would sweat outside for an evening because you want to see the stars .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would roam outside for an evening because you want to see the turtles .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-d4defb0fcbc649b08379f41a6c73bbe2", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-d4defb0fcbc649b08379f41a6c73bbe2", "input": "You can use a cloth to hold a stick .", "output": ["You can use a sidebar to maintain a slideshow .", "You can use a finger to hold a flower .", "You can use a finger to hold a stick.", "You can use a felt to hold a stick ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can use a cloth to hold a stick .\nOutput: ", "bloom_target": "You can use a sidebar to maintain a slideshow .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can use a cloth to hold a stick .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-8dd8a4b4c44240229e1e290f8e901dea", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-8dd8a4b4c44240229e1e290f8e901dea", "input": "You are likely to find a snake in the desert.", "output": ["You are difficult to find a snake in the colony .", "You are likely to slip a snake in the milk .", "You are likely to witness a snake in the wind .", "You are mistaken to fight a snake in the desert ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to find a snake in the desert.\nOutput: ", "bloom_target": "You are difficult to find a snake in the colony .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to find a snake in the desert.\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-0858af84a57b4b81b939e82a236aae90", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-0858af84a57b4b81b939e82a236aae90", "input": "American hair would make you want to go to purchase a haircut .", "output": ["brown hair would make you want to go to wear a haircut .", "Long hair would make you want to go to get a haircut.", "Long hair would make you urge to go to get a kid .", "Random hair would make you want to go to require a haircut ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: American hair would make you want to go to purchase a haircut .\nOutput: ", "bloom_target": "Long hair would make you want to go to get a haircut.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: American hair would make you want to go to purchase a haircut .\nOutput:  American", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-4e4525734fde478a8e63831d6cacab93", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-4e4525734fde478a8e63831d6cacab93", "input": "People can resolve their fate .", "output": ["People can decide their fate.", "Events can continue their fate .", "women can assert their assertion .", "People can protest their fate ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: People can resolve their fate .\nOutput: ", "bloom_target": "People can protest their fate .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: People can resolve their fate .\nOutput:  People", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-eb258ec9855149f7805a33d8d7500628", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-eb258ec9855149f7805a33d8d7500628", "input": "If you want to look a result then you should turn a cookbook .", "output": ["If you want to devise a recipe then you should raid a cookbook .", "If you want to handle a horse then you should enter a cookbook .", "If you want to invent a drug then you should examine a cookbook .", "If you want to follow a recipe then you should read a cookbook."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you want to look a result then you should turn a cookbook .\nOutput: ", "bloom_target": "If you want to handle a horse then you should enter a cookbook .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you want to look a result then you should turn a cookbook .\nOutput:  If", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-2f498f1ef99c4c76b60f5a5915d88458", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-2f498f1ef99c4c76b60f5a5915d88458", "input": "A home is a slave to sunlight outside .", "output": ["A tent is a place to sleep outside.", "A tent is a trailer to block outside .", "A flag is a place to measure outside .", "A trailer is a place to sleep freely ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: A home is a slave to sunlight outside .\nOutput: ", "bloom_target": "A tent is a place to sleep outside.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: A home is a slave to sunlight outside .\nOutput:  A", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-8b99ed1a66644999b933b4af6e6004c3", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-8b99ed1a66644999b933b4af6e6004c3", "input": "You are likely to find an arm in a hangar .", "output": ["You are likely to send an explanation in a hangar .", "You are likely to trust an airplane in a hangar .", "You are glad to snag an eagle in a hangar .", "You are likely to find an airplane in a hangar."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to find an arm in a hangar .\nOutput: ", "bloom_target": "You are glad to snag an eagle in a hangar .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to find an arm in a hangar .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-adc6b4b7cc5141c382695e43f8b05c12", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-adc6b4b7cc5141c382695e43f8b05c12", "input": "If you Want to add up numbers then you should float a sequence .", "output": ["If you want to tie up numbers then you should remember a system .", "If you want to start up numbers then you should learn a color .", "If you strive to add up numbers then you should lift a cross .", "If you want to add up numbers then you should make a column."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you Want to add up numbers then you should float a sequence .\nOutput: ", "bloom_target": "If you strive to add up numbers then you should lift a cross .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you Want to add up numbers then you should float a sequence .\nOutput:  If", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-6599d119d6a644b792fa1652cadbe4fc", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-6599d119d6a644b792fa1652cadbe4fc", "input": "You would play a house with your birds because it aids boredom .", "output": ["You would limit a game with your friends because it prevents confrontation .", "You would game a game with your friends because it prevents bribery .", "You would play a game with your friends because it prevents boredom.", "You would sign a game with your friends because it prevents defeat ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would play a house with your birds because it aids boredom .\nOutput: ", "bloom_target": "You would play a game with your friends because it prevents boredom.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would play a house with your birds because it aids boredom .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-79bb43e863354264b7daf06b18154c07", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-79bb43e863354264b7daf06b18154c07", "input": "Inside ishes are grown in pots .", "output": ["Inside flowers are grown in pots.", "Asian flowers are grown in pots .", "Cool flowers are grown in pots .", "External flowers are grown in pots ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Inside ishes are grown in pots .\nOutput: ", "bloom_target": "External flowers are grown in pots .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Inside ishes are grown in pots .\nOutput:  Inside", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-d6759e81f6a649e3a7fd1a150a280ba9", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-d6759e81f6a649e3a7fd1a150a280ba9", "input": "The effect of cleaning clothing is to remove stains.", "output": ["The price of cleaning litter is to remove stains .", "The effect of cleaning wood is to remove acids .", "The course of cleaning software is to remove lines .", "The one of cleaning clean is to remove smells ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: The effect of cleaning clothing is to remove stains.\nOutput: ", "bloom_target": "The one of cleaning clean is to remove smells .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: The effect of cleaning clothing is to remove stains.\nOutput:  The", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-bc12a3a3a2fc4e26a69806d97bc1c9bc", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-bc12a3a3a2fc4e26a69806d97bc1c9bc", "input": "When the weather is very terrible , we talk of heat .", "output": ["When the weather is very warm, we talk of heat.", "When the weather is very warm , we talk of fun .", "When the storm is very effective , we talk of heat .", "When the weather is very turbulent , we talk of heat ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: When the weather is very terrible , we talk of heat .\nOutput: ", "bloom_target": "When the weather is very warm, we talk of heat.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: When the weather is very terrible , we talk of heat .\nOutput:  When", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-c084a1bc6d284607af02f640a0382a15", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-c084a1bc6d284607af02f640a0382a15", "input": "You would switch a house because you need a place to stay in .", "output": ["You would buy a SUV because you need a jacket to stay in .", "You would buy a pc because you need a keyboard to stay in .", "You would adopt a house because you need a place to stay in .", "You would buy a house because you need a place to stay in."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would switch a house because you need a place to stay in .\nOutput: ", "bloom_target": "You would buy a SUV because you need a jacket to stay in .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would switch a house because you need a place to stay in .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-cca81cece69745c5b4bbc6232ca5fb36", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-cca81cece69745c5b4bbc6232ca5fb36", "input": "If you fancy to have a rest then you should hide in the bathroom .", "output": ["If you walk to have a rest then you should practice in the bathroom .", "If you want to have a rest then you should hide in the bathroom.", "If you stay to have a rest then you should hide in the bathroom .", "If you move to have a rest then you should hide in the bathroom ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you fancy to have a rest then you should hide in the bathroom .\nOutput: ", "bloom_target": "If you walk to have a rest then you should practice in the bathroom .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you fancy to have a rest then you should hide in the bathroom .\nOutput:  If", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-6c307dcd618b4d10ac707d771b1b8b8d", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-6c307dcd618b4d10ac707d771b1b8b8d", "input": "Food is rightly prepared in a stove .", "output": ["Food is often prepared in a kitchen.", "Food is slowly prepared in a manner .", "Food is partially prepared in a kitchen .", "Food is certainly prepared in a vegetable ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Food is rightly prepared in a stove .\nOutput: ", "bloom_target": "Food is partially prepared in a kitchen .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Food is rightly prepared in a stove .\nOutput:  Food", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-427870f18e5d4daab7135c3990d7616f", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-427870f18e5d4daab7135c3990d7616f", "input": "Some as competer in drivers racing .", "output": ["Some girls competer in animal racing .", "Some for competer in ski racing .", "Some pilots competer in airplane racing.", "Some frequently competer in fan racing ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Some as competer in drivers racing .\nOutput: ", "bloom_target": "Some girls competer in animal racing .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Some as competer in drivers racing .\nOutput:  Some", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-91c9812a493040a99c1688e9b41a4b4b", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-91c9812a493040a99c1688e9b41a4b4b", "input": "In general , children often attack their mother more than their community .", "output": ["In general, children often love their mother more than their father.", "In general , children notoriously love their mother more than their father .", "In general , sters often love their mother more than their father .", "In general , agents often love their mother more than their father ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: In general , children often attack their mother more than their community .\nOutput: ", "bloom_target": "In general, children often love their mother more than their father.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: In general , children often attack their mother more than their community .\nOutput:  In", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-9944dc2f7c35425fb6b73961aa756a09", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-9944dc2f7c35425fb6b73961aa756a09", "input": "If you want to accept a table then you should have a window .", "output": ["If you want to shoot a fire then you should have a flame .", "If you want to light a vehicle then you should have a flame .", "If you want to light a hand then you should have a flame .", "If you want to light a fire then you should have a flame."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you want to accept a table then you should have a window .\nOutput: ", "bloom_target": "If you want to light a fire then you should have a flame.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you want to accept a table then you should have a window .\nOutput:  If", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-d8745f013cbe4f9888d4ebbc04d746a8", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-d8745f013cbe4f9888d4ebbc04d746a8", "input": "You may jump out of a window to escape a fear .", "output": ["You may observe out of a window to describe a characteristic .", "You may zip out of a window to spy a gunman .", "You may jump out of a crate to escape a fire .", "You may jump out of a window to escape a fire."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You may jump out of a window to escape a fear .\nOutput: ", "bloom_target": "You may observe out of a window to describe a characteristic .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You may jump out of a window to escape a fear .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-4b89fd3d4f8a4320b311b2c96d92590d", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-4b89fd3d4f8a4320b311b2c96d92590d", "input": "Clothes are kidnapped in a bar room .", "output": ["Clothes are washed in a laundry room.", "Clothes are hung in a laundry room .", "Clothes are washed in a training room .", "Clothes are washed in a factory room ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Clothes are kidnapped in a bar room .\nOutput: ", "bloom_target": "Clothes are washed in a factory room .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Clothes are kidnapped in a bar room .\nOutput:  Cl", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-e80624961f634392a26547f033e7ad0d", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-e80624961f634392a26547f033e7ad0d", "input": "Washing your hands is for good health.", "output": ["Getting your age is for good r .", "icing your hands is for true health .", "Starting your hands is for daily health .", "Washing your fiber is for quality health ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Washing your hands is for good health.\nOutput: ", "bloom_target": "Getting your age is for good r .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Washing your hands is for good health.\nOutput:  W", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task770-28cb14c614a342bf8c6595ff3688252f", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-28cb14c614a342bf8c6595ff3688252f", "input": "Kevin Lepage started third , Terry Labonte as fourth and Robby Gordon qualified as fifth .", "output": ["Kevin Lepage started third , Terry Labonte fourth , and Robby Gordon qualified fifth ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Kevin Lepage started third , Terry Labonte as fourth and Robby Gordon qualified as fifth .\nOutput: ", "bloom_target": "Kevin Lepage started third , Terry Labonte fourth , and Robby Gordon qualified fifth .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Kevin Lepage started third , Terry Labonte as fourth and Robby Gordon qualified as fifth .\nOutput:  Kevin", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-5cc74d40009345b79010747d43c88971", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-5cc74d40009345b79010747d43c88971", "input": "Taieri is a former parliamentary electorate in the Otago region of New Zealand , from 1866 to 1911 .", "output": ["Taieri is a former parliamentary electorate in the New Zealand region of Otago , from 1866 to 1911 ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Taieri is a former parliamentary electorate in the Otago region of New Zealand , from 1866 to 1911 .\nOutput: ", "bloom_target": "Taieri is a former parliamentary electorate in the New Zealand region of Otago , from 1866 to 1911 .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Taieri is a former parliamentary electorate in the Otago region of New Zealand , from 1866 to 1911 .\nOutput:  Tai", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-a77dc074fa8446558edf596735f17e8d", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-a77dc074fa8446558edf596735f17e8d", "input": "The first five weapons were delivered in the first half of 1916 , with a total of 57 barrels and 56 cars completed by the end of the war .", "output": ["The first five weapons were delivered in the first half of 1916 . A total of 57 barrels and 56 carriages were completed by the end of the war ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The first five weapons were delivered in the first half of 1916 , with a total of 57 barrels and 56 cars completed by the end of the war .\nOutput: ", "bloom_target": "The first five weapons were delivered in the first half of 1916 . A total of 57 barrels and 56 carriages were completed by the end of the war .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The first five weapons were delivered in the first half of 1916 , with a total of 57 barrels and 56 cars completed by the end of the war .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-0e7ee4e9f09f40e3b930aff0f09741ac", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-0e7ee4e9f09f40e3b930aff0f09741ac", "input": "Shaffer Creek is a tributary of the Raystown Branch Juniata River ( Brush Creek ) in Bedford County , Pennsylvania , United States .", "output": ["Shaffer Creek is an tributary of Brush Creek ( Raystown Branch Juniata River ) in Bedford County , Pennsylvania in the United States ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Shaffer Creek is a tributary of the Raystown Branch Juniata River ( Brush Creek ) in Bedford County , Pennsylvania , United States .\nOutput: ", "bloom_target": "Shaffer Creek is an tributary of Brush Creek ( Raystown Branch Juniata River ) in Bedford County , Pennsylvania in the United States .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Shaffer Creek is a tributary of the Raystown Branch Juniata River ( Brush Creek ) in Bedford County , Pennsylvania , United States .\nOutput:  Sha", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-e1f4250f762f4cd0a5580ba8f2f8b666", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-e1f4250f762f4cd0a5580ba8f2f8b666", "input": "The school is connected with the secondary department of the autistic school Ysgol Plas Brondyffryn , which was built in 2003 .", "output": ["The school is in conjunction with the secondary department of the autistic school Ysgol Plas Brondyffryn , which was built in 2003 ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The school is connected with the secondary department of the autistic school Ysgol Plas Brondyffryn , which was built in 2003 .\nOutput: ", "bloom_target": "The school is in conjunction with the secondary department of the autistic school Ysgol Plas Brondyffryn , which was built in 2003 .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The school is connected with the secondary department of the autistic school Ysgol Plas Brondyffryn , which was built in 2003 .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-c6b6476aaa4a4a3cbef53f6bded130b6", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-c6b6476aaa4a4a3cbef53f6bded130b6", "input": "Pelmus has been exhibited in museums in Romania , France , Germany , Israel , Canada , Austria , Chile and Brazil , including five solo exhibitions .", "output": ["Pelmus has been exhibited in museums of Romania , France , Germany , Israel , Canada , Austria , Chile and Brazil , including five solo exhibitions ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Pelmus has been exhibited in museums in Romania , France , Germany , Israel , Canada , Austria , Chile and Brazil , including five solo exhibitions .\nOutput: ", "bloom_target": "Pelmus has been exhibited in museums of Romania , France , Germany , Israel , Canada , Austria , Chile and Brazil , including five solo exhibitions .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Pelmus has been exhibited in museums in Romania , France , Germany , Israel , Canada , Austria , Chile and Brazil , including five solo exhibitions .\nOutput:  Pel", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-15f6a2097b3e44a7af513f90dcee9ce4", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-15f6a2097b3e44a7af513f90dcee9ce4", "input": "Malhar Joshi should not be confused with the author Waman Gopal Joshi , they were contemporaries .", "output": ["Vaman Malhar Joshi should not be confused with the writer Waman Gopal Joshi . They were contemporaries ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Malhar Joshi should not be confused with the author Waman Gopal Joshi , they were contemporaries .\nOutput: ", "bloom_target": "Vaman Malhar Joshi should not be confused with the writer Waman Gopal Joshi . They were contemporaries .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Malhar Joshi should not be confused with the author Waman Gopal Joshi , they were contemporaries .\nOutput:  Mal", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-1c0577a26e89434389e6fab7ed0d6b28", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-1c0577a26e89434389e6fab7ed0d6b28", "input": "On November 10 , 2015 , ATP editor Josh Meiseles confirmed the official list of 8 players on the final website of the ATP World Tour .", "output": ["On 10 November 2015 , ATP editor Josh Meiseles confirmed the official list of 8 players on the ATP World Tour final website ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: On November 10 , 2015 , ATP editor Josh Meiseles confirmed the official list of 8 players on the final website of the ATP World Tour .\nOutput: ", "bloom_target": "On 10 November 2015 , ATP editor Josh Meiseles confirmed the official list of 8 players on the ATP World Tour final website .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: On November 10 , 2015 , ATP editor Josh Meiseles confirmed the official list of 8 players on the final website of the ATP World Tour .\nOutput:  On", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-7dc941fe5f114cf49aa8560653382cd7", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-7dc941fe5f114cf49aa8560653382cd7", "input": "Most pre- 1976 series produced by CBS or distributed by CBS Films were later distributed by Viacom and Paramount Television .", "output": ["Most of the series produced by CBS before 1976 or distributed by CBS films were later distributed by Viacom and Paramount Television ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Most pre- 1976 series produced by CBS or distributed by CBS Films were later distributed by Viacom and Paramount Television .\nOutput: ", "bloom_target": "Most of the series produced by CBS before 1976 or distributed by CBS films were later distributed by Viacom and Paramount Television .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Most pre- 1976 series produced by CBS or distributed by CBS Films were later distributed by Viacom and Paramount Television .\nOutput:  Most", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-de3ef751ce5b4c65b0bd7fb9d131da67", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-de3ef751ce5b4c65b0bd7fb9d131da67", "input": "It is located to the north of New Square and New Hempstead , east of Viola , south of Spring Valley and west of New City .", "output": ["It is located north of New Square and New Hempstead , east of Viola , south of Spring Valley , and west of New City ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: It is located to the north of New Square and New Hempstead , east of Viola , south of Spring Valley and west of New City .\nOutput: ", "bloom_target": "It is located north of New Square and New Hempstead , east of Viola , south of Spring Valley , and west of New City .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: It is located to the north of New Square and New Hempstead , east of Viola , south of Spring Valley and west of New City .\nOutput:  It", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-3fef9d08bf1e47269609eeb354444b3f", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-3fef9d08bf1e47269609eeb354444b3f", "input": "Dorothy Kate Richmond , Frances Hodgkins , and Gwen Knight was a contemporary of Stewart .", "output": ["Dorothy Kate Richmond , Frances Hodgkins and Gwen Knight was a Stewart contemporary ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Dorothy Kate Richmond , Frances Hodgkins , and Gwen Knight was a contemporary of Stewart .\nOutput: ", "bloom_target": "Dorothy Kate Richmond , Frances Hodgkins and Gwen Knight was a Stewart contemporary .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Dorothy Kate Richmond , Frances Hodgkins , and Gwen Knight was a contemporary of Stewart .\nOutput:  Dorothy", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-6921f6693b074db7a880769da30667ac", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-6921f6693b074db7a880769da30667ac", "input": "Chicago Public School is a DeWitt Clinton School on the north side of Chicago , Illinois .", "output": ["Chicago Public School is a DeWitt Clinton School on the northern side of Chicago , Illinois ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Chicago Public School is a DeWitt Clinton School on the north side of Chicago , Illinois .\nOutput: ", "bloom_target": "Chicago Public School is a DeWitt Clinton School on the northern side of Chicago , Illinois .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Chicago Public School is a DeWitt Clinton School on the north side of Chicago , Illinois .\nOutput:  Chicago", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-248cbf566b4f4176843e4b2973d15f25", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-248cbf566b4f4176843e4b2973d15f25", "input": "As a songwriter , Lars Ankerstjerne has written songs for Nik & Jay , Burhan G and Rasmus Seebach .", "output": ["Lars Ankerstjerne has written as songwriter for Nik 'Jay , Burhan G and Rasmus Seebach songs ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: As a songwriter , Lars Ankerstjerne has written songs for Nik & Jay , Burhan G and Rasmus Seebach .\nOutput: ", "bloom_target": "Lars Ankerstjerne has written as songwriter for Nik 'Jay , Burhan G and Rasmus Seebach songs .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: As a songwriter , Lars Ankerstjerne has written songs for Nik & Jay , Burhan G and Rasmus Seebach .\nOutput:  As", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-c198b7d643bc4d2ea6f6f9a2856e3f85", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-c198b7d643bc4d2ea6f6f9a2856e3f85", "input": "The North Downs Way crosses the Medway Viaduct at the eastern end of the Medway Valley Walk or motorway bridge .", "output": ["The North Downs Way crosses the Medway viaduct at the eastern end of Medway Valley Walk or the Autobahn bridge ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The North Downs Way crosses the Medway Viaduct at the eastern end of the Medway Valley Walk or motorway bridge .\nOutput: ", "bloom_target": "The North Downs Way crosses the Medway viaduct at the eastern end of Medway Valley Walk or the Autobahn bridge .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The North Downs Way crosses the Medway Viaduct at the eastern end of the Medway Valley Walk or motorway bridge .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-2d5cc5510040480eb665313997f0e92f", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-2d5cc5510040480eb665313997f0e92f", "input": "Below is the early version of the album with all the original segues and \" The Sacrifice of Victor \" is slightly longer in the early configuration .", "output": ["Below is the early version of the album with all the original segues . Also , \" The Sacrifice of Victor \" is slightly longer on the early configuration ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Below is the early version of the album with all the original segues and \" The Sacrifice of Victor \" is slightly longer in the early configuration .\nOutput: ", "bloom_target": "Below is the early version of the album with all the original segues . Also , \" The Sacrifice of Victor \" is slightly longer on the early configuration .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Below is the early version of the album with all the original segues and \" The Sacrifice of Victor \" is slightly longer in the early configuration .\nOutput:  Below", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-b1b9b0b8d8c34817b109461d998f3be0", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-b1b9b0b8d8c34817b109461d998f3be0", "input": "Of the twelve stories that are included , six were previously published in the author 's first collection , \" evening news \" .", "output": ["Of the twelve stories included , six were previously published in the author 's first collection , \" The Evening News \" ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Of the twelve stories that are included , six were previously published in the author 's first collection , \" evening news \" .\nOutput: ", "bloom_target": "Of the twelve stories included , six were previously published in the author 's first collection , \" The Evening News \" .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Of the twelve stories that are included , six were previously published in the author 's first collection , \" evening news \" .\nOutput:  Of", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-fc0ddd91526642edbf45d852ec3809f0", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-fc0ddd91526642edbf45d852ec3809f0", "input": "The branch codice 2 is updated daily , the branch codice 3 is updated every 6 months .", "output": ["The codice 2 branch is updated daily , and the branch codice 3 is updated every 6 months ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The branch codice 2 is updated daily , the branch codice 3 is updated every 6 months .\nOutput: ", "bloom_target": "The codice 2 branch is updated daily , and the branch codice 3 is updated every 6 months .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The branch codice 2 is updated daily , the branch codice 3 is updated every 6 months .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-e5f94d227a2f410b8ae24946725f1ae8", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-e5f94d227a2f410b8ae24946725f1ae8", "input": "She worked and lived in Stuttgart , Berlin ( Germany ) and in Vienna ( Austria ) .", "output": ["She worked and lived in Germany ( Stuttgart , Berlin ) and in Vienna ( Austria ) ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: She worked and lived in Stuttgart , Berlin ( Germany ) and in Vienna ( Austria ) .\nOutput: ", "bloom_target": "She worked and lived in Germany ( Stuttgart , Berlin ) and in Vienna ( Austria ) .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: She worked and lived in Stuttgart , Berlin ( Germany ) and in Vienna ( Austria ) .\nOutput:  She", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-7f91235d9f834f40aea302c03b60a675", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-7f91235d9f834f40aea302c03b60a675", "input": "He has previously played for Wolverhampton Wanderers , Kidderminster Harriers , Mansfield Town , Lincoln City , Northampton Town , Chesterfield and Gateshead .", "output": ["He formerly played for Wolverhampton Wanderers , Kidderminster Harriers , Mansfield Town , Chesterfield , Northampton Town , Lincoln City and Gateshead ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: He has previously played for Wolverhampton Wanderers , Kidderminster Harriers , Mansfield Town , Lincoln City , Northampton Town , Chesterfield and Gateshead .\nOutput: ", "bloom_target": "He formerly played for Wolverhampton Wanderers , Kidderminster Harriers , Mansfield Town , Chesterfield , Northampton Town , Lincoln City and Gateshead .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: He has previously played for Wolverhampton Wanderers , Kidderminster Harriers , Mansfield Town , Lincoln City , Northampton Town , Chesterfield and Gateshead .\nOutput:  He", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-489fd865283d43a0ad4c2ebd5d64f8e4", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-489fd865283d43a0ad4c2ebd5d64f8e4", "input": "Chris Egan ( Nick Smith ) settles down with his family in Summer Bay , and he and Duncan quickly become friends and get into various crises .", "output": ["Nick Smith ( Chris Egan ) settles in Summer Bay with his family and he and Duncan quickly become friends and get into various scrapes ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Chris Egan ( Nick Smith ) settles down with his family in Summer Bay , and he and Duncan quickly become friends and get into various crises .\nOutput: ", "bloom_target": "Nick Smith ( Chris Egan ) settles in Summer Bay with his family and he and Duncan quickly become friends and get into various scrapes .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Chris Egan ( Nick Smith ) settles down with his family in Summer Bay , and he and Duncan quickly become friends and get into various crises .\nOutput:  Chris", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-84dfba11a2d04511951cc58224038120", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-84dfba11a2d04511951cc58224038120", "input": "Bailey was replaced by Henry Cole as caretaker of Breakheart Hill .", "output": ["Bailey was succeeded as caretaker of Breakheart Hill by Henry Cole ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Bailey was replaced by Henry Cole as caretaker of Breakheart Hill .\nOutput: ", "bloom_target": "Bailey was succeeded as caretaker of Breakheart Hill by Henry Cole .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Bailey was replaced by Henry Cole as caretaker of Breakheart Hill .\nOutput:  Bailey", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-325379183b384b688531dd5f296f9b90", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-325379183b384b688531dd5f296f9b90", "input": "Based on the city of Baltimore , only mentioned , never visited in the show .", "output": ["Based on the city of Baltimore , only mentioned , has never visited in the show ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Based on the city of Baltimore , only mentioned , never visited in the show .\nOutput: ", "bloom_target": "Based on the city of Baltimore , only mentioned , has never visited in the show .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Based on the city of Baltimore , only mentioned , never visited in the show .\nOutput:  Based", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-59fdde37612a47728899798f140bae49", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-59fdde37612a47728899798f140bae49", "input": "It is covered with a natural vegetation of grassland of less fertile red clay bases , and Saltbush bushland on more fertile earths .", "output": ["It is covered with a natural vegetation of grassland of less fertile red clay soils , and saltbush shrubland on more fertile earths ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: It is covered with a natural vegetation of grassland of less fertile red clay bases , and Saltbush bushland on more fertile earths .\nOutput: ", "bloom_target": "It is covered with a natural vegetation of grassland of less fertile red clay soils , and saltbush shrubland on more fertile earths .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: It is covered with a natural vegetation of grassland of less fertile red clay bases , and Saltbush bushland on more fertile earths .\nOutput:  It", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-8bbfbf77b67e4fa9835f07643f82b5c9", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-8bbfbf77b67e4fa9835f07643f82b5c9", "input": "It located in Himachal Pradesh ( Tattapani ) , at the altitude of 650mts , perfect temperature for the healing treatments .", "output": ["It is located in Tattapani ( Himachal Pradesh ) , at an altitude of 650mts , the perfect temperature for the treatments ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: It located in Himachal Pradesh ( Tattapani ) , at the altitude of 650mts , perfect temperature for the healing treatments .\nOutput: ", "bloom_target": "It is located in Tattapani ( Himachal Pradesh ) , at an altitude of 650mts , the perfect temperature for the treatments .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: It located in Himachal Pradesh ( Tattapani ) , at the altitude of 650mts , perfect temperature for the healing treatments .\nOutput:  It", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-35695d85b811408da42172fae045da8e", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-35695d85b811408da42172fae045da8e", "input": "The tsunami was observed along the Pacific coast of Japan from the Izu peninsula to Kyushu and recorded by tides from Hawaii to Alaska .", "output": ["The tsunami was observed along the Pacific coast of Japan from Izu Peninsula to Kyushu , and recorded by tide gauges from Hawaii to Alaska ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The tsunami was observed along the Pacific coast of Japan from the Izu peninsula to Kyushu and recorded by tides from Hawaii to Alaska .\nOutput: ", "bloom_target": "The tsunami was observed along the Pacific coast of Japan from Izu Peninsula to Kyushu , and recorded by tide gauges from Hawaii to Alaska .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The tsunami was observed along the Pacific coast of Japan from the Izu peninsula to Kyushu and recorded by tides from Hawaii to Alaska .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-b738eb9836004e45bf19e18c37da287d", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-b738eb9836004e45bf19e18c37da287d", "input": "The bones of Zrinski and Frankopan were found in Austria in 1907 and brought to Zagreb in 1919 , where they were reburied in the Zagreb Cathedral .", "output": ["The bones of Zrinski and Frankopan were found in 1907 in Austria and brought to Zagreb in 1919 , where they were rebuilt in the Zagreb Cathedral ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The bones of Zrinski and Frankopan were found in Austria in 1907 and brought to Zagreb in 1919 , where they were reburied in the Zagreb Cathedral .\nOutput: ", "bloom_target": "The bones of Zrinski and Frankopan were found in 1907 in Austria and brought to Zagreb in 1919 , where they were rebuilt in the Zagreb Cathedral .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The bones of Zrinski and Frankopan were found in Austria in 1907 and brought to Zagreb in 1919 , where they were reburied in the Zagreb Cathedral .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-5683aeb617c743388f9ee8a3d8704281", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-5683aeb617c743388f9ee8a3d8704281", "input": "The light novels are written by Dojyomaru and are illustrated by Fuyuyuki and published by Overlap Bunko .", "output": ["The light novels are written by Dojyomaru and illustrated by Fuyuyuki , and are published by Overlap Bunko ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The light novels are written by Dojyomaru and are illustrated by Fuyuyuki and published by Overlap Bunko .\nOutput: ", "bloom_target": "The light novels are written by Dojyomaru and illustrated by Fuyuyuki , and are published by Overlap Bunko .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The light novels are written by Dojyomaru and are illustrated by Fuyuyuki and published by Overlap Bunko .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-db6ea381b9e948169f8abd73df3e2488", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-db6ea381b9e948169f8abd73df3e2488", "input": "Debbie Downer is a name of a fictional \" Saturday Night Live \" character that was debuted in 2004 and portrayed by Rachel Dratch .", "output": ["Debbie Downer is a name of a fictional \" Saturday Night Live \" character who debuted in 2004 , and who was portrayed by Rachel Dratch ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Debbie Downer is a name of a fictional \" Saturday Night Live \" character that was debuted in 2004 and portrayed by Rachel Dratch .\nOutput: ", "bloom_target": "Debbie Downer is a name of a fictional \" Saturday Night Live \" character who debuted in 2004 , and who was portrayed by Rachel Dratch .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Debbie Downer is a name of a fictional \" Saturday Night Live \" character that was debuted in 2004 and portrayed by Rachel Dratch .\nOutput:  Deb", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-44f14c04f1cc4fea93b45ac534ad9a6f", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-44f14c04f1cc4fea93b45ac534ad9a6f", "input": "He gained popularity also by replacing Archie Kao and Adam Rodriguez .", "output": ["He also gained popularity by replacing Archie Kao with Adam Rodriguez ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: He gained popularity also by replacing Archie Kao and Adam Rodriguez .\nOutput: ", "bloom_target": "He also gained popularity by replacing Archie Kao with Adam Rodriguez .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: He gained popularity also by replacing Archie Kao and Adam Rodriguez .\nOutput:  He", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-babb3dc958274ea880e1ea523519da62", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-babb3dc958274ea880e1ea523519da62", "input": "Jont\u00e9 Buhl ( born April 4 , 1982 ) is a former Canadian professional football player who played for four years at the Edmonton Eskimos of the Canadian Football League .", "output": ["Jont\u00e9 Buhl ( born April 4 , 1982 ) is a Canadian former professional football cornerback who played four seasons for the Edmonton Eskimos of the Canadian Football League ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Jont\u00e9 Buhl ( born April 4 , 1982 ) is a former Canadian professional football player who played for four years at the Edmonton Eskimos of the Canadian Football League .\nOutput: ", "bloom_target": "Jont\u00e9 Buhl ( born April 4 , 1982 ) is a Canadian former professional football cornerback who played four seasons for the Edmonton Eskimos of the Canadian Football League .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Jont\u00e9 Buhl ( born April 4 , 1982 ) is a former Canadian professional football player who played for four years at the Edmonton Eskimos of the Canadian Football League .\nOutput:  J", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-c0b06ad2990842e2a0277821cd282668", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-c0b06ad2990842e2a0277821cd282668", "input": "In December 2006 , Muspratt was named \" Chicagoan of the Year by John von Rhein and the staff of the \" Chicago Tribune \" in the Classic .", "output": ["In December 2006 , Muspratt was named \" Chicagoan of the Year \" in classical music by John von Rhein and the staff of the \" Chicago Tribune \" ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: In December 2006 , Muspratt was named \" Chicagoan of the Year by John von Rhein and the staff of the \" Chicago Tribune \" in the Classic .\nOutput: ", "bloom_target": "In December 2006 , Muspratt was named \" Chicagoan of the Year \" in classical music by John von Rhein and the staff of the \" Chicago Tribune \" .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: In December 2006 , Muspratt was named \" Chicagoan of the Year by John von Rhein and the staff of the \" Chicago Tribune \" in the Classic .\nOutput:  In", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-cff11252602c42b3ad4e738de85479c1", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-cff11252602c42b3ad4e738de85479c1", "input": "Other car manufacturers that have produced models with suicide doors include Citro\u00ebn , Lancia , Opel , Panhard , Rover , Saab , Saturn , Skoda and Volkswagen .", "output": ["Other car manufacturers which have produced models with suicide doors include Opel , Lancia , Citro\u00ebn , Panhard , Rover , Saab , Saturn , \u0160koda , and Volkswagen ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Other car manufacturers that have produced models with suicide doors include Citro\u00ebn , Lancia , Opel , Panhard , Rover , Saab , Saturn , Skoda and Volkswagen .\nOutput: ", "bloom_target": "Other car manufacturers which have produced models with suicide doors include Opel , Lancia , Citro\u00ebn , Panhard , Rover , Saab , Saturn , \u0160koda , and Volkswagen .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Other car manufacturers that have produced models with suicide doors include Citro\u00ebn , Lancia , Opel , Panhard , Rover , Saab , Saturn , Skoda and Volkswagen .\nOutput:  Other", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-8f4c7dbc65844f719d570abe5015ecfe", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-8f4c7dbc65844f719d570abe5015ecfe", "input": "He spent several more years in New York City as a computer consultant and software engineer , then moved to Atlanta in 1998 .", "output": ["He spent several years in New York City as a computer consultant and software engineer and moved to Atlanta in 1998 ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: He spent several more years in New York City as a computer consultant and software engineer , then moved to Atlanta in 1998 .\nOutput: ", "bloom_target": "He spent several years in New York City as a computer consultant and software engineer and moved to Atlanta in 1998 .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: He spent several more years in New York City as a computer consultant and software engineer , then moved to Atlanta in 1998 .\nOutput:  He", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-5b59c6f2bd664c91b8f4108f33f0cd4b", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-5b59c6f2bd664c91b8f4108f33f0cd4b", "input": "On 7 July 2011 , Russell was traded for Kris Russell on the Columbus Blue Jackets and joined his brother Michael Blunden with the Blue Jackets organization .", "output": ["On July 7 , 2011 , Russell was traded to the Columbus Blue Jackets for Kris Russell . He joined his brother Michael Blunden with the Blue Jackets organization ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: On 7 July 2011 , Russell was traded for Kris Russell on the Columbus Blue Jackets and joined his brother Michael Blunden with the Blue Jackets organization .\nOutput: ", "bloom_target": "On July 7 , 2011 , Russell was traded to the Columbus Blue Jackets for Kris Russell . He joined his brother Michael Blunden with the Blue Jackets organization .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: On 7 July 2011 , Russell was traded for Kris Russell on the Columbus Blue Jackets and joined his brother Michael Blunden with the Blue Jackets organization .\nOutput:  On", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-62291aab558f4900a088eabb8e6ef9d9", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-62291aab558f4900a088eabb8e6ef9d9", "input": "Chad Ochocinco ( born 1978 ; formerly Chad Johnson ) is an American football wide receiver .", "output": ["Chad Ochocinco ( born 1978 ; formerly Chad Johnson ) is an American - American - football receiver ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Chad Ochocinco ( born 1978 ; formerly Chad Johnson ) is an American football wide receiver .\nOutput: ", "bloom_target": "Chad Ochocinco ( born 1978 ; formerly Chad Johnson ) is an American - American - football receiver .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Chad Ochocinco ( born 1978 ; formerly Chad Johnson ) is an American football wide receiver .\nOutput:  Chad", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-62c9b28476484d2484246d21b0a660b8", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-62c9b28476484d2484246d21b0a660b8", "input": "Sussex and Lancing Colleges are recorded as having played a football match in November 1860 , the first by public schools in Brighton .", "output": ["Sussex and Lancing Colleges , which played a football match in November 1860 , are recorded as the first of public schools in Brighton ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Sussex and Lancing Colleges are recorded as having played a football match in November 1860 , the first by public schools in Brighton .\nOutput: ", "bloom_target": "Sussex and Lancing Colleges , which played a football match in November 1860 , are recorded as the first of public schools in Brighton .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Sussex and Lancing Colleges are recorded as having played a football match in November 1860 , the first by public schools in Brighton .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-cf978674d0504664beafc3825f9e1e08", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-cf978674d0504664beafc3825f9e1e08", "input": "Kathryn Lindskoog , an independent Lewis scholar , argued that Hooper 's scholarship is not reliable and that he has made false statements and attributed forged works to Lewis .", "output": ["Kathryn Lindskoog , an independent Lewis scholar , argued that Hooper 's scholarship is not reliable and that he has made false statements and attributed false works ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Kathryn Lindskoog , an independent Lewis scholar , argued that Hooper 's scholarship is not reliable and that he has made false statements and attributed forged works to Lewis .\nOutput: ", "bloom_target": "Kathryn Lindskoog , an independent Lewis scholar , argued that Hooper 's scholarship is not reliable and that he has made false statements and attributed false works .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Kathryn Lindskoog , an independent Lewis scholar , argued that Hooper 's scholarship is not reliable and that he has made false statements and attributed forged works to Lewis .\nOutput:  Kath", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-fbd3f5523abb427d92f4cb83d15d92b5", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-fbd3f5523abb427d92f4cb83d15d92b5", "input": "It has a high school ( \" Cornway Junior College \" ) and a nursery school ( \" Cornway Senior College \" ) .", "output": ["It has a high school ( \" Cornway Junior College \" ) and a preparatory school ( \" Cornway Senior College \" ) ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: It has a high school ( \" Cornway Junior College \" ) and a nursery school ( \" Cornway Senior College \" ) .\nOutput: ", "bloom_target": "It has a high school ( \" Cornway Junior College \" ) and a preparatory school ( \" Cornway Senior College \" ) .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: It has a high school ( \" Cornway Junior College \" ) and a nursery school ( \" Cornway Senior College \" ) .\nOutput:  It", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-37cdf6947bac4805927ed406525b3612", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-37cdf6947bac4805927ed406525b3612", "input": "The isodynamic conjugates of Fermat - points are the isogonal points and vice versa .", "output": ["The isodynamic conjugates of the Fermat points are the isogonal points and vice versa ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The isodynamic conjugates of Fermat - points are the isogonal points and vice versa .\nOutput: ", "bloom_target": "The isodynamic conjugates of the Fermat points are the isogonal points and vice versa .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The isodynamic conjugates of Fermat - points are the isogonal points and vice versa .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-cec36122f8c04ce99b5ba79ae6a30d0d", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-cec36122f8c04ce99b5ba79ae6a30d0d", "input": "In 1951 , he died and retired in 1956 .", "output": ["He died in 1951 and retired in 1956 ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: In 1951 , he died and retired in 1956 .\nOutput: ", "bloom_target": "He died in 1951 and retired in 1956 .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: In 1951 , he died and retired in 1956 .\nOutput:  In", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-e74d814a95eb4998a334781d86628c45", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-e74d814a95eb4998a334781d86628c45", "input": "Seb Janiak is a French photographer and video director of Polish origin .", "output": ["Seb Janiak is the French photographer and video director of Polish origin ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Seb Janiak is a French photographer and video director of Polish origin .\nOutput: ", "bloom_target": "Seb Janiak is the French photographer and video director of Polish origin .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Seb Janiak is a French photographer and video director of Polish origin .\nOutput:  Seb", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-577293331e7b465e81cb755e4a9e6fe2", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-577293331e7b465e81cb755e4a9e6fe2", "input": "The department of dramatic speaking and public arts was originally headed by T. Earl Pardoe .", "output": ["The Dramatic and Public Arts Department was originally headed by T. Earl Pardoe ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The department of dramatic speaking and public arts was originally headed by T. Earl Pardoe .\nOutput: ", "bloom_target": "The Dramatic and Public Arts Department was originally headed by T. Earl Pardoe .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The department of dramatic speaking and public arts was originally headed by T. Earl Pardoe .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-a220e5e6434b408997d1faa1608122a3", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-a220e5e6434b408997d1faa1608122a3", "input": "This was the last AFL Championship to end the season ; the first Super Bowl followed the 1966 season .", "output": ["This was the last AFL Championship to complete the season , the first Super Bowl followed in the 1966 season ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: This was the last AFL Championship to end the season ; the first Super Bowl followed the 1966 season .\nOutput: ", "bloom_target": "This was the last AFL Championship to complete the season , the first Super Bowl followed in the 1966 season .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: This was the last AFL Championship to end the season ; the first Super Bowl followed the 1966 season .\nOutput:  This", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-bdd98d7a66174a0f8209bc63263e3a65", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-bdd98d7a66174a0f8209bc63263e3a65", "input": "The river Oraciu or Orociu is a tributary of the River Pustnic in Romania .", "output": ["The Oraciu River or Orociu River is a tributary of the Pustnic River in Romania ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The river Oraciu or Orociu is a tributary of the River Pustnic in Romania .\nOutput: ", "bloom_target": "The Oraciu River or Orociu River is a tributary of the Pustnic River in Romania .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The river Oraciu or Orociu is a tributary of the River Pustnic in Romania .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-7c4d982253354d71be64412123d4736d", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-7c4d982253354d71be64412123d4736d", "input": "The complex of the Trabzon World Trade Center is close to Trabzon Airport .", "output": ["The complex of World Trade Center Trabzon is situated close to Trabzon Airport ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The complex of the Trabzon World Trade Center is close to Trabzon Airport .\nOutput: ", "bloom_target": "The complex of World Trade Center Trabzon is situated close to Trabzon Airport .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The complex of the Trabzon World Trade Center is close to Trabzon Airport .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-915154c582404157881adaeb41b47bfe", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-915154c582404157881adaeb41b47bfe", "input": "Smaller small cross circuit pylons may have two single arms on one side and one on the other .", "output": ["Smaller small cross pylons may have two single arms on one side and one on the other ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Smaller small cross circuit pylons may have two single arms on one side and one on the other .\nOutput: ", "bloom_target": "Smaller small cross pylons may have two single arms on one side and one on the other .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Smaller small cross circuit pylons may have two single arms on one side and one on the other .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-e2b1b70cf7a344d78977f7449cf3de0b", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-e2b1b70cf7a344d78977f7449cf3de0b", "input": "Togdheer ( Somali \" Wabi Togdheer \" ) is a seasonal river in the Togdheer River region in the eastern part of Somaliland .", "output": ["Togdheer ( Somali \" Wabi Togdheer \" ) is a seasonal river in the Togdheer River region of eastern Somaliland ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Togdheer ( Somali \" Wabi Togdheer \" ) is a seasonal river in the Togdheer River region in the eastern part of Somaliland .\nOutput: ", "bloom_target": "Togdheer ( Somali \" Wabi Togdheer \" ) is a seasonal river in the Togdheer River region of eastern Somaliland .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Togdheer ( Somali \" Wabi Togdheer \" ) is a seasonal river in the Togdheer River region in the eastern part of Somaliland .\nOutput:  T", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-e840af98aaa64ff588d411d1a60bcc44", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-e840af98aaa64ff588d411d1a60bcc44", "input": "St John married Elizabeth Crowley ( daughter of Ambrose Crowley ) of Greenwich on March 6 , 1725 . Their children were :", "output": ["St John married Elizabeth Crowley ( the daughter of Ambrose Crowley ) of Greenwich on 6 March 1725 . Their children included :"]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: St John married Elizabeth Crowley ( daughter of Ambrose Crowley ) of Greenwich on March 6 , 1725 . Their children were :\nOutput: ", "bloom_target": "St John married Elizabeth Crowley ( the daughter of Ambrose Crowley ) of Greenwich on 6 March 1725 . Their children included :", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: St John married Elizabeth Crowley ( daughter of Ambrose Crowley ) of Greenwich on March 6 , 1725 . Their children were :\nOutput:  St", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-fe24a3e86d644937acf99fe5f606dc38", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-fe24a3e86d644937acf99fe5f606dc38", "input": "The house was purchased in 1858 by Sir David Dundas of Dunira , who sold it to George Dewhurst from Lymm , Cheshire in 1864 .", "output": ["The house was purchased by Sir David Dundas of Dunira in 1858 , who sold it on to George Dewhurst of Lymm , Cheshire , in 1864 ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The house was purchased in 1858 by Sir David Dundas of Dunira , who sold it to George Dewhurst from Lymm , Cheshire in 1864 .\nOutput: ", "bloom_target": "The house was purchased by Sir David Dundas of Dunira in 1858 , who sold it on to George Dewhurst of Lymm , Cheshire , in 1864 .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The house was purchased in 1858 by Sir David Dundas of Dunira , who sold it to George Dewhurst from Lymm , Cheshire in 1864 .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-1e7df489116a43c196fdf30ecad0bbaa", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-1e7df489116a43c196fdf30ecad0bbaa", "input": "Quintus Caecilius Metellus Macedonicus was the second son of Roman politician and general Lucius Caecilius Metellus Diadematus .", "output": ["Quintus Caecilius Metellus Macedonicus was the second son of the Roman politician , General Lucius Caecilius Metellus Diadematus ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Quintus Caecilius Metellus Macedonicus was the second son of Roman politician and general Lucius Caecilius Metellus Diadematus .\nOutput: ", "bloom_target": "Quintus Caecilius Metellus Macedonicus was the second son of the Roman politician , General Lucius Caecilius Metellus Diadematus .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Quintus Caecilius Metellus Macedonicus was the second son of Roman politician and general Lucius Caecilius Metellus Diadematus .\nOutput:  Quint", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-b2d2ef0ef68244e1a63bd9f50090a1a2", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-b2d2ef0ef68244e1a63bd9f50090a1a2", "input": "\" Bonne Citoyenne \" had the misfortune to become damaged in a storm and to be separated from the rest of the French squadron .", "output": ["\" Bonne Citoyenne \" had the misfortune to be damaged in a storm and separated from the rest of the French squadron ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: \" Bonne Citoyenne \" had the misfortune to become damaged in a storm and to be separated from the rest of the French squadron .\nOutput: ", "bloom_target": "\" Bonne Citoyenne \" had the misfortune to be damaged in a storm and separated from the rest of the French squadron .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: \" Bonne Citoyenne \" had the misfortune to become damaged in a storm and to be separated from the rest of the French squadron .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-4344c38acd54482a89b6c8e5ae387f7d", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-4344c38acd54482a89b6c8e5ae387f7d", "input": "\" The Timber \" was released in North America on February 27 , 2015 , Well Go USA Entertainment released it in Germany on October 6 , 2015 .", "output": ["On February 27 , 2015 , \" The Timber \" was released in North America , Well Go USA Entertainment was released on October 6 , 2015 in Germany ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: \" The Timber \" was released in North America on February 27 , 2015 , Well Go USA Entertainment released it in Germany on October 6 , 2015 .\nOutput: ", "bloom_target": "On February 27 , 2015 , \" The Timber \" was released in North America , Well Go USA Entertainment was released on October 6 , 2015 in Germany .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: \" The Timber \" was released in North America on February 27 , 2015 , Well Go USA Entertainment released it in Germany on October 6 , 2015 .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-4323ef3109a242f382fbf3fb822c55ea", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-4323ef3109a242f382fbf3fb822c55ea", "input": "It is located in Shawnee Township and adjacent to Duchouquet Township in Allen County .", "output": ["It is located in Shawnee Township and is adjacent to Duchouquet Township in Allen County ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: It is located in Shawnee Township and adjacent to Duchouquet Township in Allen County .\nOutput: ", "bloom_target": "It is located in Shawnee Township and is adjacent to Duchouquet Township in Allen County .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: It is located in Shawnee Township and adjacent to Duchouquet Township in Allen County .\nOutput:  It", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-5e24e91533e24ed3a0dca7088cfc412c", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-5e24e91533e24ed3a0dca7088cfc412c", "input": "Ashley was born on 1 November 1986 and is a contemporary dancer from Los Angeles who originally grew up in Arizona .", "output": ["Born on November 1 , 1986 , Ashley is a contemporary dancer from Los Angeles who was originally raised in Arizona ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Ashley was born on 1 November 1986 and is a contemporary dancer from Los Angeles who originally grew up in Arizona .\nOutput: ", "bloom_target": "Born on November 1 , 1986 , Ashley is a contemporary dancer from Los Angeles who was originally raised in Arizona .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Ashley was born on 1 November 1986 and is a contemporary dancer from Los Angeles who originally grew up in Arizona .\nOutput:  Ashley", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-e509a7f7894e44ebb6ffbf54b1a93cb0", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-e509a7f7894e44ebb6ffbf54b1a93cb0", "input": "The Little Jocko River flows across the Saint Lawrence River and the Ottawa River to the Jocko River .", "output": ["The Little Jocko River flows via the Saint Lawrence River and the Ottawa River to the Jocko River ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The Little Jocko River flows across the Saint Lawrence River and the Ottawa River to the Jocko River .\nOutput: ", "bloom_target": "The Little Jocko River flows via the Saint Lawrence River and the Ottawa River to the Jocko River .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The Little Jocko River flows across the Saint Lawrence River and the Ottawa River to the Jocko River .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-a9860be17cd44eefb546a1ea83715b5b", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-a9860be17cd44eefb546a1ea83715b5b", "input": "After his dismissal , he moved from Germany to New Mexico and then to Los Angeles , then to San Francisco .", "output": ["After his discharge he moved from Germany to New Mexico and then to Los Angeles then to San Francisco ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: After his dismissal , he moved from Germany to New Mexico and then to Los Angeles , then to San Francisco .\nOutput: ", "bloom_target": "After his discharge he moved from Germany to New Mexico and then to Los Angeles then to San Francisco .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: After his dismissal , he moved from Germany to New Mexico and then to Los Angeles , then to San Francisco .\nOutput:  After", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-65fd7e5f66714bd6b809bed0c39b85ec", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-65fd7e5f66714bd6b809bed0c39b85ec", "input": "On 9 June 2017 , Barkchi appointed Mubin Ergashev as their manager after Vitaliy Levchenko joined the coaching staff of Krylia Sovetov .", "output": ["On 9 June 2017 , Barkchi Mubin Ergashev appointed her manager after Vitaliy Levchenko joined the Krylia Sovetov coaching staff ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: On 9 June 2017 , Barkchi appointed Mubin Ergashev as their manager after Vitaliy Levchenko joined the coaching staff of Krylia Sovetov .\nOutput: ", "bloom_target": "On 9 June 2017 , Barkchi Mubin Ergashev appointed her manager after Vitaliy Levchenko joined the Krylia Sovetov coaching staff .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: On 9 June 2017 , Barkchi appointed Mubin Ergashev as their manager after Vitaliy Levchenko joined the coaching staff of Krylia Sovetov .\nOutput:  On", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-c04a0bad540041d9907a3760d74dd250", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-c04a0bad540041d9907a3760d74dd250", "input": "This is a list of caves in the United Kingdom , including information about the largest and deepest caves in the UK .", "output": ["This is a list of caves in the United Kingdom , including information on the largest and deepest caves in the UK ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: This is a list of caves in the United Kingdom , including information about the largest and deepest caves in the UK .\nOutput: ", "bloom_target": "This is a list of caves in the United Kingdom , including information on the largest and deepest caves in the UK .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: This is a list of caves in the United Kingdom , including information about the largest and deepest caves in the UK .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-c137880cfd4943548f49e3b038fa1d01", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-c137880cfd4943548f49e3b038fa1d01", "input": "The female lead role was played by Cortez in \" Ali Baba and the Sacred Crown \" , directed by Erminio Salvi .", "output": ["Cortez played the female lead in \" Ali Baba and the Sacred Crown \" , directed by Erminio Salvi ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The female lead role was played by Cortez in \" Ali Baba and the Sacred Crown \" , directed by Erminio Salvi .\nOutput: ", "bloom_target": "Cortez played the female lead in \" Ali Baba and the Sacred Crown \" , directed by Erminio Salvi .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The female lead role was played by Cortez in \" Ali Baba and the Sacred Crown \" , directed by Erminio Salvi .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-a8d838e5a2f14f5c95c16bd68b8b1437", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-a8d838e5a2f14f5c95c16bd68b8b1437", "input": "According to the United States Census Bureau , the city is a total surface area of which has land and , or 1.35 % , is water .", "output": ["According to the United States Census Bureau , the town has a total area of , of which is land and , or 1.35 % , is water ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: According to the United States Census Bureau , the city is a total surface area of which has land and , or 1.35 % , is water .\nOutput: ", "bloom_target": "According to the United States Census Bureau , the town has a total area of , of which is land and , or 1.35 % , is water .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: According to the United States Census Bureau , the city is a total surface area of which has land and , or 1.35 % , is water .\nOutput:  According", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-4e3c25560b0e4accb6caf137fd7bb7c3", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-4e3c25560b0e4accb6caf137fd7bb7c3", "input": "He graduated from the Galatasaray High School in Shumen in 1858 and became a teacher in Istanbul , where he remained until 1864 .", "output": ["In 1858 he graduated from the Galatasaray School in Schumen and became a teacher in Istanbul , where he remained until 1864 ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: He graduated from the Galatasaray High School in Shumen in 1858 and became a teacher in Istanbul , where he remained until 1864 .\nOutput: ", "bloom_target": "In 1858 he graduated from the Galatasaray School in Schumen and became a teacher in Istanbul , where he remained until 1864 .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: He graduated from the Galatasaray High School in Shumen in 1858 and became a teacher in Istanbul , where he remained until 1864 .\nOutput:  He", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-1638304d15b945aba69537497d7af766", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-1638304d15b945aba69537497d7af766", "input": "Werder 's forces invested Belfort and reached the city on 3 November .", "output": ["Werder 's troops invested Belfort and reached the city on November 3 ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Werder 's forces invested Belfort and reached the city on 3 November .\nOutput: ", "bloom_target": "Werder 's troops invested Belfort and reached the city on November 3 .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Werder 's forces invested Belfort and reached the city on 3 November .\nOutput:  Wer", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-c2f38acae2d044d795cd58b88ea4621f", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-c2f38acae2d044d795cd58b88ea4621f", "input": "( Don Wyatt had been in the penguins in 1956 , and both Eddie and Ray had been with Ray in the later Colts / Fortunes . )", "output": ["( Don Wyatt had been in the Penguins in 1956 and both Eddie and Ray had been in the later Colts/Fortunes with Ray . )"]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: ( Don Wyatt had been in the penguins in 1956 , and both Eddie and Ray had been with Ray in the later Colts / Fortunes . )\nOutput: ", "bloom_target": "( Don Wyatt had been in the Penguins in 1956 and both Eddie and Ray had been in the later Colts/Fortunes with Ray . )", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: ( Don Wyatt had been in the penguins in 1956 , and both Eddie and Ray had been with Ray in the later Colts / Fortunes . )\nOutput:  Eddie", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-d0fbf7f18a6243aebc5d1246aa25706e", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-d0fbf7f18a6243aebc5d1246aa25706e", "input": "Jie\u0163 is a tributary of the Slivei River in Romania .", "output": ["The Jie\u0163 is a tributary of the Slivei River in Romania ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Jie\u0163 is a tributary of the Slivei River in Romania .\nOutput: ", "bloom_target": "The Jie\u0163 is a tributary of the Slivei River in Romania .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Jie\u0163 is a tributary of the Slivei River in Romania .\nOutput:  J", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-fdc95dbca9a44717b657c915ce9c73a3", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-fdc95dbca9a44717b657c915ce9c73a3", "input": "The 2007 -- 08 Kansas State Wildcats Men 's Basketball Team represents Kansas State University at the 2007 -- 08 College - Basketball - Season .", "output": ["The 2007 -- 08 Kansas State Wildcats men 's basketball team represented Kansas State University in the 2007 -- 08 college basketball season ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The 2007 -- 08 Kansas State Wildcats Men 's Basketball Team represents Kansas State University at the 2007 -- 08 College - Basketball - Season .\nOutput: ", "bloom_target": "The 2007 -- 08 Kansas State Wildcats men 's basketball team represented Kansas State University in the 2007 -- 08 college basketball season .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The 2007 -- 08 Kansas State Wildcats Men 's Basketball Team represents Kansas State University at the 2007 -- 08 College - Basketball - Season .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-b10c10156d004d1aa7fee1a4cee8cec2", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-b10c10156d004d1aa7fee1a4cee8cec2", "input": "Earlier in 209 , Sun Quan Sun married Quan \u2019 s younger sister , Lady Sun , to strengthen the alliance between him and Liu Bei .", "output": ["Earlier in 209 , Sun Quan married Sun Quan 's younger sister Lady Sun to strengthen the alliance between him and Liu Bei ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Earlier in 209 , Sun Quan Sun married Quan \u2019 s younger sister , Lady Sun , to strengthen the alliance between him and Liu Bei .\nOutput: ", "bloom_target": "Earlier in 209 , Sun Quan married Sun Quan 's younger sister Lady Sun to strengthen the alliance between him and Liu Bei .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Earlier in 209 , Sun Quan Sun married Quan \u2019 s younger sister , Lady Sun , to strengthen the alliance between him and Liu Bei .\nOutput:  Sun", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-b9234a56b4264f1badc45fb1e603b620", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-b9234a56b4264f1badc45fb1e603b620", "input": "Contempo Magazine is a monthly print and daily online American magazine published in McAllen , Texas .", "output": ["Contempo Magazine is a monthly American print and online magazine in McAllen , Texas ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Contempo Magazine is a monthly print and daily online American magazine published in McAllen , Texas .\nOutput: ", "bloom_target": "Contempo Magazine is a monthly American print and online magazine in McAllen , Texas .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Contempo Magazine is a monthly print and daily online American magazine published in McAllen , Texas .\nOutput:  Cont", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-b27718272dea455789acea0b7fb3957a", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-b27718272dea455789acea0b7fb3957a", "input": "The city sits at the confluence of the Snake River with the great Weiser River , which marks the border with Oregon .", "output": ["The city lies at the confluence of the Snake River and the Great Weiser River , which marks the border with Oregon ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The city sits at the confluence of the Snake River with the great Weiser River , which marks the border with Oregon .\nOutput: ", "bloom_target": "The city lies at the confluence of the Snake River and the Great Weiser River , which marks the border with Oregon .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The city sits at the confluence of the Snake River with the great Weiser River , which marks the border with Oregon .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-ca4cd486d83c4b7986fb081536b2ddb8", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-ca4cd486d83c4b7986fb081536b2ddb8", "input": "The Argentine government remained neutral until the last days of war , but quietly tolerated the entry of Nazi leaders who fled Germany , Belgium , and Vichy - France in 1945 .", "output": ["The Argentine government remained neutral until the last days of war , but quietly tolerated the entry of Nazi leaders fleeing from Vichy , France , Belgium , and Germany in 1945 ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The Argentine government remained neutral until the last days of war , but quietly tolerated the entry of Nazi leaders who fled Germany , Belgium , and Vichy - France in 1945 .\nOutput: ", "bloom_target": "The Argentine government remained neutral until the last days of war , but quietly tolerated the entry of Nazi leaders fleeing from Vichy , France , Belgium , and Germany in 1945 .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The Argentine government remained neutral until the last days of war , but quietly tolerated the entry of Nazi leaders who fled Germany , Belgium , and Vichy - France in 1945 .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-1dd35bea533e49fcb19abe78c146b409", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-1dd35bea533e49fcb19abe78c146b409", "input": "He moved to Quebec in 1685 and lived for some time in New - France .", "output": ["He moved to Quebec around 1685 and lived in New France for some time ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: He moved to Quebec in 1685 and lived for some time in New - France .\nOutput: ", "bloom_target": "He moved to Quebec around 1685 and lived in New France for some time .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: He moved to Quebec in 1685 and lived for some time in New - France .\nOutput:  He", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-87dde3c3b7a44e84b8f54cfc055f66d3", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-87dde3c3b7a44e84b8f54cfc055f66d3", "input": "The friendship between him and Duncan ended at a club meeting in 1951 when the two disagreed at an annual meeting and Duncan reported that Greaves said :", "output": ["The friendship between him and Duncan ended in 1951 at a club meeting , when the two did not agree at an annual meeting , and Duncan reported that Greaves said :"]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The friendship between him and Duncan ended at a club meeting in 1951 when the two disagreed at an annual meeting and Duncan reported that Greaves said :\nOutput: ", "bloom_target": "The friendship between him and Duncan ended in 1951 at a club meeting , when the two did not agree at an annual meeting , and Duncan reported that Greaves said :", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The friendship between him and Duncan ended at a club meeting in 1951 when the two disagreed at an annual meeting and Duncan reported that Greaves said :\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-27df9ba59ecf4f32ac1a5797338e73c9", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-27df9ba59ecf4f32ac1a5797338e73c9", "input": "The film was filmed in Red Rock Canyon State Park ( California ) in Cantil , California .", "output": ["The film was shot in California ( Red Rock Canyon State Park ) in Cantil , California ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The film was filmed in Red Rock Canyon State Park ( California ) in Cantil , California .\nOutput: ", "bloom_target": "The film was shot in California ( Red Rock Canyon State Park ) in Cantil , California .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The film was filmed in Red Rock Canyon State Park ( California ) in Cantil , California .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-29949bcf98d04aad94cfc5b7677dbcc5", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-29949bcf98d04aad94cfc5b7677dbcc5", "input": "Hector Crawford was the brother of 3DB manager and administrator Curteis Crawford , and also brother to Dorothy Crawford .", "output": ["Hector Crawford was brother of 3DB manager and administrator Curteis Crawford , and also brother of Dorothy Crawford ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Hector Crawford was the brother of 3DB manager and administrator Curteis Crawford , and also brother to Dorothy Crawford .\nOutput: ", "bloom_target": "Hector Crawford was brother of 3DB manager and administrator Curteis Crawford , and also brother of Dorothy Crawford .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Hector Crawford was the brother of 3DB manager and administrator Curteis Crawford , and also brother to Dorothy Crawford .\nOutput:  Hector", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-7e17f4c7ed2846c3acbce29cc6ba871d", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-7e17f4c7ed2846c3acbce29cc6ba871d", "input": "He was a scholar in Metaphysical Literature , Theology and Classical sciences .", "output": ["He was a scholar in metaphysical literature , theology , and classical science ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: He was a scholar in Metaphysical Literature , Theology and Classical sciences .\nOutput: ", "bloom_target": "He was a scholar in metaphysical literature , theology , and classical science .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: He was a scholar in Metaphysical Literature , Theology and Classical sciences .\nOutput:  He", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-020cfd10c5564cca96d85a5849a74597", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-020cfd10c5564cca96d85a5849a74597", "input": "Although five issues of the series were printed , the project was finished without any of them being aborted .", "output": ["Although five issues of the series were printed , the project was finished without any of them being cancelled ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Although five issues of the series were printed , the project was finished without any of them being aborted .\nOutput: ", "bloom_target": "Although five issues of the series were printed , the project was finished without any of them being cancelled .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Although five issues of the series were printed , the project was finished without any of them being aborted .\nOutput:  Although", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-6da78b67f9e1495cb87940c9286f656f", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-6da78b67f9e1495cb87940c9286f656f", "input": "Gimnasia y Esgrima ( LP ) won 3 -- 2 and stayed in the Primera Divisi\u00f3n .", "output": ["Gimnasia y Esgrima ( LP ) won with 3 -- 2 and stayed in the Primera Divisi\u00f3n ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Gimnasia y Esgrima ( LP ) won 3 -- 2 and stayed in the Primera Divisi\u00f3n .\nOutput: ", "bloom_target": "Gimnasia y Esgrima ( LP ) won with 3 -- 2 and stayed in the Primera Divisi\u00f3n .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Gimnasia y Esgrima ( LP ) won 3 -- 2 and stayed in the Primera Divisi\u00f3n .\nOutput:  Gimnas", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-61b7593cdc6f4cb497c105bddd6154f6", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-61b7593cdc6f4cb497c105bddd6154f6", "input": "There were also 58 liaison aircraft but 20 of these were only used for messengers .", "output": ["There were also 58 connecting aircraft , but 20 of these were used for messengers only ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: There were also 58 liaison aircraft but 20 of these were only used for messengers .\nOutput: ", "bloom_target": "There were also 58 connecting aircraft , but 20 of these were used for messengers only .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: There were also 58 liaison aircraft but 20 of these were only used for messengers .\nOutput:  There", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-0247e716ac0944a0ae15e076f446a383", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-0247e716ac0944a0ae15e076f446a383", "input": "Owen believed his utopian community would create a \" social environment \" based on his ideals of superior social , intellectual and physical reform .", "output": ["Owen believed that his utopian community would create a \" social environment , based on his ideals of superior social , intellectual , and physical reform ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Owen believed his utopian community would create a \" social environment \" based on his ideals of superior social , intellectual and physical reform .\nOutput: ", "bloom_target": "Owen believed that his utopian community would create a \" social environment , based on his ideals of superior social , intellectual , and physical reform .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Owen believed his utopian community would create a \" social environment \" based on his ideals of superior social , intellectual and physical reform .\nOutput:  Owen", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-242c516a0dd347ceb1b6660734636762", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-242c516a0dd347ceb1b6660734636762", "input": "Alycia Moulton defeated Billie Jean King at 6 -- 0 , 7 -- 5 .", "output": ["Alycia Moulton defeated Billie Jean King 6 -- 0 , 7 -- 5 ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Alycia Moulton defeated Billie Jean King at 6 -- 0 , 7 -- 5 .\nOutput: ", "bloom_target": "Alycia Moulton defeated Billie Jean King 6 -- 0 , 7 -- 5 .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Alycia Moulton defeated Billie Jean King at 6 -- 0 , 7 -- 5 .\nOutput:  A", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-06776c3d84074e91be13da8ee06d5609", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-06776c3d84074e91be13da8ee06d5609", "input": "Hastings Ndlovu was buried with Hector Pieterson at Avalon Cemetery in Johannesburg .", "output": ["Hastings Ndlovu , together with Hector Pieterson , was buried at the Avalon cemetery in Johannesburg ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Hastings Ndlovu was buried with Hector Pieterson at Avalon Cemetery in Johannesburg .\nOutput: ", "bloom_target": "Hastings Ndlovu , together with Hector Pieterson , was buried at the Avalon cemetery in Johannesburg .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Hastings Ndlovu was buried with Hector Pieterson at Avalon Cemetery in Johannesburg .\nOutput:  Hastings", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-fe2c07b7882c48f7b18cad92bf572773", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-fe2c07b7882c48f7b18cad92bf572773", "input": "In the following year , Butcher returned and was eliminated in round by Ian Rotten .", "output": ["Butcher returned the following year and was eliminated in round two by Ian Rotten ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: In the following year , Butcher returned and was eliminated in round by Ian Rotten .\nOutput: ", "bloom_target": "Butcher returned the following year and was eliminated in round two by Ian Rotten .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: In the following year , Butcher returned and was eliminated in round by Ian Rotten .\nOutput:  In", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-85e06fe0cb144f3fa8af99919806cbda", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-85e06fe0cb144f3fa8af99919806cbda", "input": "These geographically correspond to the traditional Glasgow District , South , Edinburgh District , and North and Midlands districts .", "output": ["These geographically roughly correspond to the traditional districts of Glasgow District , South , Edinburgh District and North and Midlands ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: These geographically correspond to the traditional Glasgow District , South , Edinburgh District , and North and Midlands districts .\nOutput: ", "bloom_target": "These geographically roughly correspond to the traditional districts of Glasgow District , South , Edinburgh District and North and Midlands .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: These geographically correspond to the traditional Glasgow District , South , Edinburgh District , and North and Midlands districts .\nOutput:  These", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-2bb7d1c4a4ea43c6bdc5e95af0611b6b", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-2bb7d1c4a4ea43c6bdc5e95af0611b6b", "input": "Laetitia Pujol was a shadowy presence as Le Homme ; Karl Paquette was his strong , melancholy double ; and Mathieu Ganio portrayed La Femme .", "output": ["Laetitia Pujol was a shady presence as Le Homme , Karl Paquette was his strong , melancholy double , and Mathieu Ganio portrayed La Femme ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Laetitia Pujol was a shadowy presence as Le Homme ; Karl Paquette was his strong , melancholy double ; and Mathieu Ganio portrayed La Femme .\nOutput: ", "bloom_target": "Laetitia Pujol was a shady presence as Le Homme , Karl Paquette was his strong , melancholy double , and Mathieu Ganio portrayed La Femme .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Laetitia Pujol was a shadowy presence as Le Homme ; Karl Paquette was his strong , melancholy double ; and Mathieu Ganio portrayed La Femme .\nOutput:  La", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-e27fa70c4f274da1b12dedee6fbb1e5e", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-e27fa70c4f274da1b12dedee6fbb1e5e", "input": "Here we view pseudo-differential operators as a generalization of differential operators .", "output": ["We consider pseudo-differential operators here as a generalization of differential operators ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Here we view pseudo-differential operators as a generalization of differential operators .\nOutput: ", "bloom_target": "We consider pseudo-differential operators here as a generalization of differential operators .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Here we view pseudo-differential operators as a generalization of differential operators .\nOutput:  Here", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-ef06963eec2d42fda4edbebff1261a14", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-ef06963eec2d42fda4edbebff1261a14", "input": "The SR 164 was commissioned from Youngstown to Salineville in 1923 .", "output": ["SR 164 was commissioned in 1923 , routed from Youngstown to Salineville ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The SR 164 was commissioned from Youngstown to Salineville in 1923 .\nOutput: ", "bloom_target": "SR 164 was commissioned in 1923 , routed from Youngstown to Salineville .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The SR 164 was commissioned from Youngstown to Salineville in 1923 .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-a36ba40d67fe444dbfa1de3a3c22f05c", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-a36ba40d67fe444dbfa1de3a3c22f05c", "input": "Briggs later met Briggs at the 1967 Monterey Pop Festival , where Ravi Shankar was also performing , with Eric Burdon and The Animals .", "output": ["Briggs met Briggs later at the Monterey Pop Festival of 1967 , where Ravi Shankar also performed with Eric Burdon and The Animals ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Briggs later met Briggs at the 1967 Monterey Pop Festival , where Ravi Shankar was also performing , with Eric Burdon and The Animals .\nOutput: ", "bloom_target": "Briggs met Briggs later at the Monterey Pop Festival of 1967 , where Ravi Shankar also performed with Eric Burdon and The Animals .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Briggs later met Briggs at the 1967 Monterey Pop Festival , where Ravi Shankar was also performing , with Eric Burdon and The Animals .\nOutput:  Brig", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-ac71dd869f0541f0a0af515a2ecb16e6", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-ac71dd869f0541f0a0af515a2ecb16e6", "input": "She plays for Naisten Liiga of \u00c5land United .", "output": ["She plays for Naisten Liiga of the \u00c5land United ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: She plays for Naisten Liiga of \u00c5land United .\nOutput: ", "bloom_target": "She plays for Naisten Liiga of the \u00c5land United .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: She plays for Naisten Liiga of \u00c5land United .\nOutput:  She", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-2aa25b012f3e4c31a6a02ecc893aed38", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-2aa25b012f3e4c31a6a02ecc893aed38", "input": "The Tweenies consist of Bella , Milo , Fizz , Jake , Doodles , Izzles , Max , Judy , and are sometimes joined by Max 's sister Polly .", "output": ["The tweenies consist of Bella , Milo , Fizz , Jake , Scribbles , Izzles , Max , Judy and are sometimes joined by Max 'apos ; Sister Polly ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The Tweenies consist of Bella , Milo , Fizz , Jake , Doodles , Izzles , Max , Judy , and are sometimes joined by Max 's sister Polly .\nOutput: ", "bloom_target": "The tweenies consist of Bella , Milo , Fizz , Jake , Scribbles , Izzles , Max , Judy and are sometimes joined by Max 'apos ; Sister Polly .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The Tweenies consist of Bella , Milo , Fizz , Jake , Doodles , Izzles , Max , Judy , and are sometimes joined by Max 's sister Polly .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-fd27ecf55a44409ba5f2406dd6f67a1f", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-fd27ecf55a44409ba5f2406dd6f67a1f", "input": "The company was then acquired the St. Louis and Cairo Railroad , the narrow gauge .", "output": ["The company then was the St. Louis and Cairo Railroad , which acquired narrow gauge ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The company was then acquired the St. Louis and Cairo Railroad , the narrow gauge .\nOutput: ", "bloom_target": "The company then was the St. Louis and Cairo Railroad , which acquired narrow gauge .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The company was then acquired the St. Louis and Cairo Railroad , the narrow gauge .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-2f5f86e361f744909abe59032d9347e5", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-2f5f86e361f744909abe59032d9347e5", "input": "In February 2016 , Souray married the former WWE - professional - wrestler Barbara Blank , better known as Kelly Kelly , who separated in October 2017 .", "output": ["Souray married former WWE professional wrestler Barbara Blank , better known as Kelly Kelly in February 2016 . They have separated in October 2017 ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: In February 2016 , Souray married the former WWE - professional - wrestler Barbara Blank , better known as Kelly Kelly , who separated in October 2017 .\nOutput: ", "bloom_target": "Souray married former WWE professional wrestler Barbara Blank , better known as Kelly Kelly in February 2016 . They have separated in October 2017 .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: In February 2016 , Souray married the former WWE - professional - wrestler Barbara Blank , better known as Kelly Kelly , who separated in October 2017 .\nOutput:  In", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-6543cc7f918c4d708289579b1af1b196", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-6543cc7f918c4d708289579b1af1b196", "input": "The leaves are generally 1.5-4 mm long and 0.2-0.7 mm wide .", "output": ["The leaves are usually 1.5-4 mm long and 0.2-0.7 mm wide ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The leaves are generally 1.5-4 mm long and 0.2-0.7 mm wide .\nOutput: ", "bloom_target": "The leaves are usually 1.5-4 mm long and 0.2-0.7 mm wide .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The leaves are generally 1.5-4 mm long and 0.2-0.7 mm wide .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-bdfef1c900cf43fcabdff47c026595f0", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-bdfef1c900cf43fcabdff47c026595f0", "input": "Amish Mennonites of Swiss descent from Galicia settled near Dubno in 1815 .", "output": ["Amish Mennonites from Galicia with Swiss descent settled in 1815 near Dubno ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Amish Mennonites of Swiss descent from Galicia settled near Dubno in 1815 .\nOutput: ", "bloom_target": "Amish Mennonites from Galicia with Swiss descent settled in 1815 near Dubno .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Amish Mennonites of Swiss descent from Galicia settled near Dubno in 1815 .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-7a616c1bf93f414ca5f372433ff4dfc5", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-7a616c1bf93f414ca5f372433ff4dfc5", "input": "His parents are Don Luis Toranzos , himself a prominent artist , and Angelina Miers from Argentina .", "output": ["His parents are Don Luis Toranzos , a prominent artist himself , and Angelina Miers , of Argentina ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: His parents are Don Luis Toranzos , himself a prominent artist , and Angelina Miers from Argentina .\nOutput: ", "bloom_target": "His parents are Don Luis Toranzos , a prominent artist himself , and Angelina Miers , of Argentina .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: His parents are Don Luis Toranzos , himself a prominent artist , and Angelina Miers from Argentina .\nOutput:  His", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-ab82db2c2e404bf1831ca95bd2791795", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-ab82db2c2e404bf1831ca95bd2791795", "input": "Several animatronic characters were also created ... a giant goose ( Galaga ) and an animatronic head for the puppeteered Cernos .", "output": ["There were also several animatronic characters created ... a giant goose ( Galaga ) and an animatronic head for the doll 's cernos ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Several animatronic characters were also created ... a giant goose ( Galaga ) and an animatronic head for the puppeteered Cernos .\nOutput: ", "bloom_target": "There were also several animatronic characters created ... a giant goose ( Galaga ) and an animatronic head for the doll 's cernos .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Several animatronic characters were also created ... a giant goose ( Galaga ) and an animatronic head for the puppeteered Cernos .\nOutput:  Several", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-965fab80e0d44f4997160804e03f91cf", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-965fab80e0d44f4997160804e03f91cf", "input": "WORHP , also referred to as eNLP ( European NLP Solver ) , is a mathematical software library for numerically solving continuous , nonlinear optimization problems on a large scale .", "output": ["WORHP , also referred to as eNLP ( European NLP solver ) by ESA , is a mathematical software library for solving continuous large scale nonlinear optimization problems numerically ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: WORHP , also referred to as eNLP ( European NLP Solver ) , is a mathematical software library for numerically solving continuous , nonlinear optimization problems on a large scale .\nOutput: ", "bloom_target": "WORHP , also referred to as eNLP ( European NLP solver ) by ESA , is a mathematical software library for solving continuous large scale nonlinear optimization problems numerically .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: WORHP , also referred to as eNLP ( European NLP Solver ) , is a mathematical software library for numerically solving continuous , nonlinear optimization problems on a large scale .\nOutput:  W", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-7dcdf152cc8946d98c32faf27aa60b9a", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-7dcdf152cc8946d98c32faf27aa60b9a", "input": "During their relationship the pair lived in Los Angeles , though Seymour spent more time in London and Los Angeles for her work .", "output": ["Throughout her relationship , the couple lived in Los Angeles , though Seymour spent more time in London and Los Angeles for their work ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: During their relationship the pair lived in Los Angeles , though Seymour spent more time in London and Los Angeles for her work .\nOutput: ", "bloom_target": "Throughout her relationship , the couple lived in Los Angeles , though Seymour spent more time in London and Los Angeles for their work .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: During their relationship the pair lived in Los Angeles , though Seymour spent more time in London and Los Angeles for her work .\nOutput:  During", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-fa516f2aff3640c88730cdc83f5f97d9", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-fa516f2aff3640c88730cdc83f5f97d9", "input": "He won the first Prix de Rome for painting in 1813 and the second Prix de Rome in 1814 for his painting of the \" Death of Diagoras \" .", "output": ["He won the first Prix de Rome for painting in 1813 and in 1814 the second Prix de Rome for his paintings of the \" death of the Diagoras \" ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: He won the first Prix de Rome for painting in 1813 and the second Prix de Rome in 1814 for his painting of the \" Death of Diagoras \" .\nOutput: ", "bloom_target": "He won the first Prix de Rome for painting in 1813 and in 1814 the second Prix de Rome for his paintings of the \" death of the Diagoras \" .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: He won the first Prix de Rome for painting in 1813 and the second Prix de Rome in 1814 for his painting of the \" Death of Diagoras \" .\nOutput:  He", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-aa130dced7ac4ae586e7a0fb96ad1960", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-aa130dced7ac4ae586e7a0fb96ad1960", "input": "Brockton is approximately 25 miles northeast of Providence , Rhode Island , and 30 miles south of Boston .", "output": ["Brockton is located approximately 25 miles northeast of Providence , Rhode Island and 30 miles south of Boston ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Brockton is approximately 25 miles northeast of Providence , Rhode Island , and 30 miles south of Boston .\nOutput: ", "bloom_target": "Brockton is located approximately 25 miles northeast of Providence , Rhode Island and 30 miles south of Boston .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Brockton is approximately 25 miles northeast of Providence , Rhode Island , and 30 miles south of Boston .\nOutput:  Brock", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-d07d7a6e73ac41a999d433bfc764157e", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-d07d7a6e73ac41a999d433bfc764157e", "input": "Lemmings , by contrast , are conspicuously colored and behave aggressively towards predators and even human observers .", "output": ["By contrast , the lemmings are strikingly colored and behave aggressively towards predators and even human observers ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Lemmings , by contrast , are conspicuously colored and behave aggressively towards predators and even human observers .\nOutput: ", "bloom_target": "By contrast , the lemmings are strikingly colored and behave aggressively towards predators and even human observers .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Lemmings , by contrast , are conspicuously colored and behave aggressively towards predators and even human observers .\nOutput:  Lem", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-bac9e7761542439aaba6ff76e53797b6", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-bac9e7761542439aaba6ff76e53797b6", "input": "Akshuat dendropark ( Russian : \u0410\u043a\u0448\u0443\u0430\u0442\u0441\u043a\u0438\u0439 \u0434\u0435\u043d\u0434\u0440\u043e\u043f\u0430\u0440\u043a ) is a natural monument ( Ulyanovsk Oblast protected areas )", "output": ["Akshuat dendropark ( Russian : \u0410\u043a\u0448\u0443\u0430\u0442\u0441\u043a\u0438\u0439 \u0434\u0435\u043d\u0434\u0440\u043e\u043f\u0430\u0440\u043a ) is a natural monument ( Protected areas of Ulyanovsk Oblast )"]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Akshuat dendropark ( Russian : \u0410\u043a\u0448\u0443\u0430\u0442\u0441\u043a\u0438\u0439 \u0434\u0435\u043d\u0434\u0440\u043e\u043f\u0430\u0440\u043a ) is a natural monument ( Ulyanovsk Oblast protected areas )\nOutput: ", "bloom_target": "Akshuat dendropark ( Russian : \u0410\u043a\u0448\u0443\u0430\u0442\u0441\u043a\u0438\u0439 \u0434\u0435\u043d\u0434\u0440\u043e\u043f\u0430\u0440\u043a ) is a natural monument ( Protected areas of Ulyanovsk Oblast )", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Akshuat dendropark ( Russian : \u0410\u043a\u0448\u0443\u0430\u0442\u0441\u043a\u0438\u0439 \u0434\u0435\u043d\u0434\u0440\u043e\u043f\u0430\u0440\u043a ) is a natural monument ( Ulyanovsk Oblast protected areas )\nOutput:  Ak", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task1608-a883ba3b97974293ba3f0884e4ec4612", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-a883ba3b97974293ba3f0884e4ec4612", "input": "Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: Who did the Broncos defeat in the AFC Championship game?", "output": ["New England Patriots"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: Who did the Broncos defeat in the AFC Championship game?\nOutput: ", "bloom_target": "New England Patriots", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: Who did the Broncos defeat in the AFC Championship game?\nOutput:  The", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-25f2ac698e6e48699f8310741f55a930", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-25f2ac698e6e48699f8310741f55a930", "input": "Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: Who had five sacks in nine games as a Carolina Panthers starter?", "output": ["Kony Ealy"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: Who had five sacks in nine games as a Carolina Panthers starter?\nOutput: ", "bloom_target": "Kony Ealy", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: Who had five sacks in nine games as a Carolina Panthers starter?\nOutput:  the", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-964bcad2af2f44fb8ba40e4839a0b7c0", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-964bcad2af2f44fb8ba40e4839a0b7c0", "input": "Passage: Their local rivals, Polonia Warsaw, have significantly fewer supporters, yet they managed to win Ekstraklasa Championship in 2000. They also won the country\u2019s championship in 1946, and won the cup twice as well. Polonia's home venue is located at Konwiktorska Street, a ten-minute walk north from the Old Town. Polonia was relegated from the country's top flight in 2013 because of their disastrous financial situation. They are now playing in the 4th league (5th tier in Poland) -the bottom professional league in the National \u2013 Polish Football Association (PZPN) structure. Question: Why was Polonia relegated from the country's top flight in 2013?", "output": ["disastrous financial situation"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Their local rivals, Polonia Warsaw, have significantly fewer supporters, yet they managed to win Ekstraklasa Championship in 2000. They also won the country\u2019s championship in 1946, and won the cup twice as well. Polonia's home venue is located at Konwiktorska Street, a ten-minute walk north from the Old Town. Polonia was relegated from the country's top flight in 2013 because of their disastrous financial situation. They are now playing in the 4th league (5th tier in Poland) -the bottom professional league in the National \u2013 Polish Football Association (PZPN) structure. Question: Why was Polonia relegated from the country's top flight in 2013?\nOutput: ", "bloom_target": "disastrous financial situation", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Their local rivals, Polonia Warsaw, have significantly fewer supporters, yet they managed to win Ekstraklasa Championship in 2000. They also won the country\u2019s championship in 1946, and won the cup twice as well. Polonia's home venue is located at Konwiktorska Street, a ten-minute walk north from the Old Town. Polonia was relegated from the country's top flight in 2013 because of their disastrous financial situation. They are now playing in the 4th league (5th tier in Poland) -the bottom professional league in the National \u2013 Polish Football Association (PZPN) structure. Question: Why was Polonia relegated from the country's top flight in 2013?\nOutput:  The", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-9348f66b88334998a75e7b12dacad724", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-9348f66b88334998a75e7b12dacad724", "input": "Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: How many interceptions did Josh Norman score touchdowns with in 2015?", "output": ["two."]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: How many interceptions did Josh Norman score touchdowns with in 2015?\nOutput: ", "bloom_target": "two.", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: How many interceptions did Josh Norman score touchdowns with in 2015?\nOutput:  4", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-1ddca988627b42b6a548ef31377109f3", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-1ddca988627b42b6a548ef31377109f3", "input": "Passage: The two most prominent Norman families to arrive in the Mediterranean were descendants of Tancred of Hauteville and the Drengot family, of whom Rainulf Drengot received the county of Aversa, the first Norman toehold in the south, from Duke Sergius IV of Naples in 1030. The Hauteville family achieved princely rank by proclaiming prince Guaimar IV of Salerno 'Duke of Apulia and Calabria'. He promptly awarded their elected leader, William Iron Arm, with the title of count in his capital of Melfi. The Drengot family thereafter attained the principality of Capua, and emperor Henry III legally ennobled the Hauteville leader, Drogo, as 'dux et magister Italiae comesque Normannorum totius Apuliae et Calabriae' ('Duke and Master of Italy and Count of the Normans of all Apulia and Calabria') in 1047. Question: Who was Count of Melfi", "output": ["William Iron Arm"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The two most prominent Norman families to arrive in the Mediterranean were descendants of Tancred of Hauteville and the Drengot family, of whom Rainulf Drengot received the county of Aversa, the first Norman toehold in the south, from Duke Sergius IV of Naples in 1030. The Hauteville family achieved princely rank by proclaiming prince Guaimar IV of Salerno 'Duke of Apulia and Calabria'. He promptly awarded their elected leader, William Iron Arm, with the title of count in his capital of Melfi. The Drengot family thereafter attained the principality of Capua, and emperor Henry III legally ennobled the Hauteville leader, Drogo, as 'dux et magister Italiae comesque Normannorum totius Apuliae et Calabriae' ('Duke and Master of Italy and Count of the Normans of all Apulia and Calabria') in 1047. Question: Who was Count of Melfi.\nOutput: ", "bloom_target": "William Iron Arm", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The two most prominent Norman families to arrive in the Mediterranean were descendants of Tancred of Hauteville and the Drengot family, of whom Rainulf Drengot received the county of Aversa, the first Norman toehold in the south, from Duke Sergius IV of Naples in 1030. The Hauteville family achieved princely rank by proclaiming prince Guaimar IV of Salerno 'Duke of Apulia and Calabria'. He promptly awarded their elected leader, William Iron Arm, with the title of count in his capital of Melfi. The Drengot family thereafter attained the principality of Capua, and emperor Henry III legally ennobled the Hauteville leader, Drogo, as 'dux et magister Italiae comesque Normannorum totius Apuliae et Calabriae' ('Duke and Master of Italy and Count of the Normans of all Apulia and Calabria') in 1047. Question: Who was Count of Melfi.\nOutput:  Count", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-50dc9b1fa4b44944995841c7c49ba70c", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-50dc9b1fa4b44944995841c7c49ba70c", "input": "Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: How many 2015 season interceptions did the Panthers' defense get?", "output": ["24"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: How many 2015 season interceptions did the Panthers' defense get?\nOutput: ", "bloom_target": "24", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: How many 2015 season interceptions did the Panthers' defense get?\nOutput:  4", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-172201d9c5a54425af038cab558615d9", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-172201d9c5a54425af038cab558615d9", "input": "Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: How many points did the Panthers defense surrender?", "output": ["308"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: How many points did the Panthers defense surrender?\nOutput: ", "bloom_target": "308", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: How many points did the Panthers defense surrender?\nOutput:  308", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-0d04fb3ab2f44889b79f448f80c800fa", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-0d04fb3ab2f44889b79f448f80c800fa", "input": "Passage: Their local rivals, Polonia Warsaw, have significantly fewer supporters, yet they managed to win Ekstraklasa Championship in 2000. They also won the country\u2019s championship in 1946, and won the cup twice as well. Polonia's home venue is located at Konwiktorska Street, a ten-minute walk north from the Old Town. Polonia was relegated from the country's top flight in 2013 because of their disastrous financial situation. They are now playing in the 4th league (5th tier in Poland) -the bottom professional league in the National \u2013 Polish Football Association (PZPN) structure. Question: When did Polonia Warsaw win the country's championship prior to 2000?", "output": ["1946"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Their local rivals, Polonia Warsaw, have significantly fewer supporters, yet they managed to win Ekstraklasa Championship in 2000. They also won the country\u2019s championship in 1946, and won the cup twice as well. Polonia's home venue is located at Konwiktorska Street, a ten-minute walk north from the Old Town. Polonia was relegated from the country's top flight in 2013 because of their disastrous financial situation. They are now playing in the 4th league (5th tier in Poland) -the bottom professional league in the National \u2013 Polish Football Association (PZPN) structure. Question: When did Polonia Warsaw win the country's championship prior to 2000?\nOutput: ", "bloom_target": "1946", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Their local rivals, Polonia Warsaw, have significantly fewer supporters, yet they managed to win Ekstraklasa Championship in 2000. They also won the country\u2019s championship in 1946, and won the cup twice as well. Polonia's home venue is located at Konwiktorska Street, a ten-minute walk north from the Old Town. Polonia was relegated from the country's top flight in 2013 because of their disastrous financial situation. They are now playing in the 4th league (5th tier in Poland) -the bottom professional league in the National \u2013 Polish Football Association (PZPN) structure. Question: When did Polonia Warsaw win the country's championship prior to 2000?\nOutput:  1946", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-7e4846a3aa0f4d3eb3a1c6ec46d2756a", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-7e4846a3aa0f4d3eb3a1c6ec46d2756a", "input": "Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: What team was the winner of Super Bowl XXXIII?", "output": ["Broncos"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: What team was the winner of Super Bowl XXXIII?\nOutput: ", "bloom_target": "Broncos", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: What team was the winner of Super Bowl XXXIII?\nOutput:  Denver", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-787ba3df0c054a4d92025bf7d319b012", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-787ba3df0c054a4d92025bf7d319b012", "input": "Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: How many points did the Broncos score in the final three minutes of the Pittsburgh game?", "output": ["11"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: How many points did the Broncos score in the final three minutes of the Pittsburgh game?\nOutput: ", "bloom_target": "11", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: How many points did the Broncos score in the final three minutes of the Pittsburgh game?\nOutput:  11", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-58e12f7499b543028f58ef375347bd6a", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-58e12f7499b543028f58ef375347bd6a", "input": "Passage: Warsaw's first stock exchange was established in 1817 and continued trading until World War II. It was re-established in April 1991, following the end of the post-war communist control of the country and the reintroduction of a free-market economy. Today, the Warsaw Stock Exchange (WSE) is, according to many indicators, the largest market in the region, with 374 companies listed and total capitalization of 162 584 mln EUR as of 31 August 2009. From 1991 until 2000, the stock exchange was, ironically, located in the building previously used as the headquarters of the Polish United Workers' Party (PZPR). Question: Whose former headquarters was the WSE located in until 2000?", "output": ["Polish United Workers' Party"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Warsaw's first stock exchange was established in 1817 and continued trading until World War II. It was re-established in April 1991, following the end of the post-war communist control of the country and the reintroduction of a free-market economy. Today, the Warsaw Stock Exchange (WSE) is, according to many indicators, the largest market in the region, with 374 companies listed and total capitalization of 162 584 mln EUR as of 31 August 2009. From 1991 until 2000, the stock exchange was, ironically, located in the building previously used as the headquarters of the Polish United Workers' Party (PZPR). Question: Whose former headquarters was the WSE located in until 2000?\nOutput: ", "bloom_target": "Polish United Workers' Party", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Warsaw's first stock exchange was established in 1817 and continued trading until World War II. It was re-established in April 1991, following the end of the post-war communist control of the country and the reintroduction of a free-market economy. Today, the Warsaw Stock Exchange (WSE) is, according to many indicators, the largest market in the region, with 374 companies listed and total capitalization of 162 584 mln EUR as of 31 August 2009. From 1991 until 2000, the stock exchange was, ironically, located in the building previously used as the headquarters of the Polish United Workers' Party (PZPR). Question: Whose former headquarters was the WSE located in until 2000?\nOutput:  The", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-f4754ab972a54741b14747f566c99205", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-f4754ab972a54741b14747f566c99205", "input": "Passage: Six-time Grammy winner and Academy Award nominee Lady Gaga performed the national anthem, while Academy Award winner Marlee Matlin provided American Sign Language (ASL) translation. Question: What actor did sign language for the National Anthem at Superbowl 50?", "output": ["Marlee Matlin"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Six-time Grammy winner and Academy Award nominee Lady Gaga performed the national anthem, while Academy Award winner Marlee Matlin provided American Sign Language (ASL) translation. Question: What actor did sign language for the National Anthem at Superbowl 50?\nOutput: ", "bloom_target": "Marlee Matlin", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Six-time Grammy winner and Academy Award nominee Lady Gaga performed the national anthem, while Academy Award winner Marlee Matlin provided American Sign Language (ASL) translation. Question: What actor did sign language for the National Anthem at Superbowl 50?\nOutput:  Lady", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-88d4f6c290b445a49b7ab1a678163e31", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-88d4f6c290b445a49b7ab1a678163e31", "input": "Passage: Six-time Grammy winner and Academy Award nominee Lady Gaga performed the national anthem, while Academy Award winner Marlee Matlin provided American Sign Language (ASL) translation. Question: How many Grammy Award's does the text say Lady Gaga has won?", "output": ["Six"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Six-time Grammy winner and Academy Award nominee Lady Gaga performed the national anthem, while Academy Award winner Marlee Matlin provided American Sign Language (ASL) translation. Question: How many Grammy Award's does the text say Lady Gaga has won?\nOutput: ", "bloom_target": "Six", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Six-time Grammy winner and Academy Award nominee Lady Gaga performed the national anthem, while Academy Award winner Marlee Matlin provided American Sign Language (ASL) translation. Question: How many Grammy Award's does the text say Lady Gaga has won?\nOutput:  6", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-bf93239afd2f46ce8fb16ec11d86c3a6", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-bf93239afd2f46ce8fb16ec11d86c3a6", "input": "Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: Who did John Elway play for in Super Bowl XXXIII?", "output": ["Broncos"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: Who did John Elway play for in Super Bowl XXXIII?\nOutput: ", "bloom_target": "Broncos", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: Who did John Elway play for in Super Bowl XXXIII?\nOutput:  Pey", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-dbb0ca0ec5e24d07b7e7744fc4906092", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-dbb0ca0ec5e24d07b7e7744fc4906092", "input": "Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: What was the time on the clock when Carolina got the ball to their 24-yard line in the fourth quarter?", "output": ["4:51"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: What was the time on the clock when Carolina got the ball to their 24-yard line in the fourth quarter?\nOutput: ", "bloom_target": "4:51", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: What was the time on the clock when Carolina got the ball to their 24-yard line in the fourth quarter?\nOutput:  4:", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-202f7062b1c146ecac6b05162e3f9c6c", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-202f7062b1c146ecac6b05162e3f9c6c", "input": "Passage: Nearby, in Ogr\u00f3d Saski (the Saxon Garden), the Summer Theatre was in operation from 1870 to 1939, and in the inter-war period, the theatre complex also included Momus, Warsaw's first literary cabaret, and Leon Schiller's musical theatre Melodram. The Wojciech Bogus\u0142awski Theatre (1922\u201326), was the best example of 'Polish monumental theatre'. From the mid-1930s, the Great Theatre building housed the Upati Institute of Dramatic Arts \u2013 the first state-run academy of dramatic art, with an acting department and a stage directing department. Question: How long was the Summer Theatre in operation?", "output": ["1870 to 1939"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Nearby, in Ogr\u00f3d Saski (the Saxon Garden), the Summer Theatre was in operation from 1870 to 1939, and in the inter-war period, the theatre complex also included Momus, Warsaw's first literary cabaret, and Leon Schiller's musical theatre Melodram. The Wojciech Bogus\u0142awski Theatre (1922\u201326), was the best example of 'Polish monumental theatre'. From the mid-1930s, the Great Theatre building housed the Upati Institute of Dramatic Arts \u2013 the first state-run academy of dramatic art, with an acting department and a stage directing department. Question: How long was the Summer Theatre in operation?\nOutput: ", "bloom_target": "1870 to 1939", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Nearby, in Ogr\u00f3d Saski (the Saxon Garden), the Summer Theatre was in operation from 1870 to 1939, and in the inter-war period, the theatre complex also included Momus, Warsaw's first literary cabaret, and Leon Schiller's musical theatre Melodram. The Wojciech Bogus\u0142awski Theatre (1922\u201326), was the best example of 'Polish monumental theatre'. From the mid-1930s, the Great Theatre building housed the Upati Institute of Dramatic Arts \u2013 the first state-run academy of dramatic art, with an acting department and a stage directing department. Question: How long was the Summer Theatre in operation?\nOutput:  1870", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-b81d7c57297c44e082ed90be0c5eb27f", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-b81d7c57297c44e082ed90be0c5eb27f", "input": "Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: How old was Elway during his Super Bowl XXXIII win?", "output": ["38"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: How old was Elway during his Super Bowl XXXIII win?\nOutput: ", "bloom_target": "38", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: How old was Elway during his Super Bowl XXXIII win?\nOutput:  39", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-5c65682bbe544eb19bfd10ac2580d7d5", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-5c65682bbe544eb19bfd10ac2580d7d5", "input": "Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: Who did the Broncos beat in the divisional game?", "output": ["Pittsburgh Steelers"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: Who did the Broncos beat in the divisional game?\nOutput: ", "bloom_target": "Pittsburgh Steelers", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: Who did the Broncos beat in the divisional game?\nOutput:  The", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-a3918a74f5cf41168bcde5579b5951e0", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-a3918a74f5cf41168bcde5579b5951e0", "input": "Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: How old was Manning when he played Super Bowl 50?", "output": ["39"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: How old was Manning when he played Super Bowl 50?\nOutput: ", "bloom_target": "39", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: How old was Manning when he played Super Bowl 50?\nOutput:  39", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-1794f6914a1e4f278d18e5455e4c08bc", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-1794f6914a1e4f278d18e5455e4c08bc", "input": "Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: What player punched the ball in from the 2?", "output": ["Anderson"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: What player punched the ball in from the 2?\nOutput: ", "bloom_target": "Anderson", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: What player punched the ball in from the 2?\nOutput:  Newton", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-ab7e9813710d4588b3253fe5406fa17f", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-ab7e9813710d4588b3253fe5406fa17f", "input": "Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: How many interceptions are the Panthers defense credited with in 2015?", "output": ["24"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: How many interceptions are the Panthers defense credited with in 2015?\nOutput: ", "bloom_target": "24", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: How many interceptions are the Panthers defense credited with in 2015?\nOutput:  308", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-fa51a8b16df8466f957453d7306a935e", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-fa51a8b16df8466f957453d7306a935e", "input": "Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: Who was the receiver on the successful 2-point conversion?", "output": ["Bennie Fowler"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: Who was the receiver on the successful 2-point conversion?\nOutput: ", "bloom_target": "Bennie Fowler", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: Who was the receiver on the successful 2-point conversion?\nOutput:  Newton", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-00d09253946942dfbf60f7231d8370e1", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-00d09253946942dfbf60f7231d8370e1", "input": "Passage: Six-time Grammy winner and Academy Award nominee Lady Gaga performed the national anthem, while Academy Award winner Marlee Matlin provided American Sign Language (ASL) translation. Question: Into what language did Marlee Matlin translate the national anthem?", "output": ["American Sign Language"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Six-time Grammy winner and Academy Award nominee Lady Gaga performed the national anthem, while Academy Award winner Marlee Matlin provided American Sign Language (ASL) translation. Question: Into what language did Marlee Matlin translate the national anthem?\nOutput: ", "bloom_target": "American Sign Language", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Six-time Grammy winner and Academy Award nominee Lady Gaga performed the national anthem, while Academy Award winner Marlee Matlin provided American Sign Language (ASL) translation. Question: Into what language did Marlee Matlin translate the national anthem?\nOutput:  AS", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-634198cd1f1a4cb18c17ba4d034f4124", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-634198cd1f1a4cb18c17ba4d034f4124", "input": "Passage: Six-time Grammy winner and Academy Award nominee Lady Gaga performed the national anthem, while Academy Award winner Marlee Matlin provided American Sign Language (ASL) translation. Question: What did Marlee Matlin translate?", "output": ["the national anthem"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Six-time Grammy winner and Academy Award nominee Lady Gaga performed the national anthem, while Academy Award winner Marlee Matlin provided American Sign Language (ASL) translation. Question: What did Marlee Matlin translate?\nOutput: ", "bloom_target": "the national anthem", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Six-time Grammy winner and Academy Award nominee Lady Gaga performed the national anthem, while Academy Award winner Marlee Matlin provided American Sign Language (ASL) translation. Question: What did Marlee Matlin translate?\nOutput:  America", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-2962ca3154544468806bdf03ca34073c", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-2962ca3154544468806bdf03ca34073c", "input": "Passage: Nearby, in Ogr\u00f3d Saski (the Saxon Garden), the Summer Theatre was in operation from 1870 to 1939, and in the inter-war period, the theatre complex also included Momus, Warsaw's first literary cabaret, and Leon Schiller's musical theatre Melodram. The Wojciech Bogus\u0142awski Theatre (1922\u201326), was the best example of 'Polish monumental theatre'. From the mid-1930s, the Great Theatre building housed the Upati Institute of Dramatic Arts \u2013 the first state-run academy of dramatic art, with an acting department and a stage directing department. Question: What is the Saxon Garden in Polish?", "output": ["Ogr\u00f3d Saski"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Nearby, in Ogr\u00f3d Saski (the Saxon Garden), the Summer Theatre was in operation from 1870 to 1939, and in the inter-war period, the theatre complex also included Momus, Warsaw's first literary cabaret, and Leon Schiller's musical theatre Melodram. The Wojciech Bogus\u0142awski Theatre (1922\u201326), was the best example of 'Polish monumental theatre'. From the mid-1930s, the Great Theatre building housed the Upati Institute of Dramatic Arts \u2013 the first state-run academy of dramatic art, with an acting department and a stage directing department. Question: What is the Saxon Garden in Polish?\nOutput: ", "bloom_target": "Ogr\u00f3d Saski", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Nearby, in Ogr\u00f3d Saski (the Saxon Garden), the Summer Theatre was in operation from 1870 to 1939, and in the inter-war period, the theatre complex also included Momus, Warsaw's first literary cabaret, and Leon Schiller's musical theatre Melodram. The Wojciech Bogus\u0142awski Theatre (1922\u201326), was the best example of 'Polish monumental theatre'. From the mid-1930s, the Great Theatre building housed the Upati Institute of Dramatic Arts \u2013 the first state-run academy of dramatic art, with an acting department and a stage directing department. Question: What is the Saxon Garden in Polish?\nOutput:  O", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-4cb4f7f868a94ada8546f39b6835bc0f", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-4cb4f7f868a94ada8546f39b6835bc0f", "input": "Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: What team was the divisional round winner between the Broncos and Steelers?", "output": ["Broncos"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: What team was the divisional round winner between the Broncos and Steelers?\nOutput: ", "bloom_target": "Broncos", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: What team was the divisional round winner between the Broncos and Steelers?\nOutput:  The", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-d6d678d6c7b84261b5a779cee3af8e65", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-d6d678d6c7b84261b5a779cee3af8e65", "input": "Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: How many Panthers defense players were selected for the Pro Bowl?", "output": ["four"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: How many Panthers defense players were selected for the Pro Bowl?\nOutput: ", "bloom_target": "four", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: How many Panthers defense players were selected for the Pro Bowl?\nOutput:  4", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-e4efa75f5564400ab37600e1d4709736", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-e4efa75f5564400ab37600e1d4709736", "input": "Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: Who lost to the Broncos in the divisional round?", "output": ["Pittsburgh Steelers"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: Who lost to the Broncos in the divisional round?\nOutput: ", "bloom_target": "Pittsburgh Steelers", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: Who lost to the Broncos in the divisional round?\nOutput:  The", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-5b2cc4e1f1dd470b8f1e6311c6bcd2ad", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-5b2cc4e1f1dd470b8f1e6311c6bcd2ad", "input": "Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: What Panther defender was called for holding on third down?", "output": ["Josh Norman"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: What Panther defender was called for holding on third down?\nOutput: ", "bloom_target": "Josh Norman", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: What Panther defender was called for holding on third down?\nOutput:  Newton", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-f5a11fa5c9c84d98a4754aa2a1aa9b4e", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-f5a11fa5c9c84d98a4754aa2a1aa9b4e", "input": "Passage: Six-time Grammy winner and Academy Award nominee Lady Gaga performed the national anthem, while Academy Award winner Marlee Matlin provided American Sign Language (ASL) translation. Question: Who did the National Anthem at Super Bowl 50?", "output": ["Lady Gaga"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Six-time Grammy winner and Academy Award nominee Lady Gaga performed the national anthem, while Academy Award winner Marlee Matlin provided American Sign Language (ASL) translation. Question: Who did the National Anthem at Super Bowl 50?\nOutput: ", "bloom_target": "Lady Gaga", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Six-time Grammy winner and Academy Award nominee Lady Gaga performed the national anthem, while Academy Award winner Marlee Matlin provided American Sign Language (ASL) translation. Question: Who did the National Anthem at Super Bowl 50?\nOutput:  Lady", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-385d7a0e55e845beb8fed74e0380a10f", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-385d7a0e55e845beb8fed74e0380a10f", "input": "Passage: Before Rollo's arrival, its populations did not differ from Picardy or the \u00cele-de-France, which were considered 'Frankish'. Earlier Viking settlers had begun arriving in the 880s, but were divided between colonies in the east (Roumois and Pays de Caux) around the low Seine valley and in the west in the Cotentin Peninsula, and were separated by traditional pagii, where the population remained about the same with almost no foreign settlers. Rollo's contingents who raided and ultimately settled Normandy and parts of the Atlantic coast included Danes, Norwegians, Norse\u2013Gaels, Orkney Vikings, possibly Swedes, and Anglo-Danes from the English Danelaw under Norse control. Question: Who upon arriving gave the original viking settlers a common identity?", "output": ["Rollo"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Before Rollo's arrival, its populations did not differ from Picardy or the \u00cele-de-France, which were considered 'Frankish'. Earlier Viking settlers had begun arriving in the 880s, but were divided between colonies in the east (Roumois and Pays de Caux) around the low Seine valley and in the west in the Cotentin Peninsula, and were separated by traditional pagii, where the population remained about the same with almost no foreign settlers. Rollo's contingents who raided and ultimately settled Normandy and parts of the Atlantic coast included Danes, Norwegians, Norse\u2013Gaels, Orkney Vikings, possibly Swedes, and Anglo-Danes from the English Danelaw under Norse control. Question: Who upon arriving gave the original viking settlers a common identity?\nOutput: ", "bloom_target": "Rollo", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Before Rollo's arrival, its populations did not differ from Picardy or the \u00cele-de-France, which were considered 'Frankish'. Earlier Viking settlers had begun arriving in the 880s, but were divided between colonies in the east (Roumois and Pays de Caux) around the low Seine valley and in the west in the Cotentin Peninsula, and were separated by traditional pagii, where the population remained about the same with almost no foreign settlers. Rollo's contingents who raided and ultimately settled Normandy and parts of the Atlantic coast included Danes, Norwegians, Norse\u2013Gaels, Orkney Vikings, possibly Swedes, and Anglo-Danes from the English Danelaw under Norse control. Question: Who upon arriving gave the original viking settlers a common identity?\nOutput:  The", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-c5df9c7c811d4cc28a7ed8bf95542a6d", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-c5df9c7c811d4cc28a7ed8bf95542a6d", "input": "Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: Which player was criticized for not jumping into the pile to recover the ball?", "output": ["Newton"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: Which player was criticized for not jumping into the pile to recover the ball?\nOutput: ", "bloom_target": "Newton", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: Which player was criticized for not jumping into the pile to recover the ball?\nOutput:  Newton", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-b4c89ab16e284b8f8ed50be75efe4b47", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-b4c89ab16e284b8f8ed50be75efe4b47", "input": "Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: Which Super Bowl did Elway win at 38 years old?", "output": ["Super Bowl XXXIII"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: Which Super Bowl did Elway win at 38 years old?\nOutput: ", "bloom_target": "Super Bowl XXXIII", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: Which Super Bowl did Elway win at 38 years old?\nOutput:  Super", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-0e1c1185f7034c8890c3c336bcb49f6f", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-0e1c1185f7034c8890c3c336bcb49f6f", "input": "Passage: Throughout its existence, Warsaw has been a multi-cultural city. According to the 1901 census, out of 711,988 inhabitants 56.2% were Catholics, 35.7% Jews, 5% Greek orthodox Christians and 2.8% Protestants. Eight years later, in 1909, there were 281,754 Jews (36.9%), 18,189 Protestants (2.4%) and 2,818 Mariavites (0.4%). This led to construction of hundreds of places of religious worship in all parts of the town. Most of them were destroyed in the aftermath of the Warsaw Uprising of 1944. After the war, the new communist authorities of Poland discouraged church construction and only a small number were rebuilt. Question: What was Warsaw's population in 1901?", "output": ["711,988"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Throughout its existence, Warsaw has been a multi-cultural city. According to the 1901 census, out of 711,988 inhabitants 56.2% were Catholics, 35.7% Jews, 5% Greek orthodox Christians and 2.8% Protestants. Eight years later, in 1909, there were 281,754 Jews (36.9%), 18,189 Protestants (2.4%) and 2,818 Mariavites (0.4%). This led to construction of hundreds of places of religious worship in all parts of the town. Most of them were destroyed in the aftermath of the Warsaw Uprising of 1944. After the war, the new communist authorities of Poland discouraged church construction and only a small number were rebuilt. Question: What was Warsaw's population in 1901?\nOutput: ", "bloom_target": "711,988", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Throughout its existence, Warsaw has been a multi-cultural city. According to the 1901 census, out of 711,988 inhabitants 56.2% were Catholics, 35.7% Jews, 5% Greek orthodox Christians and 2.8% Protestants. Eight years later, in 1909, there were 281,754 Jews (36.9%), 18,189 Protestants (2.4%) and 2,818 Mariavites (0.4%). This led to construction of hundreds of places of religious worship in all parts of the town. Most of them were destroyed in the aftermath of the Warsaw Uprising of 1944. After the war, the new communist authorities of Poland discouraged church construction and only a small number were rebuilt. Question: What was Warsaw's population in 1901?\nOutput:  56", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-546b062387f44f62ad2d9c5627e2554f", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-546b062387f44f62ad2d9c5627e2554f", "input": "Passage: The basic unit of territorial division in Poland is a commune (gmina). A city is also a commune \u2013 but with the city charter. Both cities and communes are governed by a mayor \u2013 but in the communes the mayor is vogt (w\u00f3jt in Polish), however in the cities \u2013 burmistrz. Some bigger cities obtain the entitlements, i.e. tasks and privileges, which are possessed by the units of the second level of the territorial division \u2013 counties or powiats. An example of such entitlement is a car registration: a gmina cannot register cars, this is a powiat's task (i.e. a registration number depends on what powiat a car had been registered, not gmina). In this case we say about city county or powiat grodzki. Such cities are for example Lublin, Krak\u00f3w, Gda\u0144sk, Pozna\u0144. In Warsaw, its districts additionally have some of powiat's entitlements \u2013 like already mentioned car registration. For example, the district Wola has its own evidence and the district Ursyn\u00f3w \u2013 its own (and the cars from Wola have another type of registration number than these from Ursyn\u00f3w). But for instance the districts in Krak\u00f3w do not have entitlements of powiat, so the registration numbers in Krak\u00f3w are of the same type for all districts. Question: What is the second level of territorial division in Poland?", "output": ["counties or powiats"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The basic unit of territorial division in Poland is a commune (gmina). A city is also a commune \u2013 but with the city charter. Both cities and communes are governed by a mayor \u2013 but in the communes the mayor is vogt (w\u00f3jt in Polish), however in the cities \u2013 burmistrz. Some bigger cities obtain the entitlements, i.e. tasks and privileges, which are possessed by the units of the second level of the territorial division \u2013 counties or powiats. An example of such entitlement is a car registration: a gmina cannot register cars, this is a powiat's task (i.e. a registration number depends on what powiat a car had been registered, not gmina). In this case we say about city county or powiat grodzki. Such cities are for example Lublin, Krak\u00f3w, Gda\u0144sk, Pozna\u0144. In Warsaw, its districts additionally have some of powiat's entitlements \u2013 like already mentioned car registration. For example, the district Wola has its own evidence and the district Ursyn\u00f3w \u2013 its own (and the cars from Wola have another type of registration number than these from Ursyn\u00f3w). But for instance the districts in Krak\u00f3w do not have entitlements of powiat, so the registration numbers in Krak\u00f3w are of the same type for all districts. Question: What is the second level of territorial division in Poland?\nOutput: ", "bloom_target": "counties or powiats", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The basic unit of territorial division in Poland is a commune (gmina). A city is also a commune \u2013 but with the city charter. Both cities and communes are governed by a mayor \u2013 but in the communes the mayor is vogt (w\u00f3jt in Polish), however in the cities \u2013 burmistrz. Some bigger cities obtain the entitlements, i.e. tasks and privileges, which are possessed by the units of the second level of the territorial division \u2013 counties or powiats. An example of such entitlement is a car registration: a gmina cannot register cars, this is a powiat's task (i.e. a registration number depends on what powiat a car had been registered, not gmina). In this case we say about city county or powiat grodzki. Such cities are for example Lublin, Krak\u00f3w, Gda\u0144sk, Pozna\u0144. In Warsaw, its districts additionally have some of powiat's entitlements \u2013 like already mentioned car registration. For example, the district Wola has its own evidence and the district Ursyn\u00f3w \u2013 its own (and the cars from Wola have another type of registration number than these from Ursyn\u00f3w). But for instance the districts in Krak\u00f3w do not have entitlements of powiat, so the registration numbers in Krak\u00f3w are of the same type for all districts. Question: What is the second level of territorial division in Poland?\nOutput:  County", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-ba59a8a68b144bd1824cb64db02edf9c", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-ba59a8a68b144bd1824cb64db02edf9c", "input": "Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: What is the last name of the player who scored a 2-yard touchdown in the fourth quarter?", "output": ["Anderson"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: What is the last name of the player who scored a 2-yard touchdown in the fourth quarter?\nOutput: ", "bloom_target": "Anderson", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: What is the last name of the player who scored a 2-yard touchdown in the fourth quarter?\nOutput:  Anderson", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-a2e0482bddbd40c4b11c97c057fc7fae", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-a2e0482bddbd40c4b11c97c057fc7fae", "input": "Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: How many seconds were left in the game when the Broncos intercepted the pass that won the game?", "output": ["17"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: How many seconds were left in the game when the Broncos intercepted the pass that won the game?\nOutput: ", "bloom_target": "17", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: How many seconds were left in the game when the Broncos intercepted the pass that won the game?\nOutput:  3", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-ef13a638a1f04e8ba2762d66def829d4", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-ef13a638a1f04e8ba2762d66def829d4", "input": "Passage: Six-time Grammy winner and Academy Award nominee Lady Gaga performed the national anthem, while Academy Award winner Marlee Matlin provided American Sign Language (ASL) translation. Question: Who sang the national anthem?", "output": ["Lady Gaga"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Six-time Grammy winner and Academy Award nominee Lady Gaga performed the national anthem, while Academy Award winner Marlee Matlin provided American Sign Language (ASL) translation. Question: Who sang the national anthem?\nOutput: ", "bloom_target": "Lady Gaga", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Six-time Grammy winner and Academy Award nominee Lady Gaga performed the national anthem, while Academy Award winner Marlee Matlin provided American Sign Language (ASL) translation. Question: Who sang the national anthem?\nOutput:  Lady", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-7718b1bc75384b1eabb54e577c672759", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-7718b1bc75384b1eabb54e577c672759", "input": "Passage: The two most prominent Norman families to arrive in the Mediterranean were descendants of Tancred of Hauteville and the Drengot family, of whom Rainulf Drengot received the county of Aversa, the first Norman toehold in the south, from Duke Sergius IV of Naples in 1030. The Hauteville family achieved princely rank by proclaiming prince Guaimar IV of Salerno 'Duke of Apulia and Calabria'. He promptly awarded their elected leader, William Iron Arm, with the title of count in his capital of Melfi. The Drengot family thereafter attained the principality of Capua, and emperor Henry III legally ennobled the Hauteville leader, Drogo, as 'dux et magister Italiae comesque Normannorum totius Apuliae et Calabriae' ('Duke and Master of Italy and Count of the Normans of all Apulia and Calabria') in 1047. Question: What was the name of the leader ennobled by Henry III", "output": ["Drogo"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The two most prominent Norman families to arrive in the Mediterranean were descendants of Tancred of Hauteville and the Drengot family, of whom Rainulf Drengot received the county of Aversa, the first Norman toehold in the south, from Duke Sergius IV of Naples in 1030. The Hauteville family achieved princely rank by proclaiming prince Guaimar IV of Salerno 'Duke of Apulia and Calabria'. He promptly awarded their elected leader, William Iron Arm, with the title of count in his capital of Melfi. The Drengot family thereafter attained the principality of Capua, and emperor Henry III legally ennobled the Hauteville leader, Drogo, as 'dux et magister Italiae comesque Normannorum totius Apuliae et Calabriae' ('Duke and Master of Italy and Count of the Normans of all Apulia and Calabria') in 1047. Question: What was the name of the leader ennobled by Henry III.\nOutput: ", "bloom_target": "Drogo", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The two most prominent Norman families to arrive in the Mediterranean were descendants of Tancred of Hauteville and the Drengot family, of whom Rainulf Drengot received the county of Aversa, the first Norman toehold in the south, from Duke Sergius IV of Naples in 1030. The Hauteville family achieved princely rank by proclaiming prince Guaimar IV of Salerno 'Duke of Apulia and Calabria'. He promptly awarded their elected leader, William Iron Arm, with the title of count in his capital of Melfi. The Drengot family thereafter attained the principality of Capua, and emperor Henry III legally ennobled the Hauteville leader, Drogo, as 'dux et magister Italiae comesque Normannorum totius Apuliae et Calabriae' ('Duke and Master of Italy and Count of the Normans of all Apulia and Calabria') in 1047. Question: What was the name of the leader ennobled by Henry III.\nOutput:  William", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-176a20c323904687994beac06376e0eb", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-176a20c323904687994beac06376e0eb", "input": "Passage: The basic unit of territorial division in Poland is a commune (gmina). A city is also a commune \u2013 but with the city charter. Both cities and communes are governed by a mayor \u2013 but in the communes the mayor is vogt (w\u00f3jt in Polish), however in the cities \u2013 burmistrz. Some bigger cities obtain the entitlements, i.e. tasks and privileges, which are possessed by the units of the second level of the territorial division \u2013 counties or powiats. An example of such entitlement is a car registration: a gmina cannot register cars, this is a powiat's task (i.e. a registration number depends on what powiat a car had been registered, not gmina). In this case we say about city county or powiat grodzki. Such cities are for example Lublin, Krak\u00f3w, Gda\u0144sk, Pozna\u0144. In Warsaw, its districts additionally have some of powiat's entitlements \u2013 like already mentioned car registration. For example, the district Wola has its own evidence and the district Ursyn\u00f3w \u2013 its own (and the cars from Wola have another type of registration number than these from Ursyn\u00f3w). But for instance the districts in Krak\u00f3w do not have entitlements of powiat, so the registration numbers in Krak\u00f3w are of the same type for all districts. Question: What is the basic unit of territorial division in Poland?", "output": ["commune"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The basic unit of territorial division in Poland is a commune (gmina). A city is also a commune \u2013 but with the city charter. Both cities and communes are governed by a mayor \u2013 but in the communes the mayor is vogt (w\u00f3jt in Polish), however in the cities \u2013 burmistrz. Some bigger cities obtain the entitlements, i.e. tasks and privileges, which are possessed by the units of the second level of the territorial division \u2013 counties or powiats. An example of such entitlement is a car registration: a gmina cannot register cars, this is a powiat's task (i.e. a registration number depends on what powiat a car had been registered, not gmina). In this case we say about city county or powiat grodzki. Such cities are for example Lublin, Krak\u00f3w, Gda\u0144sk, Pozna\u0144. In Warsaw, its districts additionally have some of powiat's entitlements \u2013 like already mentioned car registration. For example, the district Wola has its own evidence and the district Ursyn\u00f3w \u2013 its own (and the cars from Wola have another type of registration number than these from Ursyn\u00f3w). But for instance the districts in Krak\u00f3w do not have entitlements of powiat, so the registration numbers in Krak\u00f3w are of the same type for all districts. Question: What is the basic unit of territorial division in Poland?\nOutput: ", "bloom_target": "commune", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The basic unit of territorial division in Poland is a commune (gmina). A city is also a commune \u2013 but with the city charter. Both cities and communes are governed by a mayor \u2013 but in the communes the mayor is vogt (w\u00f3jt in Polish), however in the cities \u2013 burmistrz. Some bigger cities obtain the entitlements, i.e. tasks and privileges, which are possessed by the units of the second level of the territorial division \u2013 counties or powiats. An example of such entitlement is a car registration: a gmina cannot register cars, this is a powiat's task (i.e. a registration number depends on what powiat a car had been registered, not gmina). In this case we say about city county or powiat grodzki. Such cities are for example Lublin, Krak\u00f3w, Gda\u0144sk, Pozna\u0144. In Warsaw, its districts additionally have some of powiat's entitlements \u2013 like already mentioned car registration. For example, the district Wola has its own evidence and the district Ursyn\u00f3w \u2013 its own (and the cars from Wola have another type of registration number than these from Ursyn\u00f3w). But for instance the districts in Krak\u00f3w do not have entitlements of powiat, so the registration numbers in Krak\u00f3w are of the same type for all districts. Question: What is the basic unit of territorial division in Poland?\nOutput:  commune", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-cea97c1a9c454770bbfeb527952e1c09", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-cea97c1a9c454770bbfeb527952e1c09", "input": "Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: Who did the Broncos beat to win their division in 2015?", "output": ["Pittsburgh Steelers"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: Who did the Broncos beat to win their division in 2015?\nOutput: ", "bloom_target": "Pittsburgh Steelers", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: Who did the Broncos beat to win their division in 2015?\nOutput:  The", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-5c81707aca5e4adc8e27ea21a8d67a7d", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-5c81707aca5e4adc8e27ea21a8d67a7d", "input": "Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: Who was the Panthers' tackle leader for 2015?", "output": ["Luke Kuechly."]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: Who was the Panthers' tackle leader for 2015?\nOutput: ", "bloom_target": "Luke Kuechly.", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: Who was the Panthers' tackle leader for 2015?\nOutput:  the", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-69ea5d64431a4cec8b3ad359c5cc8954", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-69ea5d64431a4cec8b3ad359c5cc8954", "input": "Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: Who recovered the strip ball?", "output": ["Ward"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: Who recovered the strip ball?\nOutput: ", "bloom_target": "Ward", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: Who recovered the strip ball?\nOutput:  Carolina", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-f6e9812638df41018f7d729992a856a6", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-f6e9812638df41018f7d729992a856a6", "input": "Passage: Throughout its existence, Warsaw has been a multi-cultural city. According to the 1901 census, out of 711,988 inhabitants 56.2% were Catholics, 35.7% Jews, 5% Greek orthodox Christians and 2.8% Protestants. Eight years later, in 1909, there were 281,754 Jews (36.9%), 18,189 Protestants (2.4%) and 2,818 Mariavites (0.4%). This led to construction of hundreds of places of religious worship in all parts of the town. Most of them were destroyed in the aftermath of the Warsaw Uprising of 1944. After the war, the new communist authorities of Poland discouraged church construction and only a small number were rebuilt. Question: When were most of the places of religious worship destroyed in Warsaw?", "output": ["1944"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Throughout its existence, Warsaw has been a multi-cultural city. According to the 1901 census, out of 711,988 inhabitants 56.2% were Catholics, 35.7% Jews, 5% Greek orthodox Christians and 2.8% Protestants. Eight years later, in 1909, there were 281,754 Jews (36.9%), 18,189 Protestants (2.4%) and 2,818 Mariavites (0.4%). This led to construction of hundreds of places of religious worship in all parts of the town. Most of them were destroyed in the aftermath of the Warsaw Uprising of 1944. After the war, the new communist authorities of Poland discouraged church construction and only a small number were rebuilt. Question: When were most of the places of religious worship destroyed in Warsaw?\nOutput: ", "bloom_target": "1944", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Throughout its existence, Warsaw has been a multi-cultural city. According to the 1901 census, out of 711,988 inhabitants 56.2% were Catholics, 35.7% Jews, 5% Greek orthodox Christians and 2.8% Protestants. Eight years later, in 1909, there were 281,754 Jews (36.9%), 18,189 Protestants (2.4%) and 2,818 Mariavites (0.4%). This led to construction of hundreds of places of religious worship in all parts of the town. Most of them were destroyed in the aftermath of the Warsaw Uprising of 1944. After the war, the new communist authorities of Poland discouraged church construction and only a small number were rebuilt. Question: When were most of the places of religious worship destroyed in Warsaw?\nOutput:  The", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-b17269d616d343fb830754c9cf60c820", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-b17269d616d343fb830754c9cf60c820", "input": "Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: On what yard line did Carolina begin with 4:51 left in the game?", "output": ["24"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: On what yard line did Carolina begin with 4:51 left in the game?\nOutput: ", "bloom_target": "24", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: On what yard line did Carolina begin with 4:51 left in the game?\nOutput:  The", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-ce39b8ecf1314e5a90fea731d25349cd", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-ce39b8ecf1314e5a90fea731d25349cd", "input": "Passage: Nearby, in Ogr\u00f3d Saski (the Saxon Garden), the Summer Theatre was in operation from 1870 to 1939, and in the inter-war period, the theatre complex also included Momus, Warsaw's first literary cabaret, and Leon Schiller's musical theatre Melodram. The Wojciech Bogus\u0142awski Theatre (1922\u201326), was the best example of 'Polish monumental theatre'. From the mid-1930s, the Great Theatre building housed the Upati Institute of Dramatic Arts \u2013 the first state-run academy of dramatic art, with an acting department and a stage directing department. Question: Where was the Summer Theatre located?", "output": ["Saxon Garden"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Nearby, in Ogr\u00f3d Saski (the Saxon Garden), the Summer Theatre was in operation from 1870 to 1939, and in the inter-war period, the theatre complex also included Momus, Warsaw's first literary cabaret, and Leon Schiller's musical theatre Melodram. The Wojciech Bogus\u0142awski Theatre (1922\u201326), was the best example of 'Polish monumental theatre'. From the mid-1930s, the Great Theatre building housed the Upati Institute of Dramatic Arts \u2013 the first state-run academy of dramatic art, with an acting department and a stage directing department. Question: Where was the Summer Theatre located?\nOutput: ", "bloom_target": "Saxon Garden", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Nearby, in Ogr\u00f3d Saski (the Saxon Garden), the Summer Theatre was in operation from 1870 to 1939, and in the inter-war period, the theatre complex also included Momus, Warsaw's first literary cabaret, and Leon Schiller's musical theatre Melodram. The Wojciech Bogus\u0142awski Theatre (1922\u201326), was the best example of 'Polish monumental theatre'. From the mid-1930s, the Great Theatre building housed the Upati Institute of Dramatic Arts \u2013 the first state-run academy of dramatic art, with an acting department and a stage directing department. Question: Where was the Summer Theatre located?\nOutput:  O", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-2e80a5b490af409cac349cf2075d8a0f", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-2e80a5b490af409cac349cf2075d8a0f", "input": "Passage: Warsaw's first stock exchange was established in 1817 and continued trading until World War II. It was re-established in April 1991, following the end of the post-war communist control of the country and the reintroduction of a free-market economy. Today, the Warsaw Stock Exchange (WSE) is, according to many indicators, the largest market in the region, with 374 companies listed and total capitalization of 162 584 mln EUR as of 31 August 2009. From 1991 until 2000, the stock exchange was, ironically, located in the building previously used as the headquarters of the Polish United Workers' Party (PZPR). Question: How many companies were listed on the WSE on August 2009?", "output": ["374"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Warsaw's first stock exchange was established in 1817 and continued trading until World War II. It was re-established in April 1991, following the end of the post-war communist control of the country and the reintroduction of a free-market economy. Today, the Warsaw Stock Exchange (WSE) is, according to many indicators, the largest market in the region, with 374 companies listed and total capitalization of 162 584 mln EUR as of 31 August 2009. From 1991 until 2000, the stock exchange was, ironically, located in the building previously used as the headquarters of the Polish United Workers' Party (PZPR). Question: How many companies were listed on the WSE on August 2009?\nOutput: ", "bloom_target": "374", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Warsaw's first stock exchange was established in 1817 and continued trading until World War II. It was re-established in April 1991, following the end of the post-war communist control of the country and the reintroduction of a free-market economy. Today, the Warsaw Stock Exchange (WSE) is, according to many indicators, the largest market in the region, with 374 companies listed and total capitalization of 162 584 mln EUR as of 31 August 2009. From 1991 until 2000, the stock exchange was, ironically, located in the building previously used as the headquarters of the Polish United Workers' Party (PZPR). Question: How many companies were listed on the WSE on August 2009?\nOutput:  374", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-872a80280d0d42898875f479e157ed66", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-872a80280d0d42898875f479e157ed66", "input": "Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: Who is the General Manager for the Broncos?", "output": ["John Elway"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: Who is the General Manager for the Broncos?\nOutput: ", "bloom_target": "John Elway", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: Who is the General Manager for the Broncos?\nOutput:  Pey", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-d0d19cb42a7d4bfb9c741ff147fb436c", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-d0d19cb42a7d4bfb9c741ff147fb436c", "input": "Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: Who stripped the ball from Newton on a 3rd and nine?", "output": ["Miller"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: Who stripped the ball from Newton on a 3rd and nine?\nOutput: ", "bloom_target": "Miller", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: Who stripped the ball from Newton on a 3rd and nine?\nOutput:  Carolina", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-9c8e47911f584f4faea6fe4353449f74", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-9c8e47911f584f4faea6fe4353449f74", "input": "Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: How old was Peyton Manning when he played in Super Bowl 50?", "output": ["39"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: How old was Peyton Manning when he played in Super Bowl 50?\nOutput: ", "bloom_target": "39", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: How old was Peyton Manning when he played in Super Bowl 50?\nOutput:  39", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-291abaa9d5dd4e4ab9836cd5a8828b04", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-291abaa9d5dd4e4ab9836cd5a8828b04", "input": "Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: During the Bronco's playoff games, who did not throw at all?", "output": ["Manning"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: During the Bronco's playoff games, who did not throw at all?\nOutput: ", "bloom_target": "Manning", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: During the Bronco's playoff games, who did not throw at all?\nOutput:  M", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-0b60985d3d624ed19e7a13280eaad74c", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-0b60985d3d624ed19e7a13280eaad74c", "input": "Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: What is the name of the quarterback who was 38 in Super Bowl XXXIII?", "output": ["John Elway"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: What is the name of the quarterback who was 38 in Super Bowl XXXIII?\nOutput: ", "bloom_target": "John Elway", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: What is the name of the quarterback who was 38 in Super Bowl XXXIII?\nOutput:  Pey", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-49725652fc814600b92e7561e51c9c4d", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-49725652fc814600b92e7561e51c9c4d", "input": "Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: Who fumbled the ball on 3rd-and-9?", "output": ["Newton"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: Who fumbled the ball on 3rd-and-9?\nOutput: ", "bloom_target": "Newton", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: Who fumbled the ball on 3rd-and-9?\nOutput:  Carolina", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-63e24c1460c14725bea69e78fc8dd2aa", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-63e24c1460c14725bea69e78fc8dd2aa", "input": "Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: What role does John Elway currently have in the Broncos franchise?", "output": ["Executive Vice President of Football Operations and General Manager"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: What role does John Elway currently have in the Broncos franchise?\nOutput: ", "bloom_target": "Executive Vice President of Football Operations and General Manager", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: What role does John Elway currently have in the Broncos franchise?\nOutput:  Executive", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-716fee898c204ecf8c5935b8cd70303e", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-716fee898c204ecf8c5935b8cd70303e", "input": "Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: Who previously held the record for being the oldest quarterback to play in a Super Bowl?", "output": ["John Elway"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: Who previously held the record for being the oldest quarterback to play in a Super Bowl?\nOutput: ", "bloom_target": "John Elway", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: Who previously held the record for being the oldest quarterback to play in a Super Bowl?\nOutput:  Pey", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-6fd735a18ccc47e38f0d506603a4d219", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-6fd735a18ccc47e38f0d506603a4d219", "input": "Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: How many tackles did Luke Kuechly register?", "output": ["118"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: How many tackles did Luke Kuechly register?\nOutput: ", "bloom_target": "118", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: How many tackles did Luke Kuechly register?\nOutput:  4", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-f326ef0941d74421b534ea21b618d748", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-f326ef0941d74421b534ea21b618d748", "input": "Passage: Warsaw's first stock exchange was established in 1817 and continued trading until World War II. It was re-established in April 1991, following the end of the post-war communist control of the country and the reintroduction of a free-market economy. Today, the Warsaw Stock Exchange (WSE) is, according to many indicators, the largest market in the region, with 374 companies listed and total capitalization of 162 584 mln EUR as of 31 August 2009. From 1991 until 2000, the stock exchange was, ironically, located in the building previously used as the headquarters of the Polish United Workers' Party (PZPR). Question: When was Warsaw's stock exchange brought back to life?", "output": ["April 1991"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Warsaw's first stock exchange was established in 1817 and continued trading until World War II. It was re-established in April 1991, following the end of the post-war communist control of the country and the reintroduction of a free-market economy. Today, the Warsaw Stock Exchange (WSE) is, according to many indicators, the largest market in the region, with 374 companies listed and total capitalization of 162 584 mln EUR as of 31 August 2009. From 1991 until 2000, the stock exchange was, ironically, located in the building previously used as the headquarters of the Polish United Workers' Party (PZPR). Question: When was Warsaw's stock exchange brought back to life?\nOutput: ", "bloom_target": "April 1991", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Warsaw's first stock exchange was established in 1817 and continued trading until World War II. It was re-established in April 1991, following the end of the post-war communist control of the country and the reintroduction of a free-market economy. Today, the Warsaw Stock Exchange (WSE) is, according to many indicators, the largest market in the region, with 374 companies listed and total capitalization of 162 584 mln EUR as of 31 August 2009. From 1991 until 2000, the stock exchange was, ironically, located in the building previously used as the headquarters of the Polish United Workers' Party (PZPR). Question: When was Warsaw's stock exchange brought back to life?\nOutput:  1991", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-4e2e38aee923467bbf0315e55e1c7554", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-4e2e38aee923467bbf0315e55e1c7554", "input": "Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: How many balls did Josh Norman intercept?", "output": ["four"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: How many balls did Josh Norman intercept?\nOutput: ", "bloom_target": "four", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: How many balls did Josh Norman intercept?\nOutput:  4", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-20b6cfe4f889408a9a5f97f1b32efb78", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-20b6cfe4f889408a9a5f97f1b32efb78", "input": "Passage: Six-time Grammy winner and Academy Award nominee Lady Gaga performed the national anthem, while Academy Award winner Marlee Matlin provided American Sign Language (ASL) translation. Question: What did Lady Gaga sing?", "output": ["the national anthem"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Six-time Grammy winner and Academy Award nominee Lady Gaga performed the national anthem, while Academy Award winner Marlee Matlin provided American Sign Language (ASL) translation. Question: What did Lady Gaga sing?\nOutput: ", "bloom_target": "the national anthem", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Six-time Grammy winner and Academy Award nominee Lady Gaga performed the national anthem, while Academy Award winner Marlee Matlin provided American Sign Language (ASL) translation. Question: What did Lady Gaga sing?\nOutput:  The", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-67ea1ed30603473eb0b715b68b6b44e3", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-67ea1ed30603473eb0b715b68b6b44e3", "input": "Passage: Six-time Grammy winner and Academy Award nominee Lady Gaga performed the national anthem, while Academy Award winner Marlee Matlin provided American Sign Language (ASL) translation. Question: Who did the sign language of the National Anthem at Super Bowl 50?", "output": ["Marlee Matlin"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Six-time Grammy winner and Academy Award nominee Lady Gaga performed the national anthem, while Academy Award winner Marlee Matlin provided American Sign Language (ASL) translation. Question: Who did the sign language of the National Anthem at Super Bowl 50?\nOutput: ", "bloom_target": "Marlee Matlin", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Six-time Grammy winner and Academy Award nominee Lady Gaga performed the national anthem, while Academy Award winner Marlee Matlin provided American Sign Language (ASL) translation. Question: Who did the sign language of the National Anthem at Super Bowl 50?\nOutput:  Lady", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-f38fbcaebc9e46af8e16f4aaca8f455b", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-f38fbcaebc9e46af8e16f4aaca8f455b", "input": "Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: Which Panthers player got a penalty, which gave the Broncos a new set of downs?", "output": ["Josh Norman"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: Which Panthers player got a penalty, which gave the Broncos a new set of downs?\nOutput: ", "bloom_target": "Josh Norman", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: Which Panthers player got a penalty, which gave the Broncos a new set of downs?\nOutput:  Newton", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-29a6814a26234afaabf13e73aa817a5c", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-29a6814a26234afaabf13e73aa817a5c", "input": "Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: Who registered the most sacks on the team this season?", "output": ["Kawann Short"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: Who registered the most sacks on the team this season?\nOutput: ", "bloom_target": "Kawann Short", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: Who registered the most sacks on the team this season?\nOutput:  the", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-bbf29788ec39457bb554724410863e1e", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-bbf29788ec39457bb554724410863e1e", "input": "Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: Peyton Manning took how many different teams to the Super Bowl?", "output": ["two"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: Peyton Manning took how many different teams to the Super Bowl?\nOutput: ", "bloom_target": "two", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: Peyton Manning took how many different teams to the Super Bowl?\nOutput:  2", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-4c16384cff1a49b2af918f9a0aa8c173", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-4c16384cff1a49b2af918f9a0aa8c173", "input": "Passage: Soon after the Normans began to enter Italy, they entered the Byzantine Empire and then Armenia, fighting against the Pechenegs, the Bulgars, and especially the Seljuk Turks. Norman mercenaries were first encouraged to come to the south by the Lombards to act against the Byzantines, but they soon fought in Byzantine service in Sicily. They were prominent alongside Varangian and Lombard contingents in the Sicilian campaign of George Maniaces in 1038\u201340. There is debate whether the Normans in Greek service actually were from Norman Italy, and it now seems likely only a few came from there. It is also unknown how many of the 'Franks', as the Byzantines called them, were Normans and not other Frenchmen. Question: Who was the Normans' main enemy in Italy, the Byzantine Empire and Armenia?", "output": ["Seljuk Turks"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Soon after the Normans began to enter Italy, they entered the Byzantine Empire and then Armenia, fighting against the Pechenegs, the Bulgars, and especially the Seljuk Turks. Norman mercenaries were first encouraged to come to the south by the Lombards to act against the Byzantines, but they soon fought in Byzantine service in Sicily. They were prominent alongside Varangian and Lombard contingents in the Sicilian campaign of George Maniaces in 1038\u201340. There is debate whether the Normans in Greek service actually were from Norman Italy, and it now seems likely only a few came from there. It is also unknown how many of the 'Franks', as the Byzantines called them, were Normans and not other Frenchmen. Question: Who was the Normans' main enemy in Italy, the Byzantine Empire and Armenia?\nOutput: ", "bloom_target": "Seljuk Turks", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Soon after the Normans began to enter Italy, they entered the Byzantine Empire and then Armenia, fighting against the Pechenegs, the Bulgars, and especially the Seljuk Turks. Norman mercenaries were first encouraged to come to the south by the Lombards to act against the Byzantines, but they soon fought in Byzantine service in Sicily. They were prominent alongside Varangian and Lombard contingents in the Sicilian campaign of George Maniaces in 1038\u201340. There is debate whether the Normans in Greek service actually were from Norman Italy, and it now seems likely only a few came from there. It is also unknown how many of the 'Franks', as the Byzantines called them, were Normans and not other Frenchmen. Question: Who was the Normans' main enemy in Italy, the Byzantine Empire and Armenia?\nOutput:  The", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-542a7874d9764495af74fa61d2cca1e4", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-542a7874d9764495af74fa61d2cca1e4", "input": "Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: What is the last name of the player who finally recovered the ball Newton lost in the fourth quarter?", "output": ["Ward"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: What is the last name of the player who finally recovered the ball Newton lost in the fourth quarter?\nOutput: ", "bloom_target": "Ward", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: What is the last name of the player who finally recovered the ball Newton lost in the fourth quarter?\nOutput:  Newton", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-1c3e9be83aec49e788fd15baea47c3d5", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-1c3e9be83aec49e788fd15baea47c3d5", "input": "Passage: Throughout its existence, Warsaw has been a multi-cultural city. According to the 1901 census, out of 711,988 inhabitants 56.2% were Catholics, 35.7% Jews, 5% Greek orthodox Christians and 2.8% Protestants. Eight years later, in 1909, there were 281,754 Jews (36.9%), 18,189 Protestants (2.4%) and 2,818 Mariavites (0.4%). This led to construction of hundreds of places of religious worship in all parts of the town. Most of them were destroyed in the aftermath of the Warsaw Uprising of 1944. After the war, the new communist authorities of Poland discouraged church construction and only a small number were rebuilt. Question: What type of city has Warsaw been for as long as it's been a city?", "output": ["multi-cultural"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Throughout its existence, Warsaw has been a multi-cultural city. According to the 1901 census, out of 711,988 inhabitants 56.2% were Catholics, 35.7% Jews, 5% Greek orthodox Christians and 2.8% Protestants. Eight years later, in 1909, there were 281,754 Jews (36.9%), 18,189 Protestants (2.4%) and 2,818 Mariavites (0.4%). This led to construction of hundreds of places of religious worship in all parts of the town. Most of them were destroyed in the aftermath of the Warsaw Uprising of 1944. After the war, the new communist authorities of Poland discouraged church construction and only a small number were rebuilt. Question: What type of city has Warsaw been for as long as it's been a city?\nOutput: ", "bloom_target": "multi-cultural", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Throughout its existence, Warsaw has been a multi-cultural city. According to the 1901 census, out of 711,988 inhabitants 56.2% were Catholics, 35.7% Jews, 5% Greek orthodox Christians and 2.8% Protestants. Eight years later, in 1909, there were 281,754 Jews (36.9%), 18,189 Protestants (2.4%) and 2,818 Mariavites (0.4%). This led to construction of hundreds of places of religious worship in all parts of the town. Most of them were destroyed in the aftermath of the Warsaw Uprising of 1944. After the war, the new communist authorities of Poland discouraged church construction and only a small number were rebuilt. Question: What type of city has Warsaw been for as long as it's been a city?\nOutput:  a", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-e3883a7700744d2693cf15808f2207ae", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-e3883a7700744d2693cf15808f2207ae", "input": "Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: How much time was left in the game when Denver took the score to 24-10?", "output": ["3:08"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: How much time was left in the game when Denver took the score to 24-10?\nOutput: ", "bloom_target": "3:08", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: How much time was left in the game when Denver took the score to 24-10?\nOutput:  4:", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-b5d9fc4e2e1848a49aaaa2cc497e21e7", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-b5d9fc4e2e1848a49aaaa2cc497e21e7", "input": "Passage: Their local rivals, Polonia Warsaw, have significantly fewer supporters, yet they managed to win Ekstraklasa Championship in 2000. They also won the country\u2019s championship in 1946, and won the cup twice as well. Polonia's home venue is located at Konwiktorska Street, a ten-minute walk north from the Old Town. Polonia was relegated from the country's top flight in 2013 because of their disastrous financial situation. They are now playing in the 4th league (5th tier in Poland) -the bottom professional league in the National \u2013 Polish Football Association (PZPN) structure. Question: Who won the Ekstraklasa Championship in 2000?", "output": ["Polonia Warsaw"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Their local rivals, Polonia Warsaw, have significantly fewer supporters, yet they managed to win Ekstraklasa Championship in 2000. They also won the country\u2019s championship in 1946, and won the cup twice as well. Polonia's home venue is located at Konwiktorska Street, a ten-minute walk north from the Old Town. Polonia was relegated from the country's top flight in 2013 because of their disastrous financial situation. They are now playing in the 4th league (5th tier in Poland) -the bottom professional league in the National \u2013 Polish Football Association (PZPN) structure. Question: Who won the Ekstraklasa Championship in 2000?\nOutput: ", "bloom_target": "Polonia Warsaw", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Their local rivals, Polonia Warsaw, have significantly fewer supporters, yet they managed to win Ekstraklasa Championship in 2000. They also won the country\u2019s championship in 1946, and won the cup twice as well. Polonia's home venue is located at Konwiktorska Street, a ten-minute walk north from the Old Town. Polonia was relegated from the country's top flight in 2013 because of their disastrous financial situation. They are now playing in the 4th league (5th tier in Poland) -the bottom professional league in the National \u2013 Polish Football Association (PZPN) structure. Question: Who won the Ekstraklasa Championship in 2000?\nOutput:  Polonia", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-260dedc057814de6a06f6bf76ad42d0c", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-260dedc057814de6a06f6bf76ad42d0c", "input": "Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: Prior to Manning, who was the oldest quarterback to play in a Super Bowl?", "output": ["John Elway"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: Prior to Manning, who was the oldest quarterback to play in a Super Bowl?\nOutput: ", "bloom_target": "John Elway", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: Prior to Manning, who was the oldest quarterback to play in a Super Bowl?\nOutput:  Pey", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-a2c0dadfbcec46aa881a2085e6256676", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-a2c0dadfbcec46aa881a2085e6256676", "input": "Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: Which player had the most interceptions for the season?", "output": ["Kurt Coleman"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: Which player had the most interceptions for the season?\nOutput: ", "bloom_target": "Kurt Coleman", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: Which player had the most interceptions for the season?\nOutput:  Josh", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-0c1d197804bd4fc1ac290cd97347430f", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-0c1d197804bd4fc1ac290cd97347430f", "input": "Passage: Six-time Grammy winner and Academy Award nominee Lady Gaga performed the national anthem, while Academy Award winner Marlee Matlin provided American Sign Language (ASL) translation. Question: Who did the Super Bowl 50 National Anthem?", "output": ["Lady Gaga"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Six-time Grammy winner and Academy Award nominee Lady Gaga performed the national anthem, while Academy Award winner Marlee Matlin provided American Sign Language (ASL) translation. Question: Who did the Super Bowl 50 National Anthem?\nOutput: ", "bloom_target": "Lady Gaga", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Six-time Grammy winner and Academy Award nominee Lady Gaga performed the national anthem, while Academy Award winner Marlee Matlin provided American Sign Language (ASL) translation. Question: Who did the Super Bowl 50 National Anthem?\nOutput:  Lady", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-898a1643551a49f5a83501063ee6e3ca", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-898a1643551a49f5a83501063ee6e3ca", "input": "Passage: Nearby, in Ogr\u00f3d Saski (the Saxon Garden), the Summer Theatre was in operation from 1870 to 1939, and in the inter-war period, the theatre complex also included Momus, Warsaw's first literary cabaret, and Leon Schiller's musical theatre Melodram. The Wojciech Bogus\u0142awski Theatre (1922\u201326), was the best example of 'Polish monumental theatre'. From the mid-1930s, the Great Theatre building housed the Upati Institute of Dramatic Arts \u2013 the first state-run academy of dramatic art, with an acting department and a stage directing department. Question: What was Warsaw's first literary cabaret?", "output": ["Momus"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Nearby, in Ogr\u00f3d Saski (the Saxon Garden), the Summer Theatre was in operation from 1870 to 1939, and in the inter-war period, the theatre complex also included Momus, Warsaw's first literary cabaret, and Leon Schiller's musical theatre Melodram. The Wojciech Bogus\u0142awski Theatre (1922\u201326), was the best example of 'Polish monumental theatre'. From the mid-1930s, the Great Theatre building housed the Upati Institute of Dramatic Arts \u2013 the first state-run academy of dramatic art, with an acting department and a stage directing department. Question: What was Warsaw's first literary cabaret?\nOutput: ", "bloom_target": "Momus", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Nearby, in Ogr\u00f3d Saski (the Saxon Garden), the Summer Theatre was in operation from 1870 to 1939, and in the inter-war period, the theatre complex also included Momus, Warsaw's first literary cabaret, and Leon Schiller's musical theatre Melodram. The Wojciech Bogus\u0142awski Theatre (1922\u201326), was the best example of 'Polish monumental theatre'. From the mid-1930s, the Great Theatre building housed the Upati Institute of Dramatic Arts \u2013 the first state-run academy of dramatic art, with an acting department and a stage directing department. Question: What was Warsaw's first literary cabaret?\nOutput:  Mom", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-1205482293ee4613944810835e32d5ad", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-1205482293ee4613944810835e32d5ad", "input": "Passage: Nearby, in Ogr\u00f3d Saski (the Saxon Garden), the Summer Theatre was in operation from 1870 to 1939, and in the inter-war period, the theatre complex also included Momus, Warsaw's first literary cabaret, and Leon Schiller's musical theatre Melodram. The Wojciech Bogus\u0142awski Theatre (1922\u201326), was the best example of 'Polish monumental theatre'. From the mid-1930s, the Great Theatre building housed the Upati Institute of Dramatic Arts \u2013 the first state-run academy of dramatic art, with an acting department and a stage directing department. Question: What theatre was the best example of 'Polish monumental theatre'?", "output": ["Wojciech Bogus\u0142awski Theatre"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Nearby, in Ogr\u00f3d Saski (the Saxon Garden), the Summer Theatre was in operation from 1870 to 1939, and in the inter-war period, the theatre complex also included Momus, Warsaw's first literary cabaret, and Leon Schiller's musical theatre Melodram. The Wojciech Bogus\u0142awski Theatre (1922\u201326), was the best example of 'Polish monumental theatre'. From the mid-1930s, the Great Theatre building housed the Upati Institute of Dramatic Arts \u2013 the first state-run academy of dramatic art, with an acting department and a stage directing department. Question: What theatre was the best example of 'Polish monumental theatre'?\nOutput: ", "bloom_target": "Wojciech Bogus\u0142awski Theatre", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Nearby, in Ogr\u00f3d Saski (the Saxon Garden), the Summer Theatre was in operation from 1870 to 1939, and in the inter-war period, the theatre complex also included Momus, Warsaw's first literary cabaret, and Leon Schiller's musical theatre Melodram. The Wojciech Bogus\u0142awski Theatre (1922\u201326), was the best example of 'Polish monumental theatre'. From the mid-1930s, the Great Theatre building housed the Upati Institute of Dramatic Arts \u2013 the first state-run academy of dramatic art, with an acting department and a stage directing department. Question: What theatre was the best example of 'Polish monumental theatre'?\nOutput:  The", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-96b52b2cd40247b6a55c6271636906e9", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-96b52b2cd40247b6a55c6271636906e9", "input": "Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: Who was the first quarterback to take two teams to more than one Super Bowl?", "output": ["Peyton Manning"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: Who was the first quarterback to take two teams to more than one Super Bowl?\nOutput: ", "bloom_target": "Peyton Manning", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: Who was the first quarterback to take two teams to more than one Super Bowl?\nOutput:  Pey", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-88dae7f8ca6a4d799fa71b40a1ca04f2", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-88dae7f8ca6a4d799fa71b40a1ca04f2", "input": "Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: Who did the Broncos beat tp become the AFC champions?", "output": ["New England Patriots"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: Who did the Broncos beat tp become the AFC champions?\nOutput: ", "bloom_target": "New England Patriots", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: Who did the Broncos beat tp become the AFC champions?\nOutput:  The", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-ad95aaa5cb9f4988a7c5224708cf7bbe", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-ad95aaa5cb9f4988a7c5224708cf7bbe", "input": "Passage: Warsaw's first stock exchange was established in 1817 and continued trading until World War II. It was re-established in April 1991, following the end of the post-war communist control of the country and the reintroduction of a free-market economy. Today, the Warsaw Stock Exchange (WSE) is, according to many indicators, the largest market in the region, with 374 companies listed and total capitalization of 162 584 mln EUR as of 31 August 2009. From 1991 until 2000, the stock exchange was, ironically, located in the building previously used as the headquarters of the Polish United Workers' Party (PZPR). Question: When was Warsaw's first stock exchange established?", "output": ["1817"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Warsaw's first stock exchange was established in 1817 and continued trading until World War II. It was re-established in April 1991, following the end of the post-war communist control of the country and the reintroduction of a free-market economy. Today, the Warsaw Stock Exchange (WSE) is, according to many indicators, the largest market in the region, with 374 companies listed and total capitalization of 162 584 mln EUR as of 31 August 2009. From 1991 until 2000, the stock exchange was, ironically, located in the building previously used as the headquarters of the Polish United Workers' Party (PZPR). Question: When was Warsaw's first stock exchange established?\nOutput: ", "bloom_target": "1817", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Warsaw's first stock exchange was established in 1817 and continued trading until World War II. It was re-established in April 1991, following the end of the post-war communist control of the country and the reintroduction of a free-market economy. Today, the Warsaw Stock Exchange (WSE) is, according to many indicators, the largest market in the region, with 374 companies listed and total capitalization of 162 584 mln EUR as of 31 August 2009. From 1991 until 2000, the stock exchange was, ironically, located in the building previously used as the headquarters of the Polish United Workers' Party (PZPR). Question: When was Warsaw's first stock exchange established?\nOutput:  1817", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-b77971d096a745bc8207abb03bff4f9f", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-b77971d096a745bc8207abb03bff4f9f", "input": "Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: Who led the Panthers in sacks?", "output": ["Kawann Short"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: Who led the Panthers in sacks?\nOutput: ", "bloom_target": "Kawann Short", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: Who led the Panthers in sacks?\nOutput:  the", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-dec8114174a94140adca2e929e9e03d7", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-dec8114174a94140adca2e929e9e03d7", "input": "Passage: Six-time Grammy winner and Academy Award nominee Lady Gaga performed the national anthem, while Academy Award winner Marlee Matlin provided American Sign Language (ASL) translation. Question: What actress did the ASL translation for the game?", "output": ["Marlee Matlin"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Six-time Grammy winner and Academy Award nominee Lady Gaga performed the national anthem, while Academy Award winner Marlee Matlin provided American Sign Language (ASL) translation. Question: What actress did the ASL translation for the game?\nOutput: ", "bloom_target": "Marlee Matlin", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Six-time Grammy winner and Academy Award nominee Lady Gaga performed the national anthem, while Academy Award winner Marlee Matlin provided American Sign Language (ASL) translation. Question: What actress did the ASL translation for the game?\nOutput:  Lady", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-33601917992a41bfb0d1a02c59994157", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-33601917992a41bfb0d1a02c59994157", "input": "Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: How many teams has Manning played for that reached the Super Bowl, while he was on their team?", "output": ["two"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: How many teams has Manning played for that reached the Super Bowl, while he was on their team?\nOutput: ", "bloom_target": "two", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: How many teams has Manning played for that reached the Super Bowl, while he was on their team?\nOutput:  2", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-803677560a9f400e91c873d4048d1226", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-803677560a9f400e91c873d4048d1226", "input": "Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: Who is the oldest quarterback to play in a Super Bowl?", "output": ["Peyton Manning"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: Who is the oldest quarterback to play in a Super Bowl?\nOutput: ", "bloom_target": "Peyton Manning", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: Who is the oldest quarterback to play in a Super Bowl?\nOutput:  Pey", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-e3600cfa62e844b2848dfff336122226", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-e3600cfa62e844b2848dfff336122226", "input": "Passage: Throughout its existence, Warsaw has been a multi-cultural city. According to the 1901 census, out of 711,988 inhabitants 56.2% were Catholics, 35.7% Jews, 5% Greek orthodox Christians and 2.8% Protestants. Eight years later, in 1909, there were 281,754 Jews (36.9%), 18,189 Protestants (2.4%) and 2,818 Mariavites (0.4%). This led to construction of hundreds of places of religious worship in all parts of the town. Most of them were destroyed in the aftermath of the Warsaw Uprising of 1944. After the war, the new communist authorities of Poland discouraged church construction and only a small number were rebuilt. Question: What percentage of Warsaw's population was Protestant in 1901?", "output": ["2.8%"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Throughout its existence, Warsaw has been a multi-cultural city. According to the 1901 census, out of 711,988 inhabitants 56.2% were Catholics, 35.7% Jews, 5% Greek orthodox Christians and 2.8% Protestants. Eight years later, in 1909, there were 281,754 Jews (36.9%), 18,189 Protestants (2.4%) and 2,818 Mariavites (0.4%). This led to construction of hundreds of places of religious worship in all parts of the town. Most of them were destroyed in the aftermath of the Warsaw Uprising of 1944. After the war, the new communist authorities of Poland discouraged church construction and only a small number were rebuilt. Question: What percentage of Warsaw's population was Protestant in 1901?\nOutput: ", "bloom_target": "2.8%", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Throughout its existence, Warsaw has been a multi-cultural city. According to the 1901 census, out of 711,988 inhabitants 56.2% were Catholics, 35.7% Jews, 5% Greek orthodox Christians and 2.8% Protestants. Eight years later, in 1909, there were 281,754 Jews (36.9%), 18,189 Protestants (2.4%) and 2,818 Mariavites (0.4%). This led to construction of hundreds of places of religious worship in all parts of the town. Most of them were destroyed in the aftermath of the Warsaw Uprising of 1944. After the war, the new communist authorities of Poland discouraged church construction and only a small number were rebuilt. Question: What percentage of Warsaw's population was Protestant in 1901?\nOutput:  0", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-9441da3d6c0b465dae457c7a5c7656d8", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-9441da3d6c0b465dae457c7a5c7656d8", "input": "Passage: Their local rivals, Polonia Warsaw, have significantly fewer supporters, yet they managed to win Ekstraklasa Championship in 2000. They also won the country\u2019s championship in 1946, and won the cup twice as well. Polonia's home venue is located at Konwiktorska Street, a ten-minute walk north from the Old Town. Polonia was relegated from the country's top flight in 2013 because of their disastrous financial situation. They are now playing in the 4th league (5th tier in Poland) -the bottom professional league in the National \u2013 Polish Football Association (PZPN) structure. Question: Where is Polonia's home venue located?", "output": ["Konwiktorska Street"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Their local rivals, Polonia Warsaw, have significantly fewer supporters, yet they managed to win Ekstraklasa Championship in 2000. They also won the country\u2019s championship in 1946, and won the cup twice as well. Polonia's home venue is located at Konwiktorska Street, a ten-minute walk north from the Old Town. Polonia was relegated from the country's top flight in 2013 because of their disastrous financial situation. They are now playing in the 4th league (5th tier in Poland) -the bottom professional league in the National \u2013 Polish Football Association (PZPN) structure. Question: Where is Polonia's home venue located?\nOutput: ", "bloom_target": "Konwiktorska Street", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Their local rivals, Polonia Warsaw, have significantly fewer supporters, yet they managed to win Ekstraklasa Championship in 2000. They also won the country\u2019s championship in 1946, and won the cup twice as well. Polonia's home venue is located at Konwiktorska Street, a ten-minute walk north from the Old Town. Polonia was relegated from the country's top flight in 2013 because of their disastrous financial situation. They are now playing in the 4th league (5th tier in Poland) -the bottom professional league in the National \u2013 Polish Football Association (PZPN) structure. Question: Where is Polonia's home venue located?\nOutput:  Kon", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-b2ae4bafc0914d5caa89762deb64e8ca", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-b2ae4bafc0914d5caa89762deb64e8ca", "input": "Passage: Six-time Grammy winner and Academy Award nominee Lady Gaga performed the national anthem, while Academy Award winner Marlee Matlin provided American Sign Language (ASL) translation. Question: What award has Marlee Matlin won?", "output": ["Academy Award"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Six-time Grammy winner and Academy Award nominee Lady Gaga performed the national anthem, while Academy Award winner Marlee Matlin provided American Sign Language (ASL) translation. Question: What award has Marlee Matlin won?\nOutput: ", "bloom_target": "Academy Award", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Six-time Grammy winner and Academy Award nominee Lady Gaga performed the national anthem, while Academy Award winner Marlee Matlin provided American Sign Language (ASL) translation. Question: What award has Marlee Matlin won?\nOutput:  Grammy", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-4c38ec30f4b44d7284685541f8630059", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-4c38ec30f4b44d7284685541f8630059", "input": "Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: What was the final score of the AFC Championship Game?", "output": ["20\u201318"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: What was the final score of the AFC Championship Game?\nOutput: ", "bloom_target": "20\u201318", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: What was the final score of the AFC Championship Game?\nOutput:  The", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-80309c5ccdfe4874a0160c2a529b5846", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-80309c5ccdfe4874a0160c2a529b5846", "input": "Passage: The basic unit of territorial division in Poland is a commune (gmina). A city is also a commune \u2013 but with the city charter. Both cities and communes are governed by a mayor \u2013 but in the communes the mayor is vogt (w\u00f3jt in Polish), however in the cities \u2013 burmistrz. Some bigger cities obtain the entitlements, i.e. tasks and privileges, which are possessed by the units of the second level of the territorial division \u2013 counties or powiats. An example of such entitlement is a car registration: a gmina cannot register cars, this is a powiat's task (i.e. a registration number depends on what powiat a car had been registered, not gmina). In this case we say about city county or powiat grodzki. Such cities are for example Lublin, Krak\u00f3w, Gda\u0144sk, Pozna\u0144. In Warsaw, its districts additionally have some of powiat's entitlements \u2013 like already mentioned car registration. For example, the district Wola has its own evidence and the district Ursyn\u00f3w \u2013 its own (and the cars from Wola have another type of registration number than these from Ursyn\u00f3w). But for instance the districts in Krak\u00f3w do not have entitlements of powiat, so the registration numbers in Krak\u00f3w are of the same type for all districts. Question: In what districts are the registration numbers for cars all of the same type?", "output": ["Krak\u00f3w"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The basic unit of territorial division in Poland is a commune (gmina). A city is also a commune \u2013 but with the city charter. Both cities and communes are governed by a mayor \u2013 but in the communes the mayor is vogt (w\u00f3jt in Polish), however in the cities \u2013 burmistrz. Some bigger cities obtain the entitlements, i.e. tasks and privileges, which are possessed by the units of the second level of the territorial division \u2013 counties or powiats. An example of such entitlement is a car registration: a gmina cannot register cars, this is a powiat's task (i.e. a registration number depends on what powiat a car had been registered, not gmina). In this case we say about city county or powiat grodzki. Such cities are for example Lublin, Krak\u00f3w, Gda\u0144sk, Pozna\u0144. In Warsaw, its districts additionally have some of powiat's entitlements \u2013 like already mentioned car registration. For example, the district Wola has its own evidence and the district Ursyn\u00f3w \u2013 its own (and the cars from Wola have another type of registration number than these from Ursyn\u00f3w). But for instance the districts in Krak\u00f3w do not have entitlements of powiat, so the registration numbers in Krak\u00f3w are of the same type for all districts. Question: In what districts are the registration numbers for cars all of the same type?\nOutput: ", "bloom_target": "Krak\u00f3w", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The basic unit of territorial division in Poland is a commune (gmina). A city is also a commune \u2013 but with the city charter. Both cities and communes are governed by a mayor \u2013 but in the communes the mayor is vogt (w\u00f3jt in Polish), however in the cities \u2013 burmistrz. Some bigger cities obtain the entitlements, i.e. tasks and privileges, which are possessed by the units of the second level of the territorial division \u2013 counties or powiats. An example of such entitlement is a car registration: a gmina cannot register cars, this is a powiat's task (i.e. a registration number depends on what powiat a car had been registered, not gmina). In this case we say about city county or powiat grodzki. Such cities are for example Lublin, Krak\u00f3w, Gda\u0144sk, Pozna\u0144. In Warsaw, its districts additionally have some of powiat's entitlements \u2013 like already mentioned car registration. For example, the district Wola has its own evidence and the district Ursyn\u00f3w \u2013 its own (and the cars from Wola have another type of registration number than these from Ursyn\u00f3w). But for instance the districts in Krak\u00f3w do not have entitlements of powiat, so the registration numbers in Krak\u00f3w are of the same type for all districts. Question: In what districts are the registration numbers for cars all of the same type?\nOutput:  Kr", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-fe90e85cefa54f1fb78ae01b5847739f", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-fe90e85cefa54f1fb78ae01b5847739f", "input": "Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: Who has the record of being the oldest quarter back in any Super Bowl game?", "output": ["Peyton Manning"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: Who has the record of being the oldest quarter back in any Super Bowl game?\nOutput: ", "bloom_target": "Peyton Manning", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: Who has the record of being the oldest quarter back in any Super Bowl game?\nOutput:  Pey", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-6a3b8952a9f7444fb48bc0c69d3f00bc", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-6a3b8952a9f7444fb48bc0c69d3f00bc", "input": "Passage: Throughout its existence, Warsaw has been a multi-cultural city. According to the 1901 census, out of 711,988 inhabitants 56.2% were Catholics, 35.7% Jews, 5% Greek orthodox Christians and 2.8% Protestants. Eight years later, in 1909, there were 281,754 Jews (36.9%), 18,189 Protestants (2.4%) and 2,818 Mariavites (0.4%). This led to construction of hundreds of places of religious worship in all parts of the town. Most of them were destroyed in the aftermath of the Warsaw Uprising of 1944. After the war, the new communist authorities of Poland discouraged church construction and only a small number were rebuilt. Question: Of Warsaw's inhabitants in 1901, what percentage was Catholic?", "output": ["56.2%"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Throughout its existence, Warsaw has been a multi-cultural city. According to the 1901 census, out of 711,988 inhabitants 56.2% were Catholics, 35.7% Jews, 5% Greek orthodox Christians and 2.8% Protestants. Eight years later, in 1909, there were 281,754 Jews (36.9%), 18,189 Protestants (2.4%) and 2,818 Mariavites (0.4%). This led to construction of hundreds of places of religious worship in all parts of the town. Most of them were destroyed in the aftermath of the Warsaw Uprising of 1944. After the war, the new communist authorities of Poland discouraged church construction and only a small number were rebuilt. Question: Of Warsaw's inhabitants in 1901, what percentage was Catholic?\nOutput: ", "bloom_target": "56.2%", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Throughout its existence, Warsaw has been a multi-cultural city. According to the 1901 census, out of 711,988 inhabitants 56.2% were Catholics, 35.7% Jews, 5% Greek orthodox Christians and 2.8% Protestants. Eight years later, in 1909, there were 281,754 Jews (36.9%), 18,189 Protestants (2.4%) and 2,818 Mariavites (0.4%). This led to construction of hundreds of places of religious worship in all parts of the town. Most of them were destroyed in the aftermath of the Warsaw Uprising of 1944. After the war, the new communist authorities of Poland discouraged church construction and only a small number were rebuilt. Question: Of Warsaw's inhabitants in 1901, what percentage was Catholic?\nOutput:  56", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-5a154aa6f56e47edbee473dbbee4c72a", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-5a154aa6f56e47edbee473dbbee4c72a", "input": "Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: How many forced fumbles did Thomas Davis have?", "output": ["four"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: How many forced fumbles did Thomas Davis have?\nOutput: ", "bloom_target": "four", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: How many forced fumbles did Thomas Davis have?\nOutput:  4", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-c921d641bc2f4f329a61be3e6e285119", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-c921d641bc2f4f329a61be3e6e285119", "input": "Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: How many career sacks did Jared Allen have?", "output": ["136"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: How many career sacks did Jared Allen have?\nOutput: ", "bloom_target": "136", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: How many career sacks did Jared Allen have?\nOutput:  5", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-5f796b785e9b4c66ba0c32a495f6f2fe", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-5f796b785e9b4c66ba0c32a495f6f2fe", "input": "Passage: Their local rivals, Polonia Warsaw, have significantly fewer supporters, yet they managed to win Ekstraklasa Championship in 2000. They also won the country\u2019s championship in 1946, and won the cup twice as well. Polonia's home venue is located at Konwiktorska Street, a ten-minute walk north from the Old Town. Polonia was relegated from the country's top flight in 2013 because of their disastrous financial situation. They are now playing in the 4th league (5th tier in Poland) -the bottom professional league in the National \u2013 Polish Football Association (PZPN) structure. Question: How many times has Polonia won the cup?", "output": ["twice"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Their local rivals, Polonia Warsaw, have significantly fewer supporters, yet they managed to win Ekstraklasa Championship in 2000. They also won the country\u2019s championship in 1946, and won the cup twice as well. Polonia's home venue is located at Konwiktorska Street, a ten-minute walk north from the Old Town. Polonia was relegated from the country's top flight in 2013 because of their disastrous financial situation. They are now playing in the 4th league (5th tier in Poland) -the bottom professional league in the National \u2013 Polish Football Association (PZPN) structure. Question: How many times has Polonia won the cup?\nOutput: ", "bloom_target": "twice", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Their local rivals, Polonia Warsaw, have significantly fewer supporters, yet they managed to win Ekstraklasa Championship in 2000. They also won the country\u2019s championship in 1946, and won the cup twice as well. Polonia's home venue is located at Konwiktorska Street, a ten-minute walk north from the Old Town. Polonia was relegated from the country's top flight in 2013 because of their disastrous financial situation. They are now playing in the 4th league (5th tier in Poland) -the bottom professional league in the National \u2013 Polish Football Association (PZPN) structure. Question: How many times has Polonia won the cup?\nOutput:  3", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-6ce29694c0c34684924eee4d66937d90", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-6ce29694c0c34684924eee4d66937d90", "input": "Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: What was the last name of the player who took the ball away from Newton late in the fourth quarter?", "output": ["Miller"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: What was the last name of the player who took the ball away from Newton late in the fourth quarter?\nOutput: ", "bloom_target": "Miller", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: What was the last name of the player who took the ball away from Newton late in the fourth quarter?\nOutput:  Anderson", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-faf25f10d7ca4922a4a41ab19a4c63d8", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-faf25f10d7ca4922a4a41ab19a4c63d8", "input": "Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: Who won Super Bowl XLIX?", "output": ["New England Patriots"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: Who won Super Bowl XLIX?\nOutput: ", "bloom_target": "New England Patriots", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: Who won Super Bowl XLIX?\nOutput:  The", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-3b4e98abc5a2479a84c4fa21fd0b06e3", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-3b4e98abc5a2479a84c4fa21fd0b06e3", "input": "Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: How much time remained on the clock when the Broncos made the interception that clinched the AFC Championship Game?", "output": ["17 seconds"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: How much time remained on the clock when the Broncos made the interception that clinched the AFC Championship Game?\nOutput: ", "bloom_target": "17 seconds", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: How much time remained on the clock when the Broncos made the interception that clinched the AFC Championship Game?\nOutput:  3", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-739e5568c66046fd9e54c38e7134ca2c", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-739e5568c66046fd9e54c38e7134ca2c", "input": "Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: What was the final score of the game between the Broncos and Steelers?", "output": ["23\u201316"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: What was the final score of the game between the Broncos and Steelers?\nOutput: ", "bloom_target": "23\u201316", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: What was the final score of the game between the Broncos and Steelers?\nOutput:  23", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-970525e873c5489eb5593f125559f826", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-970525e873c5489eb5593f125559f826", "input": "Passage: Warsaw's first stock exchange was established in 1817 and continued trading until World War II. It was re-established in April 1991, following the end of the post-war communist control of the country and the reintroduction of a free-market economy. Today, the Warsaw Stock Exchange (WSE) is, according to many indicators, the largest market in the region, with 374 companies listed and total capitalization of 162 584 mln EUR as of 31 August 2009. From 1991 until 2000, the stock exchange was, ironically, located in the building previously used as the headquarters of the Polish United Workers' Party (PZPR). Question: What brought Warsaw's stock exchange to a stop?", "output": ["World War II"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Warsaw's first stock exchange was established in 1817 and continued trading until World War II. It was re-established in April 1991, following the end of the post-war communist control of the country and the reintroduction of a free-market economy. Today, the Warsaw Stock Exchange (WSE) is, according to many indicators, the largest market in the region, with 374 companies listed and total capitalization of 162 584 mln EUR as of 31 August 2009. From 1991 until 2000, the stock exchange was, ironically, located in the building previously used as the headquarters of the Polish United Workers' Party (PZPR). Question: What brought Warsaw's stock exchange to a stop?\nOutput: ", "bloom_target": "World War II", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Warsaw's first stock exchange was established in 1817 and continued trading until World War II. It was re-established in April 1991, following the end of the post-war communist control of the country and the reintroduction of a free-market economy. Today, the Warsaw Stock Exchange (WSE) is, according to many indicators, the largest market in the region, with 374 companies listed and total capitalization of 162 584 mln EUR as of 31 August 2009. From 1991 until 2000, the stock exchange was, ironically, located in the building previously used as the headquarters of the Polish United Workers' Party (PZPR). Question: What brought Warsaw's stock exchange to a stop?\nOutput:  The", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-d9e7089b50a0499e915f2f21bf16d328", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-d9e7089b50a0499e915f2f21bf16d328", "input": "Passage: Six-time Grammy winner and Academy Award nominee Lady Gaga performed the national anthem, while Academy Award winner Marlee Matlin provided American Sign Language (ASL) translation. Question: How many Grammys has Lady Gaga won?", "output": ["Six"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Six-time Grammy winner and Academy Award nominee Lady Gaga performed the national anthem, while Academy Award winner Marlee Matlin provided American Sign Language (ASL) translation. Question: How many Grammys has Lady Gaga won?\nOutput: ", "bloom_target": "Six", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Six-time Grammy winner and Academy Award nominee Lady Gaga performed the national anthem, while Academy Award winner Marlee Matlin provided American Sign Language (ASL) translation. Question: How many Grammys has Lady Gaga won?\nOutput:  6", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-b2dd0c77bbaf44188d44bb759c66c1f4", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-b2dd0c77bbaf44188d44bb759c66c1f4", "input": "Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: How many seconds were left in the game when the Patriots failed their 2-point conversion?", "output": ["17"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: How many seconds were left in the game when the Patriots failed their 2-point conversion?\nOutput: ", "bloom_target": "17", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: How many seconds were left in the game when the Patriots failed their 2-point conversion?\nOutput:  17", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-d883324cdf114f69a0c7d1add5f71534", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-d883324cdf114f69a0c7d1add5f71534", "input": "Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: How old was John Elway when he played in Super Bowl XXXIII?", "output": ["38"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: How old was John Elway when he played in Super Bowl XXXIII?\nOutput: ", "bloom_target": "38", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: How old was John Elway when he played in Super Bowl XXXIII?\nOutput:  39", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-3f7cefc5b67941c694632b9b4e8d3f27", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-3f7cefc5b67941c694632b9b4e8d3f27", "input": "Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: How many plays was Denver kept out of the end zone after getting the ball from Newton?", "output": ["three"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: How many plays was Denver kept out of the end zone after getting the ball from Newton?\nOutput: ", "bloom_target": "three", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: How many plays was Denver kept out of the end zone after getting the ball from Newton?\nOutput:  2", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-d9794ff9a9704ff4882bfe1dfc254565", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-d9794ff9a9704ff4882bfe1dfc254565", "input": "Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: How many points did the Broncos score in the last three minutes of the game versus Pittsburgh?", "output": ["11"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: How many points did the Broncos score in the last three minutes of the game versus Pittsburgh?\nOutput: ", "bloom_target": "11", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: How many points did the Broncos score in the last three minutes of the game versus Pittsburgh?\nOutput:  11", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1609-3f8d978bb5f84f358811d74ac5c21b4c", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-3f8d978bb5f84f358811d74ac5c21b4c", "input": "As previously arranged by his father, Tem\u00fcjin married B\u00f6rte of the Onggirat tribe when he was around 16 in order to cement alliances between their respective tribes. Soon after B\u00f6rte's marriage to Tem\u00fcjin, she was kidnapped by the Merkits and reportedly given away as a wife. Tem\u00fcjin rescued her with the help of his friend and future rival, Jamukha, and his protector, Toghrul Khan of the Keraite tribe. She gave birth to a son, Jochi (1185\u20131226), nine months later, clouding the issue of his parentage. Despite speculation over Jochi, B\u00f6rte would be Tem\u00fcjin's only empress, though he did follow tradition by taking several morganatic wives.", "output": ["What was the tribe of the woman Tem\u00fcjin married when he was around 16 years old?", "Who kidnapped Tem\u00fcjin's first wife soon after they were married?", "Who helped Tem\u00fcjin rescue his wife from the Merkits?", "What was the name of Tem\u00fcjin's wife B\u00f6rte's first son?", "What year did B\u00f6rte's give birth to Jochi?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: As previously arranged by his father, Tem\u00fcjin married B\u00f6rte of the Onggirat tribe when he was around 16 in order to cement alliances between their respective tribes. Soon after B\u00f6rte's marriage to Tem\u00fcjin, she was kidnapped by the Merkits and reportedly given away as a wife. Tem\u00fcjin rescued her with the help of his friend and future rival, Jamukha, and his protector, Toghrul Khan of the Keraite tribe. She gave birth to a son, Jochi (1185\u20131226), nine months later, clouding the issue of his parentage. Despite speculation over Jochi, B\u00f6rte would be Tem\u00fcjin's only empress, though he did follow tradition by taking several morganatic wives.\nOutput: ", "bloom_target": "Who helped Tem\u00fcjin rescue his wife from the Merkits?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: As previously arranged by his father, Tem\u00fcjin married B\u00f6rte of the Onggirat tribe when he was around 16 in order to cement alliances between their respective tribes. Soon after B\u00f6rte's marriage to Tem\u00fcjin, she was kidnapped by the Merkits and reportedly given away as a wife. Tem\u00fcjin rescued her with the help of his friend and future rival, Jamukha, and his protector, Toghrul Khan of the Keraite tribe. She gave birth to a son, Jochi (1185\u20131226), nine months later, clouding the issue of his parentage. Despite speculation over Jochi, B\u00f6rte would be Tem\u00fcjin's only empress, though he did follow tradition by taking several morganatic wives.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-775476f54f9041c2a73415f07d0ba569", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-775476f54f9041c2a73415f07d0ba569", "input": "Doctor Who has appeared on stage numerous times. In the early 1970s, Trevor Martin played the role in Doctor Who and the Daleks in the Seven Keys to Doomsday. In the late 1980s, Jon Pertwee and Colin Baker both played the Doctor at different times during the run of a play titled Doctor Who \u2013 The Ultimate Adventure. For two performances, while Pertwee was ill, David Banks (better known for playing Cybermen) played the Doctor. Other original plays have been staged as amateur productions, with other actors playing the Doctor, while Terry Nation wrote The Curse of the Daleks, a stage play mounted in the late 1960s, but without the Doctor.", "output": ["Who played Doctor Who on stage in the 70's?", "What was the name of the Doctor Who play from the 1980's?", "What Doctor Who was written without Doctor Who in it?", "What was the name of the play performed in the 1970's?", "Which actor was a replacement for Doctor Who due to the illness of the main actor?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Doctor Who has appeared on stage numerous times. In the early 1970s, Trevor Martin played the role in Doctor Who and the Daleks in the Seven Keys to Doomsday. In the late 1980s, Jon Pertwee and Colin Baker both played the Doctor at different times during the run of a play titled Doctor Who \u2013 The Ultimate Adventure. For two performances, while Pertwee was ill, David Banks (better known for playing Cybermen) played the Doctor. Other original plays have been staged as amateur productions, with other actors playing the Doctor, while Terry Nation wrote The Curse of the Daleks, a stage play mounted in the late 1960s, but without the Doctor.\nOutput: ", "bloom_target": "What was the name of the Doctor Who play from the 1980's?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Doctor Who has appeared on stage numerous times. In the early 1970s, Trevor Martin played the role in Doctor Who and the Daleks in the Seven Keys to Doomsday. In the late 1980s, Jon Pertwee and Colin Baker both played the Doctor at different times during the run of a play titled Doctor Who \u2013 The Ultimate Adventure. For two performances, while Pertwee was ill, David Banks (better known for playing Cybermen) played the Doctor. Other original plays have been staged as amateur productions, with other actors playing the Doctor, while Terry Nation wrote The Curse of the Daleks, a stage play mounted in the late 1960s, but without the Doctor.\nOutput:  Who", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-f1daafaa7115416b8a57a6a8903e842a", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-f1daafaa7115416b8a57a6a8903e842a", "input": "After Washington had returned to Williamsburg, Dinwiddie ordered him to lead a larger force to assist Trent in his work. While en route, Washington learned of Trent's retreat. Since Tanaghrisson had promised support to the British, Washington continued toward Fort Duquesne and met with the Mingo leader. Learning of a French scouting party in the area, Washington, with Tanaghrisson and his party, surprised the Canadians on May 28 in what became known as the Battle of Jumonville Glen. They killed many of the Canadians, including their commanding officer, Joseph Coulon de Jumonville, whose head was reportedly split open by Tanaghrisson with a tomahawk. The historian Fred Anderson suggests that Tanaghrisson was acting to gain the support of the British and regain authority over his own people. They had been inclined to support the French, with whom they had long trading relationships. One of Tanaghrisson's men told Contrecoeur that Jumonville had been killed by British musket fire.", "output": ["Upon learning of a French scounting party in the area, what did Washington do?", "What were casualties of battle?", "Why was Tanaghrisson supporting British efforts?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: After Washington had returned to Williamsburg, Dinwiddie ordered him to lead a larger force to assist Trent in his work. While en route, Washington learned of Trent's retreat. Since Tanaghrisson had promised support to the British, Washington continued toward Fort Duquesne and met with the Mingo leader. Learning of a French scouting party in the area, Washington, with Tanaghrisson and his party, surprised the Canadians on May 28 in what became known as the Battle of Jumonville Glen. They killed many of the Canadians, including their commanding officer, Joseph Coulon de Jumonville, whose head was reportedly split open by Tanaghrisson with a tomahawk. The historian Fred Anderson suggests that Tanaghrisson was acting to gain the support of the British and regain authority over his own people. They had been inclined to support the French, with whom they had long trading relationships. One of Tanaghrisson's men told Contrecoeur that Jumonville had been killed by British musket fire.\nOutput: ", "bloom_target": "What were casualties of battle?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: After Washington had returned to Williamsburg, Dinwiddie ordered him to lead a larger force to assist Trent in his work. While en route, Washington learned of Trent's retreat. Since Tanaghrisson had promised support to the British, Washington continued toward Fort Duquesne and met with the Mingo leader. Learning of a French scouting party in the area, Washington, with Tanaghrisson and his party, surprised the Canadians on May 28 in what became known as the Battle of Jumonville Glen. They killed many of the Canadians, including their commanding officer, Joseph Coulon de Jumonville, whose head was reportedly split open by Tanaghrisson with a tomahawk. The historian Fred Anderson suggests that Tanaghrisson was acting to gain the support of the British and regain authority over his own people. They had been inclined to support the French, with whom they had long trading relationships. One of Tanaghrisson's men told Contrecoeur that Jumonville had been killed by British musket fire.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-40903a8664214eddb1a173a46607b569", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-40903a8664214eddb1a173a46607b569", "input": "From 2005 to 2014, there were two Major League Soccer teams in Los Angeles \u2014 the LA Galaxy and Chivas USA \u2014 that both played at the StubHub Center and were local rivals. However, Chivas were suspended following the 2014 MLS season, with a second MLS team scheduled to return in 2018.", "output": ["Which team was suspended from the MLS?", "How many teams did Los Angeles used to have?", "Which year resulted in the suspension of one of the two soccer teams?", "What was the name of the stadium that the teams played in?", "When is the suspended team scheduled to return?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: From 2005 to 2014, there were two Major League Soccer teams in Los Angeles \u2014 the LA Galaxy and Chivas USA \u2014 that both played at the StubHub Center and were local rivals. However, Chivas were suspended following the 2014 MLS season, with a second MLS team scheduled to return in 2018.\nOutput: ", "bloom_target": "Which team was suspended from the MLS?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: From 2005 to 2014, there were two Major League Soccer teams in Los Angeles \u2014 the LA Galaxy and Chivas USA \u2014 that both played at the StubHub Center and were local rivals. However, Chivas were suspended following the 2014 MLS season, with a second MLS team scheduled to return in 2018.\nOutput:  Which", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-1c4c52a934244766903b1cf4ade8fc29", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-1c4c52a934244766903b1cf4ade8fc29", "input": "Consultant pharmacy practice focuses more on medication regimen review (i.e. 'cognitive services') than on actual dispensing of drugs. Consultant pharmacists most typically work in nursing homes, but are increasingly branching into other institutions and non-institutional settings. Traditionally consultant pharmacists were usually independent business owners, though in the United States many now work for several large pharmacy management companies (primarily Omnicare, Kindred Healthcare and PharMerica). This trend may be gradually reversing as consultant pharmacists begin to work directly with patients, primarily because many elderly people are now taking numerous medications but continue to live outside of institutional settings. Some community pharmacies employ consultant pharmacists and/or provide consulting services.", "output": ["What is consultant pharmacy mainly concerned with?", "Where do a majority of consultant pharmacists tend to work?", "What are some large pharmacy management companies?", "What is the main reason consulting pharmacists are increasingly working directly with patients?", "What do some community pharmacies do?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Consultant pharmacy practice focuses more on medication regimen review (i.e. 'cognitive services') than on actual dispensing of drugs. Consultant pharmacists most typically work in nursing homes, but are increasingly branching into other institutions and non-institutional settings. Traditionally consultant pharmacists were usually independent business owners, though in the United States many now work for several large pharmacy management companies (primarily Omnicare, Kindred Healthcare and PharMerica). This trend may be gradually reversing as consultant pharmacists begin to work directly with patients, primarily because many elderly people are now taking numerous medications but continue to live outside of institutional settings. Some community pharmacies employ consultant pharmacists and/or provide consulting services.\nOutput: ", "bloom_target": "What are some large pharmacy management companies?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Consultant pharmacy practice focuses more on medication regimen review (i.e. 'cognitive services') than on actual dispensing of drugs. Consultant pharmacists most typically work in nursing homes, but are increasingly branching into other institutions and non-institutional settings. Traditionally consultant pharmacists were usually independent business owners, though in the United States many now work for several large pharmacy management companies (primarily Omnicare, Kindred Healthcare and PharMerica). This trend may be gradually reversing as consultant pharmacists begin to work directly with patients, primarily because many elderly people are now taking numerous medications but continue to live outside of institutional settings. Some community pharmacies employ consultant pharmacists and/or provide consulting services.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-0d86017c410641bf88c0c17c043651ba", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-0d86017c410641bf88c0c17c043651ba", "input": "In past times, corporal punishment (spanking or paddling or caning or strapping or birching the student in order to cause physical pain) was one of the most common forms of school discipline throughout much of the world. Most Western countries, and some others, have now banned it, but it remains lawful in the United States following a US Supreme Court decision in 1977 which held that paddling did not violate the US Constitution.", "output": ["How common was the form of corporal punishment in the past?", "Where is corporal punishment no longer practiced?", "What Western country is corporal punishment still allowed?", "What U.S. entity said that corporal punishment was Constitutional?", "What does corporal punishment cause a student?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In past times, corporal punishment (spanking or paddling or caning or strapping or birching the student in order to cause physical pain) was one of the most common forms of school discipline throughout much of the world. Most Western countries, and some others, have now banned it, but it remains lawful in the United States following a US Supreme Court decision in 1977 which held that paddling did not violate the US Constitution.\nOutput: ", "bloom_target": "What Western country is corporal punishment still allowed?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In past times, corporal punishment (spanking or paddling or caning or strapping or birching the student in order to cause physical pain) was one of the most common forms of school discipline throughout much of the world. Most Western countries, and some others, have now banned it, but it remains lawful in the United States following a US Supreme Court decision in 1977 which held that paddling did not violate the US Constitution.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-cae19a4cec1841c0a9d604f737f02cf2", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-cae19a4cec1841c0a9d604f737f02cf2", "input": "Following the election of the UK Labour Party to government in 1997, the UK formally subscribed to the Agreement on Social Policy, which allowed it to be included with minor amendments as the Social Chapter of the 1997 Treaty of Amsterdam. The UK subsequently adopted the main legislation previously agreed under the Agreement on Social Policy, the 1994 Works Council Directive, which required workforce consultation in businesses, and the 1996 Parental Leave Directive. In the 10 years following the 1997 Treaty of Amsterdam and adoption of the Social Chapter the European Union has undertaken policy initiatives in various social policy areas, including labour and industry relations, equal opportunity, health and safety, public health, protection of children, the disabled and elderly, poverty, migrant workers, education, training and youth.", "output": ["What lead to the UK to subscribe to the agreement on Social Policy?", "When did the UK formally subscribe to the Agreement on Social Policy?", "Which directive mentioned was created in 1994?", "When was the Parental Leave directive created?", "What did the Works Council Directive require?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Following the election of the UK Labour Party to government in 1997, the UK formally subscribed to the Agreement on Social Policy, which allowed it to be included with minor amendments as the Social Chapter of the 1997 Treaty of Amsterdam. The UK subsequently adopted the main legislation previously agreed under the Agreement on Social Policy, the 1994 Works Council Directive, which required workforce consultation in businesses, and the 1996 Parental Leave Directive. In the 10 years following the 1997 Treaty of Amsterdam and adoption of the Social Chapter the European Union has undertaken policy initiatives in various social policy areas, including labour and industry relations, equal opportunity, health and safety, public health, protection of children, the disabled and elderly, poverty, migrant workers, education, training and youth.\nOutput: ", "bloom_target": "When did the UK formally subscribe to the Agreement on Social Policy?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Following the election of the UK Labour Party to government in 1997, the UK formally subscribed to the Agreement on Social Policy, which allowed it to be included with minor amendments as the Social Chapter of the 1997 Treaty of Amsterdam. The UK subsequently adopted the main legislation previously agreed under the Agreement on Social Policy, the 1994 Works Council Directive, which required workforce consultation in businesses, and the 1996 Parental Leave Directive. In the 10 years following the 1997 Treaty of Amsterdam and adoption of the Social Chapter the European Union has undertaken policy initiatives in various social policy areas, including labour and industry relations, equal opportunity, health and safety, public health, protection of children, the disabled and elderly, poverty, migrant workers, education, training and youth.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-c2d51e50e5b34d8eb78ea7ddd552f80e", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-c2d51e50e5b34d8eb78ea7ddd552f80e", "input": "In the final years of the apartheid era, parents at white government schools were given the option to convert to a 'semi-private' form called Model C, and many of these schools changed their admissions policies to accept children of other races. Following the transition to democracy, the legal form of 'Model C' was abolished, however, the term continues to be used to describe government schools formerly reserved for white children.. These schools tend to produce better academic results than government schools formerly reserved for other race groups . Former 'Model C' schools are not private schools, as they are state-controlled. All schools in South Africa (including both independent schools and public schools) have the right to set compulsory school fees, and formerly model C schools tend to set much higher school fees than other public schools.", "output": ["After apartheid, what types of schools are referred to as 'Model C' schools?", "How do academic results in former Model C schools compare to other schools?", "How do the fees at former Model C schools compare to those at other schools?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In the final years of the apartheid era, parents at white government schools were given the option to convert to a 'semi-private' form called Model C, and many of these schools changed their admissions policies to accept children of other races. Following the transition to democracy, the legal form of 'Model C' was abolished, however, the term continues to be used to describe government schools formerly reserved for white children.. These schools tend to produce better academic results than government schools formerly reserved for other race groups . Former 'Model C' schools are not private schools, as they are state-controlled. All schools in South Africa (including both independent schools and public schools) have the right to set compulsory school fees, and formerly model C schools tend to set much higher school fees than other public schools.\nOutput: ", "bloom_target": "How do the fees at former Model C schools compare to those at other schools?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In the final years of the apartheid era, parents at white government schools were given the option to convert to a 'semi-private' form called Model C, and many of these schools changed their admissions policies to accept children of other races. Following the transition to democracy, the legal form of 'Model C' was abolished, however, the term continues to be used to describe government schools formerly reserved for white children.. These schools tend to produce better academic results than government schools formerly reserved for other race groups . Former 'Model C' schools are not private schools, as they are state-controlled. All schools in South Africa (including both independent schools and public schools) have the right to set compulsory school fees, and formerly model C schools tend to set much higher school fees than other public schools.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-0f51412a7318439eb00983d64415f328", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-0f51412a7318439eb00983d64415f328", "input": "ABC dominated the American television landscape during the 1970s and early 1980s (by 1980, the three major networks represented 90% of all prime-time television viewership in the U.S.). Several flagship series debuted on the network during this time including Dynasty, an opulent drama from Aaron Spelling that became a hit when it premiered as a midseason series in 1981, five months before Spelling's other ABC hit Charlie's Angels ended its run. The network was also propelled during the early 1980s by the continued successes of Happy Days, Three's Company, Laverne & Shirley and Fantasy Island, and gained new hits in Too Close for Comfort, Soap spinoff Benson and Happy Days spinoff Mork & Mindy. In 1981, ABC (through its ABC Video Services division) launched the Alpha Repertory Television Service (ARTS), a cable channel operated as a joint venture with the Hearst Corporation offering cultural and arts programming, which aired as a nighttime service over the channel space of Nickelodeon.", "output": ["What percentage of prime-time TV viewership did the three big networks represent in 1980?", "What drama from Aaron Spelling debuted on ABC in the 80s?", "What was a Happy Days spinoff that debuted in the 1980s on ABC?", "What channel did ABC launch in 1981 that focused on cultural and arts programming?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: ABC dominated the American television landscape during the 1970s and early 1980s (by 1980, the three major networks represented 90% of all prime-time television viewership in the U.S.). Several flagship series debuted on the network during this time including Dynasty, an opulent drama from Aaron Spelling that became a hit when it premiered as a midseason series in 1981, five months before Spelling's other ABC hit Charlie's Angels ended its run. The network was also propelled during the early 1980s by the continued successes of Happy Days, Three's Company, Laverne & Shirley and Fantasy Island, and gained new hits in Too Close for Comfort, Soap spinoff Benson and Happy Days spinoff Mork & Mindy. In 1981, ABC (through its ABC Video Services division) launched the Alpha Repertory Television Service (ARTS), a cable channel operated as a joint venture with the Hearst Corporation offering cultural and arts programming, which aired as a nighttime service over the channel space of Nickelodeon.\nOutput: ", "bloom_target": "What percentage of prime-time TV viewership did the three big networks represent in 1980?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: ABC dominated the American television landscape during the 1970s and early 1980s (by 1980, the three major networks represented 90% of all prime-time television viewership in the U.S.). Several flagship series debuted on the network during this time including Dynasty, an opulent drama from Aaron Spelling that became a hit when it premiered as a midseason series in 1981, five months before Spelling's other ABC hit Charlie's Angels ended its run. The network was also propelled during the early 1980s by the continued successes of Happy Days, Three's Company, Laverne & Shirley and Fantasy Island, and gained new hits in Too Close for Comfort, Soap spinoff Benson and Happy Days spinoff Mork & Mindy. In 1981, ABC (through its ABC Video Services division) launched the Alpha Repertory Television Service (ARTS), a cable channel operated as a joint venture with the Hearst Corporation offering cultural and arts programming, which aired as a nighttime service over the channel space of Nickelodeon.\nOutput:  ABC", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-ecb6181189384bfaae513162210f8b98", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-ecb6181189384bfaae513162210f8b98", "input": "In Tibetan Buddhism the teachers of Dharma in Tibet are most commonly called a Lama. A Lama who has through phowa and siddhi consciously determined to be reborn, often many times, in order to continue their Bodhisattva vow is called a Tulku.", "output": ["What is the name of a teacher in Tibetan Buddhism?", "What has a Lama determined to do?", "What is the name of a Bodhisattva vow?", "How much has a Lama agreed to be reborn?", "What helped the Lama determine to be reborn?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In Tibetan Buddhism the teachers of Dharma in Tibet are most commonly called a Lama. A Lama who has through phowa and siddhi consciously determined to be reborn, often many times, in order to continue their Bodhisattva vow is called a Tulku.\nOutput: ", "bloom_target": "What is the name of a Bodhisattva vow?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In Tibetan Buddhism the teachers of Dharma in Tibet are most commonly called a Lama. A Lama who has through phowa and siddhi consciously determined to be reborn, often many times, in order to continue their Bodhisattva vow is called a Tulku.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-09b7c4a8c606464bab54879268518ffa", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-09b7c4a8c606464bab54879268518ffa", "input": "Hyperbaric (high-pressure) medicine uses special oxygen chambers to increase the partial pressure of O\n2 around the patient and, when needed, the medical staff. Carbon monoxide poisoning, gas gangrene, and decompression sickness (the 'bends') are sometimes treated using these devices. Increased O\n2 concentration in the lungs helps to displace carbon monoxide from the heme group of hemoglobin. Oxygen gas is poisonous to the anaerobic bacteria that cause gas gangrene, so increasing its partial pressure helps kill them. Decompression sickness occurs in divers who decompress too quickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and helium, forming in their blood. Increasing the pressure of O\n2 as soon as possible is part of the treatment.", "output": ["What device is used to treat various conditions such as carbon monoxide poisoning?", "What does increased oxygen concentrations in the patient's lungs displace?", "To what pathogen that causes gas gangrene is oxygen poisonous?", "What occurs after a dive in which a diver decompresses too quickly?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Hyperbaric (high-pressure) medicine uses special oxygen chambers to increase the partial pressure of O\n2 around the patient and, when needed, the medical staff. Carbon monoxide poisoning, gas gangrene, and decompression sickness (the 'bends') are sometimes treated using these devices. Increased O\n2 concentration in the lungs helps to displace carbon monoxide from the heme group of hemoglobin. Oxygen gas is poisonous to the anaerobic bacteria that cause gas gangrene, so increasing its partial pressure helps kill them. Decompression sickness occurs in divers who decompress too quickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and helium, forming in their blood. Increasing the pressure of O\n2 as soon as possible is part of the treatment.\nOutput: ", "bloom_target": "What does increased oxygen concentrations in the patient's lungs displace?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Hyperbaric (high-pressure) medicine uses special oxygen chambers to increase the partial pressure of O\n2 around the patient and, when needed, the medical staff. Carbon monoxide poisoning, gas gangrene, and decompression sickness (the 'bends') are sometimes treated using these devices. Increased O\n2 concentration in the lungs helps to displace carbon monoxide from the heme group of hemoglobin. Oxygen gas is poisonous to the anaerobic bacteria that cause gas gangrene, so increasing its partial pressure helps kill them. Decompression sickness occurs in divers who decompress too quickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and helium, forming in their blood. Increasing the pressure of O\n2 as soon as possible is part of the treatment.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-8d3108345cde48e5ae75e637952ad3ba", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-8d3108345cde48e5ae75e637952ad3ba", "input": "Between 1402 and 1405, the expedition led by the Norman noble Jean de Bethencourt and the Poitevine Gadifer de la Salle conquered the Canarian islands of Lanzarote, Fuerteventura and El Hierro off the Atlantic coast of Africa. Their troops were gathered in Normandy, Gascony and were later reinforced by Castilian colonists.", "output": ["What continent are the Canarian Islands off the coast of?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Between 1402 and 1405, the expedition led by the Norman noble Jean de Bethencourt and the Poitevine Gadifer de la Salle conquered the Canarian islands of Lanzarote, Fuerteventura and El Hierro off the Atlantic coast of Africa. Their troops were gathered in Normandy, Gascony and were later reinforced by Castilian colonists.\nOutput: ", "bloom_target": "What continent are the Canarian Islands off the coast of?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Between 1402 and 1405, the expedition led by the Norman noble Jean de Bethencourt and the Poitevine Gadifer de la Salle conquered the Canarian islands of Lanzarote, Fuerteventura and El Hierro off the Atlantic coast of Africa. Their troops were gathered in Normandy, Gascony and were later reinforced by Castilian colonists.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-971add33bc0b4787a05fc7ed92e7c7e9", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-971add33bc0b4787a05fc7ed92e7c7e9", "input": "Economist Joseph Stiglitz presented evidence in 2009 that both global inequality and inequality within countries prevent growth by limiting aggregate demand. Economist Branko Milanovic, wrote in 2001 that, 'The view that income inequality harms growth \u2013 or that improved equality can help sustain growth \u2013 has become more widely held in recent years. ... The main reason for this shift is the increasing importance of human capital in development. When physical capital mattered most, savings and investments were key. Then it was important to have a large contingent of rich people who could save a greater proportion of their income than the poor and invest it in physical capital. But now that human capital is scarcer than machines, widespread education has become the secret to growth.'", "output": ["What did Stiglitz present in 2009 regarding global inequality?", "How does inequality prevent growth?", "What are both Branko Milanovic and Joseph Stiglitz?", "What has been the main reason for the shift to the view that income inequality harms growth?", "What has become the secret to economic growth?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Economist Joseph Stiglitz presented evidence in 2009 that both global inequality and inequality within countries prevent growth by limiting aggregate demand. Economist Branko Milanovic, wrote in 2001 that, 'The view that income inequality harms growth \u2013 or that improved equality can help sustain growth \u2013 has become more widely held in recent years. ... The main reason for this shift is the increasing importance of human capital in development. When physical capital mattered most, savings and investments were key. Then it was important to have a large contingent of rich people who could save a greater proportion of their income than the poor and invest it in physical capital. But now that human capital is scarcer than machines, widespread education has become the secret to growth.'\nOutput: ", "bloom_target": "How does inequality prevent growth?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Economist Joseph Stiglitz presented evidence in 2009 that both global inequality and inequality within countries prevent growth by limiting aggregate demand. Economist Branko Milanovic, wrote in 2001 that, 'The view that income inequality harms growth \u2013 or that improved equality can help sustain growth \u2013 has become more widely held in recent years. ... The main reason for this shift is the increasing importance of human capital in development. When physical capital mattered most, savings and investments were key. Then it was important to have a large contingent of rich people who could save a greater proportion of their income than the poor and invest it in physical capital. But now that human capital is scarcer than machines, widespread education has become the secret to growth.'\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-d675a5a4bc96487ea30075a103bd79a2", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-d675a5a4bc96487ea30075a103bd79a2", "input": "Before Rollo's arrival, its populations did not differ from Picardy or the \u00cele-de-France, which were considered 'Frankish'. Earlier Viking settlers had begun arriving in the 880s, but were divided between colonies in the east (Roumois and Pays de Caux) around the low Seine valley and in the west in the Cotentin Peninsula, and were separated by traditional pagii, where the population remained about the same with almost no foreign settlers. Rollo's contingents who raided and ultimately settled Normandy and parts of the Atlantic coast included Danes, Norwegians, Norse\u2013Gaels, Orkney Vikings, possibly Swedes, and Anglo-Danes from the English Danelaw under Norse control.", "output": ["Who upon arriving gave the original viking settlers a common identity?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Before Rollo's arrival, its populations did not differ from Picardy or the \u00cele-de-France, which were considered 'Frankish'. Earlier Viking settlers had begun arriving in the 880s, but were divided between colonies in the east (Roumois and Pays de Caux) around the low Seine valley and in the west in the Cotentin Peninsula, and were separated by traditional pagii, where the population remained about the same with almost no foreign settlers. Rollo's contingents who raided and ultimately settled Normandy and parts of the Atlantic coast included Danes, Norwegians, Norse\u2013Gaels, Orkney Vikings, possibly Swedes, and Anglo-Danes from the English Danelaw under Norse control.\nOutput: ", "bloom_target": "Who upon arriving gave the original viking settlers a common identity?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Before Rollo's arrival, its populations did not differ from Picardy or the \u00cele-de-France, which were considered 'Frankish'. Earlier Viking settlers had begun arriving in the 880s, but were divided between colonies in the east (Roumois and Pays de Caux) around the low Seine valley and in the west in the Cotentin Peninsula, and were separated by traditional pagii, where the population remained about the same with almost no foreign settlers. Rollo's contingents who raided and ultimately settled Normandy and parts of the Atlantic coast included Danes, Norwegians, Norse\u2013Gaels, Orkney Vikings, possibly Swedes, and Anglo-Danes from the English Danelaw under Norse control.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-5562b424b3dd4f0fb14a569cb79e36e2", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-5562b424b3dd4f0fb14a569cb79e36e2", "input": "In the laboratory, stratigraphers analyze samples of stratigraphic sections that can be returned from the field, such as those from drill cores. Stratigraphers also analyze data from geophysical surveys that show the locations of stratigraphic units in the subsurface. Geophysical data and well logs can be combined to produce a better view of the subsurface, and stratigraphers often use computer programs to do this in three dimensions. Stratigraphers can then use these data to reconstruct ancient processes occurring on the surface of the Earth, interpret past environments, and locate areas for water, coal, and hydrocarbon extraction.", "output": ["The analysis of stratigraphic sections such as drill cores is done by who?", "What type of surveys show the location of stratigraphic units in the subsurface?", "What can be combined with geophysical data to produce a better view of the subsurface?", "What tool do stratigraphers use to see their data in three dimensions?", "Stratigraphers try to locate areas for what types of extraction?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In the laboratory, stratigraphers analyze samples of stratigraphic sections that can be returned from the field, such as those from drill cores. Stratigraphers also analyze data from geophysical surveys that show the locations of stratigraphic units in the subsurface. Geophysical data and well logs can be combined to produce a better view of the subsurface, and stratigraphers often use computer programs to do this in three dimensions. Stratigraphers can then use these data to reconstruct ancient processes occurring on the surface of the Earth, interpret past environments, and locate areas for water, coal, and hydrocarbon extraction.\nOutput: ", "bloom_target": "What type of surveys show the location of stratigraphic units in the subsurface?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In the laboratory, stratigraphers analyze samples of stratigraphic sections that can be returned from the field, such as those from drill cores. Stratigraphers also analyze data from geophysical surveys that show the locations of stratigraphic units in the subsurface. Geophysical data and well logs can be combined to produce a better view of the subsurface, and stratigraphers often use computer programs to do this in three dimensions. Stratigraphers can then use these data to reconstruct ancient processes occurring on the surface of the Earth, interpret past environments, and locate areas for water, coal, and hydrocarbon extraction.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-a7152e6f35d743bfa324ae1230121e34", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-a7152e6f35d743bfa324ae1230121e34", "input": "French Huguenot explorer Jean Ribault charted the St. Johns River in 1562 calling it the River of May because he discovered it in May. Ribault erected a stone column near present-day Jacksonville claiming the newly discovered land for France. In 1564, Ren\u00e9 Goulaine de Laudonni\u00e8re established the first European settlement, Fort Caroline, on the St. Johns near the main village of the Saturiwa. Philip II of Spain ordered Pedro Men\u00e9ndez de Avil\u00e9s to protect the interest of Spain by attacking the French presence at Fort Caroline. On September 20, 1565, a Spanish force from the nearby Spanish settlement of St. Augustine attacked Fort Caroline, and killed nearly all the French soldiers defending it. The Spanish renamed the fort San Mateo, and following the ejection of the French, St. Augustine's position as the most important settlement in Florida was solidified. The location of Fort Caroline is subject to debate but a reconstruction of the fort was established on the St. Johns River in 1964.", "output": ["Who mapped the St. Johns River in 1562?", "For what nation did Ribault initially claim what is now Jacksonville?", "Who led the attack of the French colony in 1565?", "What was Fort Caroline renamed to after the Spanish attack?", "Which fort was rebuilt in 1964?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: French Huguenot explorer Jean Ribault charted the St. Johns River in 1562 calling it the River of May because he discovered it in May. Ribault erected a stone column near present-day Jacksonville claiming the newly discovered land for France. In 1564, Ren\u00e9 Goulaine de Laudonni\u00e8re established the first European settlement, Fort Caroline, on the St. Johns near the main village of the Saturiwa. Philip II of Spain ordered Pedro Men\u00e9ndez de Avil\u00e9s to protect the interest of Spain by attacking the French presence at Fort Caroline. On September 20, 1565, a Spanish force from the nearby Spanish settlement of St. Augustine attacked Fort Caroline, and killed nearly all the French soldiers defending it. The Spanish renamed the fort San Mateo, and following the ejection of the French, St. Augustine's position as the most important settlement in Florida was solidified. The location of Fort Caroline is subject to debate but a reconstruction of the fort was established on the St. Johns River in 1964.\nOutput: ", "bloom_target": "Which fort was rebuilt in 1964?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: French Huguenot explorer Jean Ribault charted the St. Johns River in 1562 calling it the River of May because he discovered it in May. Ribault erected a stone column near present-day Jacksonville claiming the newly discovered land for France. In 1564, Ren\u00e9 Goulaine de Laudonni\u00e8re established the first European settlement, Fort Caroline, on the St. Johns near the main village of the Saturiwa. Philip II of Spain ordered Pedro Men\u00e9ndez de Avil\u00e9s to protect the interest of Spain by attacking the French presence at Fort Caroline. On September 20, 1565, a Spanish force from the nearby Spanish settlement of St. Augustine attacked Fort Caroline, and killed nearly all the French soldiers defending it. The Spanish renamed the fort San Mateo, and following the ejection of the French, St. Augustine's position as the most important settlement in Florida was solidified. The location of Fort Caroline is subject to debate but a reconstruction of the fort was established on the St. Johns River in 1964.\nOutput:  The", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-fbdd0d0aa4a941b09096bc96177a8c30", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-fbdd0d0aa4a941b09096bc96177a8c30", "input": "In 1466, perhaps 40,000 people died of the plague in Paris. During the 16th and 17th centuries, the plague was present in Paris around 30 per cent of the time. The Black Death ravaged Europe for three years before it continued on into Russia, where the disease was present somewhere in the country 25 times between 1350 to 1490. Plague epidemics ravaged London in 1563, 1593, 1603, 1625, 1636, and 1665, reducing its population by 10 to 30% during those years. Over 10% of Amsterdam's population died in 1623\u201325, and again in 1635\u201336, 1655, and 1664. Plague occurred in Venice 22 times between 1361 and 1528. The plague of 1576\u201377 killed 50,000 in Venice, almost a third of the population. Late outbreaks in central Europe included the Italian Plague of 1629\u20131631, which is associated with troop movements during the Thirty Years' War, and the Great Plague of Vienna in 1679. Over 60% of Norway's population died in 1348\u201350. The last plague outbreak ravaged Oslo in 1654.", "output": ["How many people died of plague in Paris in 1466?", "The black plague ravaged Europe for three years followed by what country?", "Which outbreak was associated with troops in the thirty years war?", "When was the last plague outbreak?", "How many times did plague occur in Venice?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In 1466, perhaps 40,000 people died of the plague in Paris. During the 16th and 17th centuries, the plague was present in Paris around 30 per cent of the time. The Black Death ravaged Europe for three years before it continued on into Russia, where the disease was present somewhere in the country 25 times between 1350 to 1490. Plague epidemics ravaged London in 1563, 1593, 1603, 1625, 1636, and 1665, reducing its population by 10 to 30% during those years. Over 10% of Amsterdam's population died in 1623\u201325, and again in 1635\u201336, 1655, and 1664. Plague occurred in Venice 22 times between 1361 and 1528. The plague of 1576\u201377 killed 50,000 in Venice, almost a third of the population. Late outbreaks in central Europe included the Italian Plague of 1629\u20131631, which is associated with troop movements during the Thirty Years' War, and the Great Plague of Vienna in 1679. Over 60% of Norway's population died in 1348\u201350. The last plague outbreak ravaged Oslo in 1654.\nOutput: ", "bloom_target": "When was the last plague outbreak?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In 1466, perhaps 40,000 people died of the plague in Paris. During the 16th and 17th centuries, the plague was present in Paris around 30 per cent of the time. The Black Death ravaged Europe for three years before it continued on into Russia, where the disease was present somewhere in the country 25 times between 1350 to 1490. Plague epidemics ravaged London in 1563, 1593, 1603, 1625, 1636, and 1665, reducing its population by 10 to 30% during those years. Over 10% of Amsterdam's population died in 1623\u201325, and again in 1635\u201336, 1655, and 1664. Plague occurred in Venice 22 times between 1361 and 1528. The plague of 1576\u201377 killed 50,000 in Venice, almost a third of the population. Late outbreaks in central Europe included the Italian Plague of 1629\u20131631, which is associated with troop movements during the Thirty Years' War, and the Great Plague of Vienna in 1679. Over 60% of Norway's population died in 1348\u201350. The last plague outbreak ravaged Oslo in 1654.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-b762e56d49b6446fb5b7a7e7c5a10c78", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-b762e56d49b6446fb5b7a7e7c5a10c78", "input": "Despite their soft, gelatinous bodies, fossils thought to represent ctenophores, apparently with no tentacles but many more comb-rows than modern forms, have been found in lagerst\u00e4tten as far back as the early Cambrian, about 515 million years ago. The position of the ctenophores in the evolutionary family tree of animals has long been debated, and the majority view at present, based on molecular phylogenetics, is that cnidarians and bilaterians are more closely related to each other than either is to ctenophores. A recent molecular phylogenetics analysis concluded that the common ancestor of all modern ctenophores was cydippid-like, and that all the modern groups appeared relatively recently, probably after the Cretaceous\u2013Paleogene extinction event 66 million years ago. Evidence accumulating since the 1980s indicates that the 'cydippids' are not monophyletic, in other words do not include all and only the descendants of a single common ancestor, because all the other traditional ctenophore groups are descendants of various cydippids.", "output": ["When did the Cretaceous-Paleogene extinction happen?", "Evidence indicates that Cydippids are not what?", "How old are the fossils found that represent ctenophhores ?", "What were the fossils that were found to represent ctenphores missing that current ctenphora have?", "Fossils found that were believed to be ctenophores were how old?", "What event happened 66 million years ago?", "Cypiddids are not what?", "What do current ctenophores have that fossils found did not have?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Despite their soft, gelatinous bodies, fossils thought to represent ctenophores, apparently with no tentacles but many more comb-rows than modern forms, have been found in lagerst\u00e4tten as far back as the early Cambrian, about 515 million years ago. The position of the ctenophores in the evolutionary family tree of animals has long been debated, and the majority view at present, based on molecular phylogenetics, is that cnidarians and bilaterians are more closely related to each other than either is to ctenophores. A recent molecular phylogenetics analysis concluded that the common ancestor of all modern ctenophores was cydippid-like, and that all the modern groups appeared relatively recently, probably after the Cretaceous\u2013Paleogene extinction event 66 million years ago. Evidence accumulating since the 1980s indicates that the 'cydippids' are not monophyletic, in other words do not include all and only the descendants of a single common ancestor, because all the other traditional ctenophore groups are descendants of various cydippids.\nOutput: ", "bloom_target": "Cypiddids are not what?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Despite their soft, gelatinous bodies, fossils thought to represent ctenophores, apparently with no tentacles but many more comb-rows than modern forms, have been found in lagerst\u00e4tten as far back as the early Cambrian, about 515 million years ago. The position of the ctenophores in the evolutionary family tree of animals has long been debated, and the majority view at present, based on molecular phylogenetics, is that cnidarians and bilaterians are more closely related to each other than either is to ctenophores. A recent molecular phylogenetics analysis concluded that the common ancestor of all modern ctenophores was cydippid-like, and that all the modern groups appeared relatively recently, probably after the Cretaceous\u2013Paleogene extinction event 66 million years ago. Evidence accumulating since the 1980s indicates that the 'cydippids' are not monophyletic, in other words do not include all and only the descendants of a single common ancestor, because all the other traditional ctenophore groups are descendants of various cydippids.\nOutput:  The", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-d4a8ba69612b4cabae5f9e9ac0ca8488", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-d4a8ba69612b4cabae5f9e9ac0ca8488", "input": "Disorders of the immune system can result in autoimmune diseases, inflammatory diseases and cancer. Immunodeficiency occurs when the immune system is less active than normal, resulting in recurring and life-threatening infections. In humans, immunodeficiency can either be the result of a genetic disease such as severe combined immunodeficiency, acquired conditions such as HIV/AIDS, or the use of immunosuppressive medication. In contrast, autoimmunity results from a hyperactive immune system attacking normal tissues as if they were foreign organisms. Common autoimmune diseases include Hashimoto's thyroiditis, rheumatoid arthritis, diabetes mellitus type 1, and systemic lupus erythematosus. Immunology covers the study of all aspects of the immune system.", "output": ["What can result from disorders of the immune system?", "When does immunodeficiency occur?", "What does immunodeficiency cause?", "Name one of the causes of immunodeficiency.", "Name a common autoimmune disease.", "What happens when the immune system less active than normal?", "What is the term for a hyperactive immune system that attacks normal tissues?", "What field involves the study of the immune system?", "What acquired condition results in immunodeficiency in humans?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Disorders of the immune system can result in autoimmune diseases, inflammatory diseases and cancer. Immunodeficiency occurs when the immune system is less active than normal, resulting in recurring and life-threatening infections. In humans, immunodeficiency can either be the result of a genetic disease such as severe combined immunodeficiency, acquired conditions such as HIV/AIDS, or the use of immunosuppressive medication. In contrast, autoimmunity results from a hyperactive immune system attacking normal tissues as if they were foreign organisms. Common autoimmune diseases include Hashimoto's thyroiditis, rheumatoid arthritis, diabetes mellitus type 1, and systemic lupus erythematosus. Immunology covers the study of all aspects of the immune system.\nOutput: ", "bloom_target": "What acquired condition results in immunodeficiency in humans?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Disorders of the immune system can result in autoimmune diseases, inflammatory diseases and cancer. Immunodeficiency occurs when the immune system is less active than normal, resulting in recurring and life-threatening infections. In humans, immunodeficiency can either be the result of a genetic disease such as severe combined immunodeficiency, acquired conditions such as HIV/AIDS, or the use of immunosuppressive medication. In contrast, autoimmunity results from a hyperactive immune system attacking normal tissues as if they were foreign organisms. Common autoimmune diseases include Hashimoto's thyroiditis, rheumatoid arthritis, diabetes mellitus type 1, and systemic lupus erythematosus. Immunology covers the study of all aspects of the immune system.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-2b207fb9db92445580bce0900d4ea3a7", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-2b207fb9db92445580bce0900d4ea3a7", "input": "In 1857 John Sheepshanks donated 233 paintings, mainly by contemporary British artists, and a similar number of drawings to the museum with the intention of forming a 'A National Gallery of British Art', a role since taken on by Tate Britain; artists represented are William Blake, James Barry, Henry Fuseli, Sir Edwin Henry Landseer, Sir David Wilkie, William Mulready, William Powell Frith, Millais and Hippolyte Delaroche. Although some of Constable's works came to the museum with the Sheepshanks bequest, the majority of the artist's works were donated by his daughter Isabel in 1888, including the large number of sketches in oil, the most significant being the 1821 full size oil sketch for The Hay Wain. Other artists with works in the collection include: Bernardino Fungai, Marcus Gheeraerts the Younger, Domenico di Pace Beccafumi, Fioravante Ferramola, Jan Brueghel the Elder, Anthony van Dyck, Ludovico Carracci, Antonio Verrio, Giovanni Battista Tiepolo, Domenico Tiepolo, Canaletto, Francis Hayman, Pompeo Batoni, Benjamin West, Paul Sandby, Richard Wilson, William Etty, Henry Fuseli, Sir Thomas Lawrence, James Barry, Francis Danby, Richard Parkes Bonington and Alphonse Legros.", "output": ["In which year did John Sheepshanks donated a large collection of paintings?", "How many paintings did John Sheeshanks give to the museum?", "What was the goal of John Sheepshanks considerable bequest to the museum?", "An 1821 full size oil sketch of which famous British painting was donated by John Sheepshank's daughter in 1888?", "The paintings donated by John Sheepshanks were by artists of which nationality?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In 1857 John Sheepshanks donated 233 paintings, mainly by contemporary British artists, and a similar number of drawings to the museum with the intention of forming a 'A National Gallery of British Art', a role since taken on by Tate Britain; artists represented are William Blake, James Barry, Henry Fuseli, Sir Edwin Henry Landseer, Sir David Wilkie, William Mulready, William Powell Frith, Millais and Hippolyte Delaroche. Although some of Constable's works came to the museum with the Sheepshanks bequest, the majority of the artist's works were donated by his daughter Isabel in 1888, including the large number of sketches in oil, the most significant being the 1821 full size oil sketch for The Hay Wain. Other artists with works in the collection include: Bernardino Fungai, Marcus Gheeraerts the Younger, Domenico di Pace Beccafumi, Fioravante Ferramola, Jan Brueghel the Elder, Anthony van Dyck, Ludovico Carracci, Antonio Verrio, Giovanni Battista Tiepolo, Domenico Tiepolo, Canaletto, Francis Hayman, Pompeo Batoni, Benjamin West, Paul Sandby, Richard Wilson, William Etty, Henry Fuseli, Sir Thomas Lawrence, James Barry, Francis Danby, Richard Parkes Bonington and Alphonse Legros.\nOutput: ", "bloom_target": "How many paintings did John Sheeshanks give to the museum?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In 1857 John Sheepshanks donated 233 paintings, mainly by contemporary British artists, and a similar number of drawings to the museum with the intention of forming a 'A National Gallery of British Art', a role since taken on by Tate Britain; artists represented are William Blake, James Barry, Henry Fuseli, Sir Edwin Henry Landseer, Sir David Wilkie, William Mulready, William Powell Frith, Millais and Hippolyte Delaroche. Although some of Constable's works came to the museum with the Sheepshanks bequest, the majority of the artist's works were donated by his daughter Isabel in 1888, including the large number of sketches in oil, the most significant being the 1821 full size oil sketch for The Hay Wain. Other artists with works in the collection include: Bernardino Fungai, Marcus Gheeraerts the Younger, Domenico di Pace Beccafumi, Fioravante Ferramola, Jan Brueghel the Elder, Anthony van Dyck, Ludovico Carracci, Antonio Verrio, Giovanni Battista Tiepolo, Domenico Tiepolo, Canaletto, Francis Hayman, Pompeo Batoni, Benjamin West, Paul Sandby, Richard Wilson, William Etty, Henry Fuseli, Sir Thomas Lawrence, James Barry, Francis Danby, Richard Parkes Bonington and Alphonse Legros.\nOutput:  The", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-0cde811e2d5e4aa3b2f60f99d807e4f1", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-0cde811e2d5e4aa3b2f60f99d807e4f1", "input": "As in the House of Commons, a number of qualifications apply to being an MSP. Such qualifications were introduced under the House of Commons Disqualification Act 1975 and the British Nationality Act 1981. Specifically, members must be over the age of 18 and must be a citizen of the United Kingdom, the Republic of Ireland, one of the countries in the Commonwealth of Nations, a citizen of a British overseas territory, or a European Union citizen resident in the UK. Members of the police and the armed forces are disqualified from sitting in the Scottish Parliament as elected MSPs, and similarly, civil servants and members of foreign legislatures are disqualified. An individual may not sit in the Scottish Parliament if he or she is judged to be insane under the terms of the Mental Health (Care and Treatment) (Scotland) Act 2003.", "output": ["What does being an MSP share with the House of Commons?", "When was the British Nationality Act passed?", "How old most a person be to apply to be an MSP?", "Members of which organizations are disqualified from sitting in the SP as elected MSPs?", "What act sets the term for judging the boundaries of sanity to which individuals wishing to sit on the SP must adhere?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: As in the House of Commons, a number of qualifications apply to being an MSP. Such qualifications were introduced under the House of Commons Disqualification Act 1975 and the British Nationality Act 1981. Specifically, members must be over the age of 18 and must be a citizen of the United Kingdom, the Republic of Ireland, one of the countries in the Commonwealth of Nations, a citizen of a British overseas territory, or a European Union citizen resident in the UK. Members of the police and the armed forces are disqualified from sitting in the Scottish Parliament as elected MSPs, and similarly, civil servants and members of foreign legislatures are disqualified. An individual may not sit in the Scottish Parliament if he or she is judged to be insane under the terms of the Mental Health (Care and Treatment) (Scotland) Act 2003.\nOutput: ", "bloom_target": "Members of which organizations are disqualified from sitting in the SP as elected MSPs?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: As in the House of Commons, a number of qualifications apply to being an MSP. Such qualifications were introduced under the House of Commons Disqualification Act 1975 and the British Nationality Act 1981. Specifically, members must be over the age of 18 and must be a citizen of the United Kingdom, the Republic of Ireland, one of the countries in the Commonwealth of Nations, a citizen of a British overseas territory, or a European Union citizen resident in the UK. Members of the police and the armed forces are disqualified from sitting in the Scottish Parliament as elected MSPs, and similarly, civil servants and members of foreign legislatures are disqualified. An individual may not sit in the Scottish Parliament if he or she is judged to be insane under the terms of the Mental Health (Care and Treatment) (Scotland) Act 2003.\nOutput:  The", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-84dd91e2217c48bb994ca954d630cedc", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-84dd91e2217c48bb994ca954d630cedc", "input": "A prime number (or a prime) is a natural number greater than 1 that has no positive divisors other than 1 and itself. A natural number greater than 1 that is not a prime number is called a composite number. For example, 5 is prime because 1 and 5 are its only positive integer factors, whereas 6 is composite because it has the divisors 2 and 3 in addition to 1 and 6. The fundamental theorem of arithmetic establishes the central role of primes in number theory: any integer greater than 1 can be expressed as a product of primes that is unique up to ordering. The uniqueness in this theorem requires excluding 1 as a prime because one can include arbitrarily many instances of 1 in any factorization, e.g., 3, 1 \u00b7 3, 1 \u00b7 1 \u00b7 3, etc. are all valid factorizations of 3.", "output": ["What is the only divisor besides 1 that a prime number can have?", "What are numbers greater than 1 that can be divided by 3 or more numbers called?", "What theorem defines the main role of primes in number theory?", "Any number larger than 1 can be represented as a product of what?", "Why must one be excluded in order to preserve the uniqueness of the fundamental theorem?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: A prime number (or a prime) is a natural number greater than 1 that has no positive divisors other than 1 and itself. A natural number greater than 1 that is not a prime number is called a composite number. For example, 5 is prime because 1 and 5 are its only positive integer factors, whereas 6 is composite because it has the divisors 2 and 3 in addition to 1 and 6. The fundamental theorem of arithmetic establishes the central role of primes in number theory: any integer greater than 1 can be expressed as a product of primes that is unique up to ordering. The uniqueness in this theorem requires excluding 1 as a prime because one can include arbitrarily many instances of 1 in any factorization, e.g., 3, 1 \u00b7 3, 1 \u00b7 1 \u00b7 3, etc. are all valid factorizations of 3.\nOutput: ", "bloom_target": "What is the only divisor besides 1 that a prime number can have?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: A prime number (or a prime) is a natural number greater than 1 that has no positive divisors other than 1 and itself. A natural number greater than 1 that is not a prime number is called a composite number. For example, 5 is prime because 1 and 5 are its only positive integer factors, whereas 6 is composite because it has the divisors 2 and 3 in addition to 1 and 6. The fundamental theorem of arithmetic establishes the central role of primes in number theory: any integer greater than 1 can be expressed as a product of primes that is unique up to ordering. The uniqueness in this theorem requires excluding 1 as a prime because one can include arbitrarily many instances of 1 in any factorization, e.g., 3, 1 \u00b7 3, 1 \u00b7 1 \u00b7 3, etc. are all valid factorizations of 3.\nOutput:  The", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-e1ad84720bef460e8491c7a669f7198a", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-e1ad84720bef460e8491c7a669f7198a", "input": "DECnet is a suite of network protocols created by Digital Equipment Corporation, originally released in 1975 in order to connect two PDP-11 minicomputers. It evolved into one of the first peer-to-peer network architectures, thus transforming DEC into a networking powerhouse in the 1980s. Initially built with three layers, it later (1982) evolved into a seven-layer OSI-compliant networking protocol. The DECnet protocols were designed entirely by Digital Equipment Corporation. However, DECnet Phase II (and later) were open standards with published specifications, and several implementations were developed outside DEC, including one for Linux.", "output": ["What is  DECnet", "What did DECnet originally do", "DEC originally had 3 layers but evolved into how many layers", "What did DECnet phase 2 become"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: DECnet is a suite of network protocols created by Digital Equipment Corporation, originally released in 1975 in order to connect two PDP-11 minicomputers. It evolved into one of the first peer-to-peer network architectures, thus transforming DEC into a networking powerhouse in the 1980s. Initially built with three layers, it later (1982) evolved into a seven-layer OSI-compliant networking protocol. The DECnet protocols were designed entirely by Digital Equipment Corporation. However, DECnet Phase II (and later) were open standards with published specifications, and several implementations were developed outside DEC, including one for Linux.\nOutput: ", "bloom_target": "What did DECnet phase 2 become", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: DECnet is a suite of network protocols created by Digital Equipment Corporation, originally released in 1975 in order to connect two PDP-11 minicomputers. It evolved into one of the first peer-to-peer network architectures, thus transforming DEC into a networking powerhouse in the 1980s. Initially built with three layers, it later (1982) evolved into a seven-layer OSI-compliant networking protocol. The DECnet protocols were designed entirely by Digital Equipment Corporation. However, DECnet Phase II (and later) were open standards with published specifications, and several implementations were developed outside DEC, including one for Linux.\nOutput:  The", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-4da212ba1d2848daaa88bd0659dd6572", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-4da212ba1d2848daaa88bd0659dd6572", "input": "Engineering News-Record (ENR) is a trade magazine for the construction industry. Each year, ENR compiles and reports on data about the size of design and construction companies. They publish a list of the largest companies in the United States (Top-40) and also a list the largest global firms (Top-250, by amount of work they are doing outside their home country). In 2014, ENR compiled the data in nine market segments. It was divided as transportation, petroleum, buildings, power, industrial, water, manufacturing, sewer/waste, telecom, hazardous waste plus a tenth category for other projects. In their reporting on the Top 400, they used data on transportation, sewer, hazardous waste and water to rank firms as heavy contractors.", "output": ["What is Engineering News-Record?", "What compiles and reports on data about the size of design and construction companies?", "In what year did ENR compile data in nine market segments?", "ENR used data on what to rank Top 400 firms as heavy contractors?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Engineering News-Record (ENR) is a trade magazine for the construction industry. Each year, ENR compiles and reports on data about the size of design and construction companies. They publish a list of the largest companies in the United States (Top-40) and also a list the largest global firms (Top-250, by amount of work they are doing outside their home country). In 2014, ENR compiled the data in nine market segments. It was divided as transportation, petroleum, buildings, power, industrial, water, manufacturing, sewer/waste, telecom, hazardous waste plus a tenth category for other projects. In their reporting on the Top 400, they used data on transportation, sewer, hazardous waste and water to rank firms as heavy contractors.\nOutput: ", "bloom_target": "What is Engineering News-Record?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Engineering News-Record (ENR) is a trade magazine for the construction industry. Each year, ENR compiles and reports on data about the size of design and construction companies. They publish a list of the largest companies in the United States (Top-40) and also a list the largest global firms (Top-250, by amount of work they are doing outside their home country). In 2014, ENR compiled the data in nine market segments. It was divided as transportation, petroleum, buildings, power, industrial, water, manufacturing, sewer/waste, telecom, hazardous waste plus a tenth category for other projects. In their reporting on the Top 400, they used data on transportation, sewer, hazardous waste and water to rank firms as heavy contractors.\nOutput:  Which", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-be4ad93378d74a9fb81b28ac9b18f09a", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-be4ad93378d74a9fb81b28ac9b18f09a", "input": "After the founding of the colony of New South Wales in 1788, Australia was divided into an eastern half named New South Wales and a western half named New Holland, under the administration of the colonial government in Sydney. The first European settlement in the area later known as Victoria was established in October 1803 under Lieutenant-Governor David Collins at Sullivan Bay on Port Phillip. It consisted of 402 people (5 Government officials, 9 officers of marines, 2 drummers, and 39 privates, 5 soldiers' wives, and a child, 307 convicts, 17 convicts' wives, and 7 children). They had been sent from England in HMS Calcutta under the command of Captain Daniel Woodriff, principally out of fear that the French, who had been exploring the area, might establish their own settlement and thereby challenge British rights to the continent.", "output": ["When was the colony of New South Wales founded?", "What was the name of the eastern half of the colony of 1788?", "What name was given to the western half of the colony?", "Where was the location of the colonial government that administered the new colony?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: After the founding of the colony of New South Wales in 1788, Australia was divided into an eastern half named New South Wales and a western half named New Holland, under the administration of the colonial government in Sydney. The first European settlement in the area later known as Victoria was established in October 1803 under Lieutenant-Governor David Collins at Sullivan Bay on Port Phillip. It consisted of 402 people (5 Government officials, 9 officers of marines, 2 drummers, and 39 privates, 5 soldiers' wives, and a child, 307 convicts, 17 convicts' wives, and 7 children). They had been sent from England in HMS Calcutta under the command of Captain Daniel Woodriff, principally out of fear that the French, who had been exploring the area, might establish their own settlement and thereby challenge British rights to the continent.\nOutput: ", "bloom_target": "What name was given to the western half of the colony?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: After the founding of the colony of New South Wales in 1788, Australia was divided into an eastern half named New South Wales and a western half named New Holland, under the administration of the colonial government in Sydney. The first European settlement in the area later known as Victoria was established in October 1803 under Lieutenant-Governor David Collins at Sullivan Bay on Port Phillip. It consisted of 402 people (5 Government officials, 9 officers of marines, 2 drummers, and 39 privates, 5 soldiers' wives, and a child, 307 convicts, 17 convicts' wives, and 7 children). They had been sent from England in HMS Calcutta under the command of Captain Daniel Woodriff, principally out of fear that the French, who had been exploring the area, might establish their own settlement and thereby challenge British rights to the continent.\nOutput:  The", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-1e795767ff814c3cb9ae01ce08374c9f", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-1e795767ff814c3cb9ae01ce08374c9f", "input": "In most countries, the dispensary is subject to pharmacy legislation; with requirements for storage conditions, compulsory texts, equipment, etc., specified in legislation. Where it was once the case that pharmacists stayed within the dispensary compounding/dispensing medications, there has been an increasing trend towards the use of trained pharmacy technicians while the pharmacist spends more time communicating with patients. Pharmacy technicians are now more dependent upon automation to assist them in their new role dealing with patients' prescriptions and patient safety issues.", "output": ["What is the dispensary subject to in a majority of countries?", "What responsibilities were pharmacy technicians formerly limited to?", "What do pharmacy technicians depend on more and more?", "What are new responsibilities pharmacy technicians now deal with?", "What does pharmacy legislation mandate?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In most countries, the dispensary is subject to pharmacy legislation; with requirements for storage conditions, compulsory texts, equipment, etc., specified in legislation. Where it was once the case that pharmacists stayed within the dispensary compounding/dispensing medications, there has been an increasing trend towards the use of trained pharmacy technicians while the pharmacist spends more time communicating with patients. Pharmacy technicians are now more dependent upon automation to assist them in their new role dealing with patients' prescriptions and patient safety issues.\nOutput: ", "bloom_target": "What responsibilities were pharmacy technicians formerly limited to?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In most countries, the dispensary is subject to pharmacy legislation; with requirements for storage conditions, compulsory texts, equipment, etc., specified in legislation. Where it was once the case that pharmacists stayed within the dispensary compounding/dispensing medications, there has been an increasing trend towards the use of trained pharmacy technicians while the pharmacist spends more time communicating with patients. Pharmacy technicians are now more dependent upon automation to assist them in their new role dealing with patients' prescriptions and patient safety issues.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-b8c2d744f7d947449fd6f059d0011fc5", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-b8c2d744f7d947449fd6f059d0011fc5", "input": "In 1998, the network began using a minimalist graphical identity, designed by Pittard Sullivan, featuring a small black-and-white 'ABC Circle' logo on a yellow background (promotions during this time also featured a sequence of still photos of the stars of its programs during the timeslot card as well as the schedule sequence that began each night's prime time lineup). A new four-note theme tune was introduced alongside the package, based around the network's 'We Love TV' image campaign introduced that year, creating an audio signature on par with the NBC chimes, CBS' various three-note soundmarks (including the current version used since 2000) and the Fox Fanfare. The four-note signature has been updated with every television season thereafter (though variants of it used since the 1998\u201399 season remain in use during the production company vanity cards shown following the closing credits of most programs). In the fall of 2015, ABC is stopped with its 1998\u20132002 four-note jingles for promotions and production company vanity cards following the closing credits of most of its programs over seventeen years, now it have a different and adventure-type music (with the drums of the network's four-note signature in the ending). The old four-note theme tune is still used by ABC on Demand to the beginning of the ABC show.", "output": ["Who designed ABC's 1998 new graphic design?", "In what year did ABC stop using it's four-note jingle for promotion?", "What image campaign was the new four-note jingle for ABC based around?", "Where is the ABC four-note jingle still in use?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In 1998, the network began using a minimalist graphical identity, designed by Pittard Sullivan, featuring a small black-and-white 'ABC Circle' logo on a yellow background (promotions during this time also featured a sequence of still photos of the stars of its programs during the timeslot card as well as the schedule sequence that began each night's prime time lineup). A new four-note theme tune was introduced alongside the package, based around the network's 'We Love TV' image campaign introduced that year, creating an audio signature on par with the NBC chimes, CBS' various three-note soundmarks (including the current version used since 2000) and the Fox Fanfare. The four-note signature has been updated with every television season thereafter (though variants of it used since the 1998\u201399 season remain in use during the production company vanity cards shown following the closing credits of most programs). In the fall of 2015, ABC is stopped with its 1998\u20132002 four-note jingles for promotions and production company vanity cards following the closing credits of most of its programs over seventeen years, now it have a different and adventure-type music (with the drums of the network's four-note signature in the ending). The old four-note theme tune is still used by ABC on Demand to the beginning of the ABC show.\nOutput: ", "bloom_target": "Where is the ABC four-note jingle still in use?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In 1998, the network began using a minimalist graphical identity, designed by Pittard Sullivan, featuring a small black-and-white 'ABC Circle' logo on a yellow background (promotions during this time also featured a sequence of still photos of the stars of its programs during the timeslot card as well as the schedule sequence that began each night's prime time lineup). A new four-note theme tune was introduced alongside the package, based around the network's 'We Love TV' image campaign introduced that year, creating an audio signature on par with the NBC chimes, CBS' various three-note soundmarks (including the current version used since 2000) and the Fox Fanfare. The four-note signature has been updated with every television season thereafter (though variants of it used since the 1998\u201399 season remain in use during the production company vanity cards shown following the closing credits of most programs). In the fall of 2015, ABC is stopped with its 1998\u20132002 four-note jingles for promotions and production company vanity cards following the closing credits of most of its programs over seventeen years, now it have a different and adventure-type music (with the drums of the network's four-note signature in the ending). The old four-note theme tune is still used by ABC on Demand to the beginning of the ABC show.\nOutput:  The", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-e9f24430ec16498a93a01362895b39f2", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-e9f24430ec16498a93a01362895b39f2", "input": "In India, private schools are called independent schools, but since some private schools receive financial aid from the government, it can be an aided or an unaided school. So, in a strict sense, a private school is an unaided independent school. For the purpose of this definition, only receipt of financial aid is considered, not land purchased from the government at a subsidized rate. It is within the power of both the union government and the state governments to govern schools since Education appears in the Concurrent list of legislative subjects in the constitution. The practice has been for the union government to provide the broad policy directions while the states create their own rules and regulations for the administration of the sector. Among other things, this has also resulted in 30 different Examination Boards or academic authorities that conduct examinations for school leaving certificates. Prominent Examination Boards that are present in multiple states are the CBSE and the CISCE, NENBSE", "output": ["What is the term for an Indian private school?", "Along with the CISCE and NENBSE, what is a notable Examination Board in multiple Indian states?", "How many Examination Boards exist in India?", "What body in India provides policy directions to schools?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In India, private schools are called independent schools, but since some private schools receive financial aid from the government, it can be an aided or an unaided school. So, in a strict sense, a private school is an unaided independent school. For the purpose of this definition, only receipt of financial aid is considered, not land purchased from the government at a subsidized rate. It is within the power of both the union government and the state governments to govern schools since Education appears in the Concurrent list of legislative subjects in the constitution. The practice has been for the union government to provide the broad policy directions while the states create their own rules and regulations for the administration of the sector. Among other things, this has also resulted in 30 different Examination Boards or academic authorities that conduct examinations for school leaving certificates. Prominent Examination Boards that are present in multiple states are the CBSE and the CISCE, NENBSE.\nOutput: ", "bloom_target": "What is the term for an Indian private school?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In India, private schools are called independent schools, but since some private schools receive financial aid from the government, it can be an aided or an unaided school. So, in a strict sense, a private school is an unaided independent school. For the purpose of this definition, only receipt of financial aid is considered, not land purchased from the government at a subsidized rate. It is within the power of both the union government and the state governments to govern schools since Education appears in the Concurrent list of legislative subjects in the constitution. The practice has been for the union government to provide the broad policy directions while the states create their own rules and regulations for the administration of the sector. Among other things, this has also resulted in 30 different Examination Boards or academic authorities that conduct examinations for school leaving certificates. Prominent Examination Boards that are present in multiple states are the CBSE and the CISCE, NENBSE.\nOutput:  Which", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-9a7d2ce21c5d4b09b4b10d5dc6cf4ccc", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-9a7d2ce21c5d4b09b4b10d5dc6cf4ccc", "input": "BSkyB's standard definition broadcasts are in DVB-compliant MPEG-2, with the Sky Movies and Sky Box Office channels including optional Dolby Digital soundtracks for recent films, although these are only accessible with a Sky+ box. Sky+ HD material is broadcast using MPEG-4 and most of the HD material uses the DVB-S2 standard. Interactive services and 7-day EPG use the proprietary OpenTV system, with set-top boxes including modems for a return path. Sky News, amongst other channels, provides a pseudo-video on demand interactive service by broadcasting looping video streams.", "output": ["What are BSkyB's standard definition broadcasts compliant with?", "Sky Movies and Sky Box office also include what optional soundtracks?", "What is Sky+ HD material broadcast using?", "What is the proprietary system that Sky+HD uses?", "What does most of the HD material use as a standard?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: BSkyB's standard definition broadcasts are in DVB-compliant MPEG-2, with the Sky Movies and Sky Box Office channels including optional Dolby Digital soundtracks for recent films, although these are only accessible with a Sky+ box. Sky+ HD material is broadcast using MPEG-4 and most of the HD material uses the DVB-S2 standard. Interactive services and 7-day EPG use the proprietary OpenTV system, with set-top boxes including modems for a return path. Sky News, amongst other channels, provides a pseudo-video on demand interactive service by broadcasting looping video streams.\nOutput: ", "bloom_target": "Sky Movies and Sky Box office also include what optional soundtracks?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: BSkyB's standard definition broadcasts are in DVB-compliant MPEG-2, with the Sky Movies and Sky Box Office channels including optional Dolby Digital soundtracks for recent films, although these are only accessible with a Sky+ box. Sky+ HD material is broadcast using MPEG-4 and most of the HD material uses the DVB-S2 standard. Interactive services and 7-day EPG use the proprietary OpenTV system, with set-top boxes including modems for a return path. Sky News, amongst other channels, provides a pseudo-video on demand interactive service by broadcasting looping video streams.\nOutput:  The", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-df177f292bd14e2687c9e45de2d38a53", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-df177f292bd14e2687c9e45de2d38a53", "input": "Emperor Gegeen Khan, Ayurbarwada's son and successor, ruled for only two years, from 1321 to 1323. He continued his father's policies to reform the government based on the Confucian principles, with the help of his newly appointed grand chancellor Baiju. During his reign, the Da Yuan Tong Zhi (Chinese: \u5927\u5143\u901a\u5236, 'the comprehensive institutions of the Great Yuan'), a huge collection of codes and regulations of the Yuan dynasty begun by his father, was formally promulgated. Gegeen was assassinated in a coup involving five princes from a rival faction, perhaps steppe elite opposed to Confucian reforms. They placed Yes\u00fcn Tem\u00fcr (or Taidingdi) on the throne, and, after an unsuccessful attempt to calm the princes, he also succumbed to regicide.", "output": ["Who was Ayurbarwada's son?", "When was Geegen the emperor?", "Who did Gegeen appoint as grand chancellor?", "What did 'Da Yuan Tong Zhi' mean?", "How many rival princes were involved in assassinating Gegeen?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Emperor Gegeen Khan, Ayurbarwada's son and successor, ruled for only two years, from 1321 to 1323. He continued his father's policies to reform the government based on the Confucian principles, with the help of his newly appointed grand chancellor Baiju. During his reign, the Da Yuan Tong Zhi (Chinese: \u5927\u5143\u901a\u5236, 'the comprehensive institutions of the Great Yuan'), a huge collection of codes and regulations of the Yuan dynasty begun by his father, was formally promulgated. Gegeen was assassinated in a coup involving five princes from a rival faction, perhaps steppe elite opposed to Confucian reforms. They placed Yes\u00fcn Tem\u00fcr (or Taidingdi) on the throne, and, after an unsuccessful attempt to calm the princes, he also succumbed to regicide.\nOutput: ", "bloom_target": "Who was Ayurbarwada's son?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Emperor Gegeen Khan, Ayurbarwada's son and successor, ruled for only two years, from 1321 to 1323. He continued his father's policies to reform the government based on the Confucian principles, with the help of his newly appointed grand chancellor Baiju. During his reign, the Da Yuan Tong Zhi (Chinese: \u5927\u5143\u901a\u5236, 'the comprehensive institutions of the Great Yuan'), a huge collection of codes and regulations of the Yuan dynasty begun by his father, was formally promulgated. Gegeen was assassinated in a coup involving five princes from a rival faction, perhaps steppe elite opposed to Confucian reforms. They placed Yes\u00fcn Tem\u00fcr (or Taidingdi) on the throne, and, after an unsuccessful attempt to calm the princes, he also succumbed to regicide.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-70d94a18393e4bf4acb9d521c871a42e", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-70d94a18393e4bf4acb9d521c871a42e", "input": "Following the Peterloo massacre of 1819, poet Percy Shelley wrote the political poem The Mask of Anarchy later that year, that begins with the images of what he thought to be the unjust forms of authority of his time\u2014and then imagines the stirrings of a new form of social action. It is perhaps the first modern[vague] statement of the principle of nonviolent protest. A version was taken up by the author Henry David Thoreau in his essay Civil Disobedience, and later by Gandhi in his doctrine of Satyagraha. Gandhi's Satyagraha was partially influenced and inspired by Shelley's nonviolence in protest and political action. In particular, it is known that Gandhi would often quote Shelley's Masque of Anarchy to vast audiences during the campaign for a free India.", "output": ["After the Peterloo massacre what poet wrote The Massacre of Anarchy?", "His poem is considered the first kind of what type of protest?", "Inspired by Shelley what was the name of Gandhi's doctrine?", "Gandhi often referenced Shelley's poem in his efforts to do what?", "What famous author used similarity and likeness of Percy Shelly in his writing?", "Who wrote the poem The Mark of Anarchy?", "The Mark of Anarchy was written to protest against what?", "What was the principle shown in The Mark of Anarchy?", "What was Ghandi's work called?", "Which famous Indian practiced civil disobedience?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Following the Peterloo massacre of 1819, poet Percy Shelley wrote the political poem The Mask of Anarchy later that year, that begins with the images of what he thought to be the unjust forms of authority of his time\u2014and then imagines the stirrings of a new form of social action. It is perhaps the first modern[vague] statement of the principle of nonviolent protest. A version was taken up by the author Henry David Thoreau in his essay Civil Disobedience, and later by Gandhi in his doctrine of Satyagraha. Gandhi's Satyagraha was partially influenced and inspired by Shelley's nonviolence in protest and political action. In particular, it is known that Gandhi would often quote Shelley's Masque of Anarchy to vast audiences during the campaign for a free India.\nOutput: ", "bloom_target": "What was the principle shown in The Mark of Anarchy?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Following the Peterloo massacre of 1819, poet Percy Shelley wrote the political poem The Mask of Anarchy later that year, that begins with the images of what he thought to be the unjust forms of authority of his time\u2014and then imagines the stirrings of a new form of social action. It is perhaps the first modern[vague] statement of the principle of nonviolent protest. A version was taken up by the author Henry David Thoreau in his essay Civil Disobedience, and later by Gandhi in his doctrine of Satyagraha. Gandhi's Satyagraha was partially influenced and inspired by Shelley's nonviolence in protest and political action. In particular, it is known that Gandhi would often quote Shelley's Masque of Anarchy to vast audiences during the campaign for a free India.\nOutput:  Sh", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-fad4a5c56061488dbcabd5d1e2162aa7", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-fad4a5c56061488dbcabd5d1e2162aa7", "input": "Between Bingen and Bonn, the Middle Rhine flows through the Rhine Gorge, a formation which was created by erosion. The rate of erosion equaled the uplift in the region, such that the river was left at about its original level while the surrounding lands raised. The gorge is quite deep and is the stretch of the river which is known for its many castles and vineyards. It is a UNESCO World Heritage Site (2002) and known as 'the Romantic Rhine', with more than 40 castles and fortresses from the Middle Ages and many quaint and lovely country villages.", "output": ["What flows between the Bingen and Bonn?", "What gorge is between the Bingen and Bonn?", "How was the Rhine Gorge formed?", "What is the area called near the Rhine Gorge with castles from the middle ages?", "What flows between Bingen and Bonn?", "Where does the Middle Rhine flow between Bingen and Bonn?", "What is the Rhine Gorge known for?", "What is the section of the Rhine Gorge recognized by UNESCO called?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Between Bingen and Bonn, the Middle Rhine flows through the Rhine Gorge, a formation which was created by erosion. The rate of erosion equaled the uplift in the region, such that the river was left at about its original level while the surrounding lands raised. The gorge is quite deep and is the stretch of the river which is known for its many castles and vineyards. It is a UNESCO World Heritage Site (2002) and known as 'the Romantic Rhine', with more than 40 castles and fortresses from the Middle Ages and many quaint and lovely country villages.\nOutput: ", "bloom_target": "Where does the Middle Rhine flow between Bingen and Bonn?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Between Bingen and Bonn, the Middle Rhine flows through the Rhine Gorge, a formation which was created by erosion. The rate of erosion equaled the uplift in the region, such that the river was left at about its original level while the surrounding lands raised. The gorge is quite deep and is the stretch of the river which is known for its many castles and vineyards. It is a UNESCO World Heritage Site (2002) and known as 'the Romantic Rhine', with more than 40 castles and fortresses from the Middle Ages and many quaint and lovely country villages.\nOutput:  The", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-1598fbf5ffdf46a4982e9b79265f2fde", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-1598fbf5ffdf46a4982e9b79265f2fde", "input": "Almost all ctenophores are predators, taking prey ranging from microscopic larvae and rotifers to the adults of small crustaceans; the exceptions are juveniles of two species, which live as parasites on the salps on which adults of their species feed. In favorable circumstances, ctenophores can eat ten times their own weight in a day. Only 100\u2013150 species have been validated, and possibly another 25 have not been fully described and named. The textbook examples are cydippids with egg-shaped bodies and a pair of retractable tentacles fringed with tentilla ('little tentacles') that are covered with colloblasts, sticky cells that capture prey. The phylum has a wide range of body forms, including the flattened, deep-sea platyctenids, in which the adults of most species lack combs, and the coastal beroids, which lack tentacles and prey on other ctenophores by using huge mouths armed with groups of large, stiffened cilia that act as teeth. These variations enable different species to build huge populations in the same area, because they specialize in different types of prey, which they capture by as wide a range of methods as spiders use.", "output": ["How much can Ctenophores eat in one day?", "How many species of Ctenophores have been validated?", "How many species of Ctenophores have not been fully described or named?", "How many species of Ctenophora have been validated?", "What are the little tentacles that cydippids have called?", "How much food does a ctenophora eat in a day?", "What do coastal beroids not have that other ctenophora have?", "What does coastal beriods use as teeth?", "How much do ctenophore eat in a day?", "What are the small tentacles on Cydippids called?", "What do Beriods use as teeth?", "What do Cydippids use to capture their prey?", "How many different species of ctenohore are there?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Almost all ctenophores are predators, taking prey ranging from microscopic larvae and rotifers to the adults of small crustaceans; the exceptions are juveniles of two species, which live as parasites on the salps on which adults of their species feed. In favorable circumstances, ctenophores can eat ten times their own weight in a day. Only 100\u2013150 species have been validated, and possibly another 25 have not been fully described and named. The textbook examples are cydippids with egg-shaped bodies and a pair of retractable tentacles fringed with tentilla ('little tentacles') that are covered with colloblasts, sticky cells that capture prey. The phylum has a wide range of body forms, including the flattened, deep-sea platyctenids, in which the adults of most species lack combs, and the coastal beroids, which lack tentacles and prey on other ctenophores by using huge mouths armed with groups of large, stiffened cilia that act as teeth. These variations enable different species to build huge populations in the same area, because they specialize in different types of prey, which they capture by as wide a range of methods as spiders use.\nOutput: ", "bloom_target": "How much do ctenophore eat in a day?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Almost all ctenophores are predators, taking prey ranging from microscopic larvae and rotifers to the adults of small crustaceans; the exceptions are juveniles of two species, which live as parasites on the salps on which adults of their species feed. In favorable circumstances, ctenophores can eat ten times their own weight in a day. Only 100\u2013150 species have been validated, and possibly another 25 have not been fully described and named. The textbook examples are cydippids with egg-shaped bodies and a pair of retractable tentacles fringed with tentilla ('little tentacles') that are covered with colloblasts, sticky cells that capture prey. The phylum has a wide range of body forms, including the flattened, deep-sea platyctenids, in which the adults of most species lack combs, and the coastal beroids, which lack tentacles and prey on other ctenophores by using huge mouths armed with groups of large, stiffened cilia that act as teeth. These variations enable different species to build huge populations in the same area, because they specialize in different types of prey, which they capture by as wide a range of methods as spiders use.\nOutput:  Which", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-17c8bf867b644f5e9677cadc48bd2c37", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-17c8bf867b644f5e9677cadc48bd2c37", "input": "Environmentalists are concerned about loss of biodiversity that will result from destruction of the forest, and also about the release of the carbon contained within the vegetation, which could accelerate global warming. Amazonian evergreen forests account for about 10% of the world's terrestrial primary productivity and 10% of the carbon stores in ecosystems\u2014of the order of 1.1 \u00d7 1011 metric tonnes of carbon. Amazonian forests are estimated to have accumulated 0.62 \u00b1 0.37 tons of carbon per hectare per year between 1975 and 1996.", "output": ["What are environmentalists concerned about losing in the Amazon forest?", "The loss of biodiversity may be the result of what, according to environmentalists?", "What are environmentalists concerned about having released from the Amazon region?", "What amount of the worlds carbon is stored in the Amazon forest?", "How many metric tons of carbon are believed to be stored in the Amazon forest?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Environmentalists are concerned about loss of biodiversity that will result from destruction of the forest, and also about the release of the carbon contained within the vegetation, which could accelerate global warming. Amazonian evergreen forests account for about 10% of the world's terrestrial primary productivity and 10% of the carbon stores in ecosystems\u2014of the order of 1.1 \u00d7 1011 metric tonnes of carbon. Amazonian forests are estimated to have accumulated 0.62 \u00b1 0.37 tons of carbon per hectare per year between 1975 and 1996.\nOutput: ", "bloom_target": "How many metric tons of carbon are believed to be stored in the Amazon forest?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Environmentalists are concerned about loss of biodiversity that will result from destruction of the forest, and also about the release of the carbon contained within the vegetation, which could accelerate global warming. Amazonian evergreen forests account for about 10% of the world's terrestrial primary productivity and 10% of the carbon stores in ecosystems\u2014of the order of 1.1 \u00d7 1011 metric tonnes of carbon. Amazonian forests are estimated to have accumulated 0.62 \u00b1 0.37 tons of carbon per hectare per year between 1975 and 1996.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-2eae9ee428ea46839353f200a4130ea6", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-2eae9ee428ea46839353f200a4130ea6", "input": "After an unmanned LM test flight AS-206, a crew would fly the first Block II CSM and LM in a dual mission known as AS-207/208, or AS-278 (each spacecraft would be launched on a separate Saturn IB.) The Block II crew positions were titled Commander (CDR) Command Module Pilot (CMP) and Lunar Module Pilot (LMP). The astronauts would begin wearing a new Apollo spacesuit, designed to accommodate lunar extravehicular activity (EVA). The traditional visor helmet was replaced with a clear 'fishbowl' type for greater visibility, and the lunar surface EVA suit would include a water-cooled undergarment.", "output": ["How many people were on the test flight of the AS-206?", "What were the astronauts wearing during the dual mission AS-278?", "What was originally on the spacesuits prior to the clear 'fishbowl' helmet?", "What type of undergarment, if any, was included into the Apollo spacesuit?", "What did the LMP acronym stand for regarding the Block II launch positions?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: After an unmanned LM test flight AS-206, a crew would fly the first Block II CSM and LM in a dual mission known as AS-207/208, or AS-278 (each spacecraft would be launched on a separate Saturn IB.) The Block II crew positions were titled Commander (CDR) Command Module Pilot (CMP) and Lunar Module Pilot (LMP). The astronauts would begin wearing a new Apollo spacesuit, designed to accommodate lunar extravehicular activity (EVA). The traditional visor helmet was replaced with a clear 'fishbowl' type for greater visibility, and the lunar surface EVA suit would include a water-cooled undergarment.\nOutput: ", "bloom_target": "How many people were on the test flight of the AS-206?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: After an unmanned LM test flight AS-206, a crew would fly the first Block II CSM and LM in a dual mission known as AS-207/208, or AS-278 (each spacecraft would be launched on a separate Saturn IB.) The Block II crew positions were titled Commander (CDR) Command Module Pilot (CMP) and Lunar Module Pilot (LMP). The astronauts would begin wearing a new Apollo spacesuit, designed to accommodate lunar extravehicular activity (EVA). The traditional visor helmet was replaced with a clear 'fishbowl' type for greater visibility, and the lunar surface EVA suit would include a water-cooled undergarment.\nOutput:  The", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-518cffef22c944c6b46c66233b586a11", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-518cffef22c944c6b46c66233b586a11", "input": "Chloroplasts have their own DNA, often abbreviated as ctDNA, or cpDNA. It is also known as the plastome. Its existence was first proved in 1962, and first sequenced in 1986\u2014when two Japanese research teams sequenced the chloroplast DNA of liverwort and tobacco. Since then, hundreds of chloroplast DNAs from various species have been sequenced, but they're mostly those of land plants and green algae\u2014glaucophytes, red algae, and other algal groups are extremely underrepresented, potentially introducing some bias in views of 'typical' chloroplast DNA structure and content.", "output": ["What is chloroplast DNA abbreviated as?", "What is a synonym for chloroplast DNA?", "When was the plastome discovered?", "When was the first plastome sequenced?", "Who sequenced the first plastome?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Chloroplasts have their own DNA, often abbreviated as ctDNA, or cpDNA. It is also known as the plastome. Its existence was first proved in 1962, and first sequenced in 1986\u2014when two Japanese research teams sequenced the chloroplast DNA of liverwort and tobacco. Since then, hundreds of chloroplast DNAs from various species have been sequenced, but they're mostly those of land plants and green algae\u2014glaucophytes, red algae, and other algal groups are extremely underrepresented, potentially introducing some bias in views of 'typical' chloroplast DNA structure and content.\nOutput: ", "bloom_target": "When was the first plastome sequenced?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Chloroplasts have their own DNA, often abbreviated as ctDNA, or cpDNA. It is also known as the plastome. Its existence was first proved in 1962, and first sequenced in 1986\u2014when two Japanese research teams sequenced the chloroplast DNA of liverwort and tobacco. Since then, hundreds of chloroplast DNAs from various species have been sequenced, but they're mostly those of land plants and green algae\u2014glaucophytes, red algae, and other algal groups are extremely underrepresented, potentially introducing some bias in views of 'typical' chloroplast DNA structure and content.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-4d75f7e21056406dbb39886456a3d8e1", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-4d75f7e21056406dbb39886456a3d8e1", "input": "In the summer of 1521, Luther widened his target from individual pieties like indulgences and pilgrimages to doctrines at the heart of Church practices. In On the Abrogation of the Private Mass, he condemned as idolatry the idea that the mass is a sacrifice, asserting instead that it is a gift, to be received with thanksgiving by the whole congregation. His essay On Confession, Whether the Pope has the Power to Require It rejected compulsory confession and encouraged private confession and absolution, since 'every Christian is a confessor.' In November, Luther wrote The Judgement of Martin Luther on Monastic Vows. He assured monks and nuns that they could break their vows without sin, because vows were an illegitimate and vain attempt to win salvation.", "output": ["When did Luther broaden his attacks to include core Church doctrines?", "How did Luther describe the mass that was viewed as sacrifice?", "What did Luther call the mass instead of sacrifice?", "After rejecting compulsory confession, what did Luther call for?", "What did Luther tell monks and nuns about their vows?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In the summer of 1521, Luther widened his target from individual pieties like indulgences and pilgrimages to doctrines at the heart of Church practices. In On the Abrogation of the Private Mass, he condemned as idolatry the idea that the mass is a sacrifice, asserting instead that it is a gift, to be received with thanksgiving by the whole congregation. His essay On Confession, Whether the Pope has the Power to Require It rejected compulsory confession and encouraged private confession and absolution, since 'every Christian is a confessor.' In November, Luther wrote The Judgement of Martin Luther on Monastic Vows. He assured monks and nuns that they could break their vows without sin, because vows were an illegitimate and vain attempt to win salvation.\nOutput: ", "bloom_target": "What did Luther tell monks and nuns about their vows?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In the summer of 1521, Luther widened his target from individual pieties like indulgences and pilgrimages to doctrines at the heart of Church practices. In On the Abrogation of the Private Mass, he condemned as idolatry the idea that the mass is a sacrifice, asserting instead that it is a gift, to be received with thanksgiving by the whole congregation. His essay On Confession, Whether the Pope has the Power to Require It rejected compulsory confession and encouraged private confession and absolution, since 'every Christian is a confessor.' In November, Luther wrote The Judgement of Martin Luther on Monastic Vows. He assured monks and nuns that they could break their vows without sin, because vows were an illegitimate and vain attempt to win salvation.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-231f3bc6c2ce45d1883f690bfbe75717", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-231f3bc6c2ce45d1883f690bfbe75717", "input": "After leaving Edison's company Tesla partnered with two businessmen in 1886, Robert Lane and Benjamin Vail, who agreed to finance an electric lighting company in Tesla's name, Tesla Electric Light & Manufacturing. The company installed electrical arc light based illumination systems designed by Tesla and also had designs for dynamo electric machine commutators, the first patents issued to Tesla in the US.", "output": ["Who did Tesla partner with in 1886?", "What did lane and vail finance?", "What did Tesla Electric Light & Manufacturing do?", "What did Tesla first receive after starting his company?", "What was produced at tesla's company?", "What were the names of Tesla's new partners?", "When did the partnership between Tesla, Lane and Vail form?", "What was the name of the company the businessmen financed?", "Who designed the illumination systems that Tesla Electric Light & Manufacturing installed?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: After leaving Edison's company Tesla partnered with two businessmen in 1886, Robert Lane and Benjamin Vail, who agreed to finance an electric lighting company in Tesla's name, Tesla Electric Light & Manufacturing. The company installed electrical arc light based illumination systems designed by Tesla and also had designs for dynamo electric machine commutators, the first patents issued to Tesla in the US.\nOutput: ", "bloom_target": "What were the names of Tesla's new partners?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: After leaving Edison's company Tesla partnered with two businessmen in 1886, Robert Lane and Benjamin Vail, who agreed to finance an electric lighting company in Tesla's name, Tesla Electric Light & Manufacturing. The company installed electrical arc light based illumination systems designed by Tesla and also had designs for dynamo electric machine commutators, the first patents issued to Tesla in the US.\nOutput:  The", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-36669aee3cd24744a4876792800f0fd7", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-36669aee3cd24744a4876792800f0fd7", "input": "Because of their soft, gelatinous bodies, ctenophores are extremely rare as fossils, and fossils that have been interpreted as ctenophores have been found only in lagerst\u00e4tten, places where the environment was exceptionally suited to preservation of soft tissue. Until the mid-1990s only two specimens good enough for analysis were known, both members of the crown group, from the early Devonian (Emsian) period. Three additional putative species were then found in the Burgess Shale and other Canadian rocks of similar age, about 505 million years ago in the mid-Cambrian period. All three apparently lacked tentacles but had between 24 and 80 comb rows, far more than the 8 typical of living species. They also appear to have had internal organ-like structures unlike anything found in living ctenophores. One of the fossil species first reported in 1996 had a large mouth, apparently surrounded by a folded edge that may have been muscular. Evidence from China a year later suggests that such ctenophores were widespread in the Cambrian, but perhaps very different from modern species \u2013 for example one fossil's comb-rows were mounted on prominent vanes. The Ediacaran Eoandromeda could putatively represent a comb jelly.", "output": ["Why are ctenophores extremely rare as fossils?", "Ediacaran eoandromeda can be regarded to represent what?", "What was the period called that was 505 million years ago?", "How many species were found in the Burgess Shale?", "What did the  fossils found in the Burgess Shale lack?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Because of their soft, gelatinous bodies, ctenophores are extremely rare as fossils, and fossils that have been interpreted as ctenophores have been found only in lagerst\u00e4tten, places where the environment was exceptionally suited to preservation of soft tissue. Until the mid-1990s only two specimens good enough for analysis were known, both members of the crown group, from the early Devonian (Emsian) period. Three additional putative species were then found in the Burgess Shale and other Canadian rocks of similar age, about 505 million years ago in the mid-Cambrian period. All three apparently lacked tentacles but had between 24 and 80 comb rows, far more than the 8 typical of living species. They also appear to have had internal organ-like structures unlike anything found in living ctenophores. One of the fossil species first reported in 1996 had a large mouth, apparently surrounded by a folded edge that may have been muscular. Evidence from China a year later suggests that such ctenophores were widespread in the Cambrian, but perhaps very different from modern species \u2013 for example one fossil's comb-rows were mounted on prominent vanes. The Ediacaran Eoandromeda could putatively represent a comb jelly.\nOutput: ", "bloom_target": "What was the period called that was 505 million years ago?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Because of their soft, gelatinous bodies, ctenophores are extremely rare as fossils, and fossils that have been interpreted as ctenophores have been found only in lagerst\u00e4tten, places where the environment was exceptionally suited to preservation of soft tissue. Until the mid-1990s only two specimens good enough for analysis were known, both members of the crown group, from the early Devonian (Emsian) period. Three additional putative species were then found in the Burgess Shale and other Canadian rocks of similar age, about 505 million years ago in the mid-Cambrian period. All three apparently lacked tentacles but had between 24 and 80 comb rows, far more than the 8 typical of living species. They also appear to have had internal organ-like structures unlike anything found in living ctenophores. One of the fossil species first reported in 1996 had a large mouth, apparently surrounded by a folded edge that may have been muscular. Evidence from China a year later suggests that such ctenophores were widespread in the Cambrian, but perhaps very different from modern species \u2013 for example one fossil's comb-rows were mounted on prominent vanes. The Ediacaran Eoandromeda could putatively represent a comb jelly.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-dd715d3be4e74babb739ad53acb20366", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-dd715d3be4e74babb739ad53acb20366", "input": "As Jamukha and Tem\u00fcjin drifted apart in their friendship, each began consolidating power, and soon became rivals. Jamukha supported the traditional Mongolian aristocracy, while Tem\u00fcjin followed a meritocratic method, and attracted a broader, though lower class, range of followers. Due to his earlier defeat of the Merkits, and a proclamation by the shaman Kokochu that the Eternal Blue Sky had set aside the world for Tem\u00fcjin, Tem\u00fcjin began rising to power. In 1186, Tem\u00fcjin was elected khan of the Mongols. However, Jamukha, threatened by Tem\u00fcjin's rapid ascent, quickly moved to stop Tem\u00fcjin's ambitions. In 1187, he launched an attack against his former friend with an army of thirty thousand troops. Tem\u00fcjin hastily gathered together his followers to defend against the attack, but he was decisively beaten in the Battle of Dalan Balzhut. Jamukha horrified people greatly and harmed his image by boiling seventy young male captives alive in cauldrons, alienating many of his potential followers and eliciting sympathy for Tem\u00fcjin. Toghrul, as Tem\u00fcjin's patron, was exiled to the Qara Khitai. The life of Tem\u00fcjin for the next ten years is very unclear, as historical records are mostly silent on that period.", "output": ["Who did Jamukha support that were not part of Tem\u00fcjin's power base?", "Which shaman's proclamation aided Tem\u00fcjin's rise?", "When was Tem\u00fcjin elected khan of the Mongols?", "Which battle did Tem\u00fcjin lose to Jamukha shortly after his election as khan?", "Where was Tem\u00fcjin's patron Toghrul exiled?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: As Jamukha and Tem\u00fcjin drifted apart in their friendship, each began consolidating power, and soon became rivals. Jamukha supported the traditional Mongolian aristocracy, while Tem\u00fcjin followed a meritocratic method, and attracted a broader, though lower class, range of followers. Due to his earlier defeat of the Merkits, and a proclamation by the shaman Kokochu that the Eternal Blue Sky had set aside the world for Tem\u00fcjin, Tem\u00fcjin began rising to power. In 1186, Tem\u00fcjin was elected khan of the Mongols. However, Jamukha, threatened by Tem\u00fcjin's rapid ascent, quickly moved to stop Tem\u00fcjin's ambitions. In 1187, he launched an attack against his former friend with an army of thirty thousand troops. Tem\u00fcjin hastily gathered together his followers to defend against the attack, but he was decisively beaten in the Battle of Dalan Balzhut. Jamukha horrified people greatly and harmed his image by boiling seventy young male captives alive in cauldrons, alienating many of his potential followers and eliciting sympathy for Tem\u00fcjin. Toghrul, as Tem\u00fcjin's patron, was exiled to the Qara Khitai. The life of Tem\u00fcjin for the next ten years is very unclear, as historical records are mostly silent on that period.\nOutput: ", "bloom_target": "When was Tem\u00fcjin elected khan of the Mongols?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: As Jamukha and Tem\u00fcjin drifted apart in their friendship, each began consolidating power, and soon became rivals. Jamukha supported the traditional Mongolian aristocracy, while Tem\u00fcjin followed a meritocratic method, and attracted a broader, though lower class, range of followers. Due to his earlier defeat of the Merkits, and a proclamation by the shaman Kokochu that the Eternal Blue Sky had set aside the world for Tem\u00fcjin, Tem\u00fcjin began rising to power. In 1186, Tem\u00fcjin was elected khan of the Mongols. However, Jamukha, threatened by Tem\u00fcjin's rapid ascent, quickly moved to stop Tem\u00fcjin's ambitions. In 1187, he launched an attack against his former friend with an army of thirty thousand troops. Tem\u00fcjin hastily gathered together his followers to defend against the attack, but he was decisively beaten in the Battle of Dalan Balzhut. Jamukha horrified people greatly and harmed his image by boiling seventy young male captives alive in cauldrons, alienating many of his potential followers and eliciting sympathy for Tem\u00fcjin. Toghrul, as Tem\u00fcjin's patron, was exiled to the Qara Khitai. The life of Tem\u00fcjin for the next ten years is very unclear, as historical records are mostly silent on that period.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-8f588c911e8445c4baa07d765ba7311c", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-8f588c911e8445c4baa07d765ba7311c", "input": "British settlers outnumbered the French 20 to 1 with a population of about 1.5 million ranged along the eastern coast of the continent, from Nova Scotia and Newfoundland in the north, to Georgia in the south. Many of the older colonies had land claims that extended arbitrarily far to the west, as the extent of the continent was unknown at the time their provincial charters were granted. While their population centers were along the coast, the settlements were growing into the interior. Nova Scotia, which had been captured from France in 1713, still had a significant French-speaking population. Britain also claimed Rupert's Land, where the Hudson's Bay Company traded for furs with local tribes.", "output": ["What was the ratio of British settler to French?", "Where did British settlers live?", "Where were populations centered in colonies?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: British settlers outnumbered the French 20 to 1 with a population of about 1.5 million ranged along the eastern coast of the continent, from Nova Scotia and Newfoundland in the north, to Georgia in the south. Many of the older colonies had land claims that extended arbitrarily far to the west, as the extent of the continent was unknown at the time their provincial charters were granted. While their population centers were along the coast, the settlements were growing into the interior. Nova Scotia, which had been captured from France in 1713, still had a significant French-speaking population. Britain also claimed Rupert's Land, where the Hudson's Bay Company traded for furs with local tribes.\nOutput: ", "bloom_target": "What was the ratio of British settler to French?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: British settlers outnumbered the French 20 to 1 with a population of about 1.5 million ranged along the eastern coast of the continent, from Nova Scotia and Newfoundland in the north, to Georgia in the south. Many of the older colonies had land claims that extended arbitrarily far to the west, as the extent of the continent was unknown at the time their provincial charters were granted. While their population centers were along the coast, the settlements were growing into the interior. Nova Scotia, which had been captured from France in 1713, still had a significant French-speaking population. Britain also claimed Rupert's Land, where the Hudson's Bay Company traded for furs with local tribes.\nOutput:  How", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-2b256b5fa8cc440899c45f58bad06c74", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-2b256b5fa8cc440899c45f58bad06c74", "input": "Although the reciprocating steam engine is no longer in widespread commercial use, various companies are exploring or exploiting the potential of the engine as an alternative to internal combustion engines. The company Energiprojekt AB in Sweden has made progress in using modern materials for harnessing the power of steam. The efficiency of Energiprojekt's steam engine reaches some 27-30% on high-pressure engines. It is a single-step, 5-cylinder engine (no compound) with superheated steam and consumes approx. 4 kg (8.8 lb) of steam per kWh.[not in citation given]", "output": ["What modern company has been notably working on a steam engine using modern materials?", "Where is Energiprojekt AB based?", "How many cylinders does the Energiprojekt AB engine have?", "How many pounds of steam per kilowatt hour does the Energiprojekt AB engine use?", "What percentage of a high pressure engine's efficiency has the Energiprojekt AB engine achieved?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Although the reciprocating steam engine is no longer in widespread commercial use, various companies are exploring or exploiting the potential of the engine as an alternative to internal combustion engines. The company Energiprojekt AB in Sweden has made progress in using modern materials for harnessing the power of steam. The efficiency of Energiprojekt's steam engine reaches some 27-30% on high-pressure engines. It is a single-step, 5-cylinder engine (no compound) with superheated steam and consumes approx. 4 kg (8.8 lb) of steam per kWh.[not in citation given]\nOutput: ", "bloom_target": "How many pounds of steam per kilowatt hour does the Energiprojekt AB engine use?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Although the reciprocating steam engine is no longer in widespread commercial use, various companies are exploring or exploiting the potential of the engine as an alternative to internal combustion engines. The company Energiprojekt AB in Sweden has made progress in using modern materials for harnessing the power of steam. The efficiency of Energiprojekt's steam engine reaches some 27-30% on high-pressure engines. It is a single-step, 5-cylinder engine (no compound) with superheated steam and consumes approx. 4 kg (8.8 lb) of steam per kWh.[not in citation given]\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-bfcf574e754e4c148915213ec8b10ec5", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-bfcf574e754e4c148915213ec8b10ec5", "input": "Fresno is served by State Route 99, the main north/south freeway that connects the major population centers of the California Central Valley. State Route 168, the Sierra Freeway, heads east to the city of Clovis and Huntington Lake. State Route 41 (Yosemite Freeway/Eisenhower Freeway) comes into Fresno from Atascadero in the south, and then heads north to Yosemite. State Route 180 (Kings Canyon Freeway) comes from the west via Mendota, and from the east in Kings Canyon National Park going towards the city of Reedley.", "output": ["What route connects Fresno with the California Central Valley?", "What is another name for State Route 168?", "What is another name for the Yosemite Freeway?", "State Route 180 comes from which direction via Mendota?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Fresno is served by State Route 99, the main north/south freeway that connects the major population centers of the California Central Valley. State Route 168, the Sierra Freeway, heads east to the city of Clovis and Huntington Lake. State Route 41 (Yosemite Freeway/Eisenhower Freeway) comes into Fresno from Atascadero in the south, and then heads north to Yosemite. State Route 180 (Kings Canyon Freeway) comes from the west via Mendota, and from the east in Kings Canyon National Park going towards the city of Reedley.\nOutput: ", "bloom_target": "What route connects Fresno with the California Central Valley?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Fresno is served by State Route 99, the main north/south freeway that connects the major population centers of the California Central Valley. State Route 168, the Sierra Freeway, heads east to the city of Clovis and Huntington Lake. State Route 41 (Yosemite Freeway/Eisenhower Freeway) comes into Fresno from Atascadero in the south, and then heads north to Yosemite. State Route 180 (Kings Canyon Freeway) comes from the west via Mendota, and from the east in Kings Canyon National Park going towards the city of Reedley.\nOutput:  The", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-e4a7154bb33e4c428e84c05659a5f93a", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-e4a7154bb33e4c428e84c05659a5f93a", "input": "In 1993, Galor and Zeira showed that inequality in the presence of credit market imperfections has a long lasting detrimental effect on human capital formation and economic development. A 1996 study by Perotti examined the channels through which inequality may affect economic growth. He showed that, in accordance with the credit market imperfection approach, inequality is associated with lower level of human capital formation (education, experience, and apprenticeship) and higher level of fertility, and thereby lower levels of growth. He found that inequality is associated with higher levels of redistributive taxation, which is associated with lower levels of growth from reductions in private savings and investment. Perotti concluded that, 'more equal societies have lower fertility rates and higher rates of investment in education. Both are reflected in higher rates of growth. Also, very unequal societies tend to be politically and socially unstable, which is reflected in lower rates of investment and therefore growth.'", "output": ["When did Galor and Zeria show new information about inequality?", "Inequality in the presence of credit market imperfections has what kind of effect on human capital formation?", "What did a 1996 study by Perotti examine?", "What is inequality associated with higher levels of?", "What do extremely unequal societies tend to be?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In 1993, Galor and Zeira showed that inequality in the presence of credit market imperfections has a long lasting detrimental effect on human capital formation and economic development. A 1996 study by Perotti examined the channels through which inequality may affect economic growth. He showed that, in accordance with the credit market imperfection approach, inequality is associated with lower level of human capital formation (education, experience, and apprenticeship) and higher level of fertility, and thereby lower levels of growth. He found that inequality is associated with higher levels of redistributive taxation, which is associated with lower levels of growth from reductions in private savings and investment. Perotti concluded that, 'more equal societies have lower fertility rates and higher rates of investment in education. Both are reflected in higher rates of growth. Also, very unequal societies tend to be politically and socially unstable, which is reflected in lower rates of investment and therefore growth.'\nOutput: ", "bloom_target": "When did Galor and Zeria show new information about inequality?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In 1993, Galor and Zeira showed that inequality in the presence of credit market imperfections has a long lasting detrimental effect on human capital formation and economic development. A 1996 study by Perotti examined the channels through which inequality may affect economic growth. He showed that, in accordance with the credit market imperfection approach, inequality is associated with lower level of human capital formation (education, experience, and apprenticeship) and higher level of fertility, and thereby lower levels of growth. He found that inequality is associated with higher levels of redistributive taxation, which is associated with lower levels of growth from reductions in private savings and investment. Perotti concluded that, 'more equal societies have lower fertility rates and higher rates of investment in education. Both are reflected in higher rates of growth. Also, very unequal societies tend to be politically and socially unstable, which is reflected in lower rates of investment and therefore growth.'\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-432c41838c3c459d94ded6fdfd35bba1", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-432c41838c3c459d94ded6fdfd35bba1", "input": "Cultural imperialism is when a country's influence is felt in social and cultural circles, i.e. its soft power, such that it changes the moral, cultural and societal worldview of another. This is more than just 'foreign' music, television or film becoming popular with young people, but that popular culture changing their own expectations of life and their desire for their own country to become more like the foreign country depicted. For example, depictions of opulent American lifestyles in the soap opera Dallas during the Cold War changed the expectations of Romanians; a more recent example is the influence of smuggled South Korean drama series in North Korea. The importance of soft power is not lost on authoritarian regimes, fighting such influence with bans on foreign popular culture, control of the internet and unauthorised satellite dishes etc. Nor is such a usage of culture recent, as part of Roman imperialism local elites would be exposed to the benefits and luxuries of Roman culture and lifestyle, with the aim that they would then become willing participants.", "output": ["When imperialism impacts social norms of a state, what is it called?", "What is Cultural Imperialism often referred to as?", "Which American show changed the views of Romanians during the cold war?", "Which historic empire used cultural imperialism to sway local elites?", "How do regimes fight against cultural imperialism?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Cultural imperialism is when a country's influence is felt in social and cultural circles, i.e. its soft power, such that it changes the moral, cultural and societal worldview of another. This is more than just 'foreign' music, television or film becoming popular with young people, but that popular culture changing their own expectations of life and their desire for their own country to become more like the foreign country depicted. For example, depictions of opulent American lifestyles in the soap opera Dallas during the Cold War changed the expectations of Romanians; a more recent example is the influence of smuggled South Korean drama series in North Korea. The importance of soft power is not lost on authoritarian regimes, fighting such influence with bans on foreign popular culture, control of the internet and unauthorised satellite dishes etc. Nor is such a usage of culture recent, as part of Roman imperialism local elites would be exposed to the benefits and luxuries of Roman culture and lifestyle, with the aim that they would then become willing participants.\nOutput: ", "bloom_target": "How do regimes fight against cultural imperialism?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Cultural imperialism is when a country's influence is felt in social and cultural circles, i.e. its soft power, such that it changes the moral, cultural and societal worldview of another. This is more than just 'foreign' music, television or film becoming popular with young people, but that popular culture changing their own expectations of life and their desire for their own country to become more like the foreign country depicted. For example, depictions of opulent American lifestyles in the soap opera Dallas during the Cold War changed the expectations of Romanians; a more recent example is the influence of smuggled South Korean drama series in North Korea. The importance of soft power is not lost on authoritarian regimes, fighting such influence with bans on foreign popular culture, control of the internet and unauthorised satellite dishes etc. Nor is such a usage of culture recent, as part of Roman imperialism local elites would be exposed to the benefits and luxuries of Roman culture and lifestyle, with the aim that they would then become willing participants.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-d4f26b25919e4ccf845a6fc7a760f6f4", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-d4f26b25919e4ccf845a6fc7a760f6f4", "input": "A further type of committee is normally set up to scrutinise private bills submitted to the Scottish Parliament by an outside party or promoter who is not a member of the Scottish Parliament or Scottish Government. Private bills normally relate to large-scale development projects such as infrastructure projects that require the use of land or property. Private Bill Committees have been set up to consider legislation on issues such as the development of the Edinburgh Tram Network, the Glasgow Airport Rail Link, the Airdrie-Bathgate Rail Link and extensions to the National Gallery of Scotland.", "output": ["What is set up to scrutinize private bills submitted by party outsiders?", "What topic do private bills typically have?", "Who decides how land or property is allowed to be used?", "What kind of committee considered legislation on the development of the Edinburgh Tram Network?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: A further type of committee is normally set up to scrutinise private bills submitted to the Scottish Parliament by an outside party or promoter who is not a member of the Scottish Parliament or Scottish Government. Private bills normally relate to large-scale development projects such as infrastructure projects that require the use of land or property. Private Bill Committees have been set up to consider legislation on issues such as the development of the Edinburgh Tram Network, the Glasgow Airport Rail Link, the Airdrie-Bathgate Rail Link and extensions to the National Gallery of Scotland.\nOutput: ", "bloom_target": "What is set up to scrutinize private bills submitted by party outsiders?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: A further type of committee is normally set up to scrutinise private bills submitted to the Scottish Parliament by an outside party or promoter who is not a member of the Scottish Parliament or Scottish Government. Private bills normally relate to large-scale development projects such as infrastructure projects that require the use of land or property. Private Bill Committees have been set up to consider legislation on issues such as the development of the Edinburgh Tram Network, the Glasgow Airport Rail Link, the Airdrie-Bathgate Rail Link and extensions to the National Gallery of Scotland.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-fe61170f42664efd9a18bc648923c239", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-fe61170f42664efd9a18bc648923c239", "input": "However, a problem emerged regarding the directions taken by ABC and UPT. In 1950, Noble appointed Robert Kintner to be ABC's president while he himself served as its CEO, a position he would hold until his death in 1958. Despite the promise of non-interference between ABC and UPT, Goldenson had to intervene in ABC's decisions because of financial problems and the FCC's long period of indecision. Goldenson added to the confusion when, in October 1954, he proposed a merger between UPT and the DuMont Television Network, which was also mired in financial trouble. As part of this merger, the network would have been renamed 'ABC-DuMont' for five years, and DuMont would have received $5 million in cash, room on the schedule for existing DuMont programming, and guaranteed advertising time for DuMont Laboratories receivers. In addition, to comply with FCC ownership restrictions, it would have been required to sell either WABC-TV or DuMont owned-and-operated station WABD in the New York City market, as well as two other stations. The merged ABC-DuMont would have had the resources to compete with CBS and NBC.", "output": ["Who was appointed to be ABC's president by Noble in 1950?", "Goldenson proposed a merger between UPT and what network in October 1954?", "Under Goldenson's merger plan, what would the new entity be named?", "How much money was to go to DuMont Television Network under Goldenson's merger plan?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: However, a problem emerged regarding the directions taken by ABC and UPT. In 1950, Noble appointed Robert Kintner to be ABC's president while he himself served as its CEO, a position he would hold until his death in 1958. Despite the promise of non-interference between ABC and UPT, Goldenson had to intervene in ABC's decisions because of financial problems and the FCC's long period of indecision. Goldenson added to the confusion when, in October 1954, he proposed a merger between UPT and the DuMont Television Network, which was also mired in financial trouble. As part of this merger, the network would have been renamed 'ABC-DuMont' for five years, and DuMont would have received $5 million in cash, room on the schedule for existing DuMont programming, and guaranteed advertising time for DuMont Laboratories receivers. In addition, to comply with FCC ownership restrictions, it would have been required to sell either WABC-TV or DuMont owned-and-operated station WABD in the New York City market, as well as two other stations. The merged ABC-DuMont would have had the resources to compete with CBS and NBC.\nOutput: ", "bloom_target": "Under Goldenson's merger plan, what would the new entity be named?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: However, a problem emerged regarding the directions taken by ABC and UPT. In 1950, Noble appointed Robert Kintner to be ABC's president while he himself served as its CEO, a position he would hold until his death in 1958. Despite the promise of non-interference between ABC and UPT, Goldenson had to intervene in ABC's decisions because of financial problems and the FCC's long period of indecision. Goldenson added to the confusion when, in October 1954, he proposed a merger between UPT and the DuMont Television Network, which was also mired in financial trouble. As part of this merger, the network would have been renamed 'ABC-DuMont' for five years, and DuMont would have received $5 million in cash, room on the schedule for existing DuMont programming, and guaranteed advertising time for DuMont Laboratories receivers. In addition, to comply with FCC ownership restrictions, it would have been required to sell either WABC-TV or DuMont owned-and-operated station WABD in the New York City market, as well as two other stations. The merged ABC-DuMont would have had the resources to compete with CBS and NBC.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-1145ad50023e43e488bdafc44f144948", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-1145ad50023e43e488bdafc44f144948", "input": "In 1700 several hundred French Huguenots migrated from England to the colony of Virginia, where the English Crown had promised them land grants in Lower Norfolk County. When they arrived, colonial authorities offered them instead land 20 miles above the falls of the James River, at the abandoned Monacan village known as Manakin Town, now in Powhatan County. Some settlers landed in present-day Chesterfield County. On 12 May 1705, the Virginia General Assembly passed an act to naturalise the 148 Huguenots still resident at Manakintown. Of the original 390 settlers in the isolated settlement, many had died; others lived outside town on farms in the English style; and others moved to different areas. Gradually they intermarried with their English neighbors. Through the 18th and 19th centuries, descendants of the French migrated west into the Piedmont, and across the Appalachian Mountains into the West of what became Kentucky, Tennessee, Missouri, and other states. In the Manakintown area, the Huguenot Memorial Bridge across the James River and Huguenot Road were named in their honor, as were many local features, including several schools, including Huguenot High School.", "output": ["In what English colony were Huguenot settlers promised land?", "In what area of this British colony were Huguenot land grants?", "What town was actually granted to the Huguenots on arrival?", "How many settlers original settled in Manakintown?", "When were these settlers naturalized as English colonists?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In 1700 several hundred French Huguenots migrated from England to the colony of Virginia, where the English Crown had promised them land grants in Lower Norfolk County. When they arrived, colonial authorities offered them instead land 20 miles above the falls of the James River, at the abandoned Monacan village known as Manakin Town, now in Powhatan County. Some settlers landed in present-day Chesterfield County. On 12 May 1705, the Virginia General Assembly passed an act to naturalise the 148 Huguenots still resident at Manakintown. Of the original 390 settlers in the isolated settlement, many had died; others lived outside town on farms in the English style; and others moved to different areas. Gradually they intermarried with their English neighbors. Through the 18th and 19th centuries, descendants of the French migrated west into the Piedmont, and across the Appalachian Mountains into the West of what became Kentucky, Tennessee, Missouri, and other states. In the Manakintown area, the Huguenot Memorial Bridge across the James River and Huguenot Road were named in their honor, as were many local features, including several schools, including Huguenot High School.\nOutput: ", "bloom_target": "In what English colony were Huguenot settlers promised land?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In 1700 several hundred French Huguenots migrated from England to the colony of Virginia, where the English Crown had promised them land grants in Lower Norfolk County. When they arrived, colonial authorities offered them instead land 20 miles above the falls of the James River, at the abandoned Monacan village known as Manakin Town, now in Powhatan County. Some settlers landed in present-day Chesterfield County. On 12 May 1705, the Virginia General Assembly passed an act to naturalise the 148 Huguenots still resident at Manakintown. Of the original 390 settlers in the isolated settlement, many had died; others lived outside town on farms in the English style; and others moved to different areas. Gradually they intermarried with their English neighbors. Through the 18th and 19th centuries, descendants of the French migrated west into the Piedmont, and across the Appalachian Mountains into the West of what became Kentucky, Tennessee, Missouri, and other states. In the Manakintown area, the Huguenot Memorial Bridge across the James River and Huguenot Road were named in their honor, as were many local features, including several schools, including Huguenot High School.\nOutput:  The", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-34b9f866578b4e7a8f9bcf16125bd3a9", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-34b9f866578b4e7a8f9bcf16125bd3a9", "input": "Erg\u00e4nzungsschulen are secondary or post-secondary (non-tertiary) schools, which are run by private individuals, private organizations or rarely, religious groups and offer a type of education which is not available at public schools. Most of these schools are vocational schools. However, these vocational schools are not part of the German dual education system. Erg\u00e4nzungsschulen have the freedom to operate outside of government regulation and are funded in whole by charging their students tuition fees.", "output": ["What are private secondary schools in Germany called?", "What types of schools are most erg\u00e4nzungsschulen?", "How are erg\u00e4nzungsschulen funded?", "Along with private individuals and organizations, what groups sometimes runs erg\u00e4nzungsschulen?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Erg\u00e4nzungsschulen are secondary or post-secondary (non-tertiary) schools, which are run by private individuals, private organizations or rarely, religious groups and offer a type of education which is not available at public schools. Most of these schools are vocational schools. However, these vocational schools are not part of the German dual education system. Erg\u00e4nzungsschulen have the freedom to operate outside of government regulation and are funded in whole by charging their students tuition fees.\nOutput: ", "bloom_target": "What types of schools are most erg\u00e4nzungsschulen?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Erg\u00e4nzungsschulen are secondary or post-secondary (non-tertiary) schools, which are run by private individuals, private organizations or rarely, religious groups and offer a type of education which is not available at public schools. Most of these schools are vocational schools. However, these vocational schools are not part of the German dual education system. Erg\u00e4nzungsschulen have the freedom to operate outside of government regulation and are funded in whole by charging their students tuition fees.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-c0005faaaeed4133be989f2462ec135c", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-c0005faaaeed4133be989f2462ec135c", "input": "In 1973, Nixon named William E. Simon as the first Administrator of the Federal Energy Office, a short-term organization created to coordinate the response to the embargo. Simon allocated states the same amount of domestic oil for 1974 that each had consumed in 1972, which worked for states whose populations were not increasing. In other states, lines at gasoline stations were common. The American Automobile Association reported that in the last week of February 1974, 20% of American gasoline stations had no fuel.", "output": ["Who is the first administrator of the Federal Energy Office?", "When was he elected by Nixon?", "Why was this short termed organization created?", "According to the AAA, what is the percentage of the gas stations that ran out of gasoline?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In 1973, Nixon named William E. Simon as the first Administrator of the Federal Energy Office, a short-term organization created to coordinate the response to the embargo. Simon allocated states the same amount of domestic oil for 1974 that each had consumed in 1972, which worked for states whose populations were not increasing. In other states, lines at gasoline stations were common. The American Automobile Association reported that in the last week of February 1974, 20% of American gasoline stations had no fuel.\nOutput: ", "bloom_target": "According to the AAA, what is the percentage of the gas stations that ran out of gasoline?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In 1973, Nixon named William E. Simon as the first Administrator of the Federal Energy Office, a short-term organization created to coordinate the response to the embargo. Simon allocated states the same amount of domestic oil for 1974 that each had consumed in 1972, which worked for states whose populations were not increasing. In other states, lines at gasoline stations were common. The American Automobile Association reported that in the last week of February 1974, 20% of American gasoline stations had no fuel.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-fc6e0ff6e9e443b88715e36028e2aed5", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-fc6e0ff6e9e443b88715e36028e2aed5", "input": "ARPANET and SITA HLN became operational in 1969. Before the introduction of X.25 in 1973, about twenty different network technologies had been developed. Two fundamental differences involved the division of functions and tasks between the hosts at the edge of the network and the network core. In the datagram system, the hosts have the responsibility to ensure orderly delivery of packets. The User Datagram Protocol (UDP) is an example of a datagram protocol. In the virtual call system, the network guarantees sequenced delivery of data to the host. This results in a simpler host interface with less functionality than in the datagram model. The X.25 protocol suite uses this network type.", "output": ["WHen did ARPNET and SITA become operational", "2 differences betwen X.25 and ARPNET CITA technologies", "WHat does UserDatagram Protocol gaurentee", "X.25 uses what type network type"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: ARPANET and SITA HLN became operational in 1969. Before the introduction of X.25 in 1973, about twenty different network technologies had been developed. Two fundamental differences involved the division of functions and tasks between the hosts at the edge of the network and the network core. In the datagram system, the hosts have the responsibility to ensure orderly delivery of packets. The User Datagram Protocol (UDP) is an example of a datagram protocol. In the virtual call system, the network guarantees sequenced delivery of data to the host. This results in a simpler host interface with less functionality than in the datagram model. The X.25 protocol suite uses this network type.\nOutput: ", "bloom_target": "2 differences betwen X.25 and ARPNET CITA technologies", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: ARPANET and SITA HLN became operational in 1969. Before the introduction of X.25 in 1973, about twenty different network technologies had been developed. Two fundamental differences involved the division of functions and tasks between the hosts at the edge of the network and the network core. In the datagram system, the hosts have the responsibility to ensure orderly delivery of packets. The User Datagram Protocol (UDP) is an example of a datagram protocol. In the virtual call system, the network guarantees sequenced delivery of data to the host. This results in a simpler host interface with less functionality than in the datagram model. The X.25 protocol suite uses this network type.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-ad6cf6afe4724262a2fc7b7ff04db80e", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-ad6cf6afe4724262a2fc7b7ff04db80e", "input": "In the United States, each state determines the requirements for getting a license to teach in public schools. Teaching certification generally lasts three years, but teachers can receive certificates that last as long as ten years. Public school teachers are required to have a bachelor's degree and the majority must be certified by the state in which they teach. Many charter schools do not require that their teachers be certified, provided they meet the standards to be highly qualified as set by No Child Left Behind. Additionally, the requirements for substitute/temporary teachers are generally not as rigorous as those for full-time professionals. The Bureau of Labor Statistics estimates that there are 1.4 million elementary school teachers, 674,000 middle school teachers, and 1 million secondary school teachers employed in the U.S.", "output": ["In the US, who decides on the requirements for teachers?", "What is the longest time that a teaching certificate is good for?", "What must a public school teacher have, at a minimum?", "Who may not require that its teachers be certified?", "What may a Charter school require that their teachers meet the standards to be highly qualified by?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In the United States, each state determines the requirements for getting a license to teach in public schools. Teaching certification generally lasts three years, but teachers can receive certificates that last as long as ten years. Public school teachers are required to have a bachelor's degree and the majority must be certified by the state in which they teach. Many charter schools do not require that their teachers be certified, provided they meet the standards to be highly qualified as set by No Child Left Behind. Additionally, the requirements for substitute/temporary teachers are generally not as rigorous as those for full-time professionals. The Bureau of Labor Statistics estimates that there are 1.4 million elementary school teachers, 674,000 middle school teachers, and 1 million secondary school teachers employed in the U.S.\nOutput: ", "bloom_target": "Who may not require that its teachers be certified?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In the United States, each state determines the requirements for getting a license to teach in public schools. Teaching certification generally lasts three years, but teachers can receive certificates that last as long as ten years. Public school teachers are required to have a bachelor's degree and the majority must be certified by the state in which they teach. Many charter schools do not require that their teachers be certified, provided they meet the standards to be highly qualified as set by No Child Left Behind. Additionally, the requirements for substitute/temporary teachers are generally not as rigorous as those for full-time professionals. The Bureau of Labor Statistics estimates that there are 1.4 million elementary school teachers, 674,000 middle school teachers, and 1 million secondary school teachers employed in the U.S.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-a414ea9b9a0f46229ca22d08fe9692f9", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-a414ea9b9a0f46229ca22d08fe9692f9", "input": "In Hinduism the spiritual teacher is known as a guru, and, in many traditions of Hinduism - especially those common in the West - the emphasis on spiritual mentorship is extremely high, with gurus often exercising a great deal of control over the lives of their disciples.", "output": ["What is the name of the spiritual teacher in Hinduism?", "Is the focus on spiritual mentorship in Hinduism high or low?", "Who do gurus control?", "In what area is it common for spiritual mentorship to be extremely high?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In Hinduism the spiritual teacher is known as a guru, and, in many traditions of Hinduism - especially those common in the West - the emphasis on spiritual mentorship is extremely high, with gurus often exercising a great deal of control over the lives of their disciples.\nOutput: ", "bloom_target": "Is the focus on spiritual mentorship in Hinduism high or low?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In Hinduism the spiritual teacher is known as a guru, and, in many traditions of Hinduism - especially those common in the West - the emphasis on spiritual mentorship is extremely high, with gurus often exercising a great deal of control over the lives of their disciples.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-3cbb0fec90954f09829094ff33713c9f", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-3cbb0fec90954f09829094ff33713c9f", "input": "A Pharmacy Technician in the UK is considered a health care professional and often does not work under the direct supervision of a pharmacist (if employed in a hospital pharmacy) but instead is supervised and managed by other senior pharmacy technicians. In the UK the role of a PhT has grown and responsibility has been passed on to them to manage the pharmacy department and specialised areas in pharmacy practice allowing pharmacists the time to specialise in their expert field as medication consultants spending more time working with patients and in research. A pharmacy technician once qualified has to register as a professional on the General Pharmaceutical Council (GPhC) register. The GPhC is the governing body for pharmacy health care professionals and this is who regulates the practice of pharmacists and pharmacy technicians.", "output": ["Who oversees a Pharmacy Technician in the UK?", "With what body must a pharmacy technician register?", "What is a main duty of the GPhC?", "What type of professional is a Pharmacy Technician considered to be?", "What types of responsibilities might a pharmacy technician have?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: A Pharmacy Technician in the UK is considered a health care professional and often does not work under the direct supervision of a pharmacist (if employed in a hospital pharmacy) but instead is supervised and managed by other senior pharmacy technicians. In the UK the role of a PhT has grown and responsibility has been passed on to them to manage the pharmacy department and specialised areas in pharmacy practice allowing pharmacists the time to specialise in their expert field as medication consultants spending more time working with patients and in research. A pharmacy technician once qualified has to register as a professional on the General Pharmaceutical Council (GPhC) register. The GPhC is the governing body for pharmacy health care professionals and this is who regulates the practice of pharmacists and pharmacy technicians.\nOutput: ", "bloom_target": "What type of professional is a Pharmacy Technician considered to be?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: A Pharmacy Technician in the UK is considered a health care professional and often does not work under the direct supervision of a pharmacist (if employed in a hospital pharmacy) but instead is supervised and managed by other senior pharmacy technicians. In the UK the role of a PhT has grown and responsibility has been passed on to them to manage the pharmacy department and specialised areas in pharmacy practice allowing pharmacists the time to specialise in their expert field as medication consultants spending more time working with patients and in research. A pharmacy technician once qualified has to register as a professional on the General Pharmaceutical Council (GPhC) register. The GPhC is the governing body for pharmacy health care professionals and this is who regulates the practice of pharmacists and pharmacy technicians.\nOutput:  The", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-0a862445cee749d6b4e2121df372258b", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-0a862445cee749d6b4e2121df372258b", "input": "Grissom, White, and Chaffee decided to name their flight Apollo 1 as a motivational focus on the first manned flight. They trained and conducted tests of their spacecraft at North American, and in the altitude chamber at the Kennedy Space Center. A 'plugs-out' test was planned for January, which would simulate a launch countdown on LC-34 with the spacecraft transferring from pad-supplied to internal power. If successful, this would be followed by a more rigorous countdown simulation test closer to the February 21 launch, with both spacecraft and launch vehicle fueled.", "output": ["Where did Apollo 1's crew conduct tests at Kennedy Space Center?", "Who was the main crew of Apollo 1, of which they named themselves?", "What was a plugs-out test done to simulate on the LC-34?", "What other location did Apollo 1 test at besides Kennedy Space Center?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Grissom, White, and Chaffee decided to name their flight Apollo 1 as a motivational focus on the first manned flight. They trained and conducted tests of their spacecraft at North American, and in the altitude chamber at the Kennedy Space Center. A 'plugs-out' test was planned for January, which would simulate a launch countdown on LC-34 with the spacecraft transferring from pad-supplied to internal power. If successful, this would be followed by a more rigorous countdown simulation test closer to the February 21 launch, with both spacecraft and launch vehicle fueled.\nOutput: ", "bloom_target": "Who was the main crew of Apollo 1, of which they named themselves?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Grissom, White, and Chaffee decided to name their flight Apollo 1 as a motivational focus on the first manned flight. They trained and conducted tests of their spacecraft at North American, and in the altitude chamber at the Kennedy Space Center. A 'plugs-out' test was planned for January, which would simulate a launch countdown on LC-34 with the spacecraft transferring from pad-supplied to internal power. If successful, this would be followed by a more rigorous countdown simulation test closer to the February 21 launch, with both spacecraft and launch vehicle fueled.\nOutput:  The", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-bc1bcc3d89b74de0a514104c011f12e3", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-bc1bcc3d89b74de0a514104c011f12e3", "input": "Islamist movements such as the Muslim Brotherhood, 'are well known for providing shelters, educational assistance, free or low cost medical clinics, housing assistance to students from out of town, student advisory groups, facilitation of inexpensive mass marriage ceremonies to avoid prohibitively costly dowry demands, legal assistance, sports facilities, and women's groups.' All this compares very favourably against incompetent, inefficient, or neglectful governments whose commitment to social justice is limited to rhetoric.", "output": ["What type of movement is the Muslim Brotherhood?", "The Muslim Brotherhood's competence compares well against what type of local governments?", "What type of assistance to out of town students is the Muslim Brotherhood known for?", "What are incompetent government's commitment to social justice limited to?", "Why has the Muslim Brotherhood facilitated inexpensive mass marriage ceremonies?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Islamist movements such as the Muslim Brotherhood, 'are well known for providing shelters, educational assistance, free or low cost medical clinics, housing assistance to students from out of town, student advisory groups, facilitation of inexpensive mass marriage ceremonies to avoid prohibitively costly dowry demands, legal assistance, sports facilities, and women's groups.' All this compares very favourably against incompetent, inefficient, or neglectful governments whose commitment to social justice is limited to rhetoric.\nOutput: ", "bloom_target": "What type of movement is the Muslim Brotherhood?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Islamist movements such as the Muslim Brotherhood, 'are well known for providing shelters, educational assistance, free or low cost medical clinics, housing assistance to students from out of town, student advisory groups, facilitation of inexpensive mass marriage ceremonies to avoid prohibitively costly dowry demands, legal assistance, sports facilities, and women's groups.' All this compares very favourably against incompetent, inefficient, or neglectful governments whose commitment to social justice is limited to rhetoric.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-f838814b59db4f4b969a2c8b74813879", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-f838814b59db4f4b969a2c8b74813879", "input": "Endosymbiotic gene transfer is how we know about the lost chloroplasts in many chromalveolate lineages. Even if a chloroplast is eventually lost, the genes it donated to the former host's nucleus persist, providing evidence for the lost chloroplast's existence. For example, while diatoms (a heterokontophyte) now have a red algal derived chloroplast, the presence of many green algal genes in the diatom nucleus provide evidence that the diatom ancestor (probably the ancestor of all chromalveolates too) had a green algal derived chloroplast at some point, which was subsequently replaced by the red chloroplast.", "output": ["What shows us lost chloroplasts?", "What do donated genes give evidence of?", "What kind of chloroplasts do diatoms have?", "What kind of chloroplasts did diatoms have but lost?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Endosymbiotic gene transfer is how we know about the lost chloroplasts in many chromalveolate lineages. Even if a chloroplast is eventually lost, the genes it donated to the former host's nucleus persist, providing evidence for the lost chloroplast's existence. For example, while diatoms (a heterokontophyte) now have a red algal derived chloroplast, the presence of many green algal genes in the diatom nucleus provide evidence that the diatom ancestor (probably the ancestor of all chromalveolates too) had a green algal derived chloroplast at some point, which was subsequently replaced by the red chloroplast.\nOutput: ", "bloom_target": "What do donated genes give evidence of?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Endosymbiotic gene transfer is how we know about the lost chloroplasts in many chromalveolate lineages. Even if a chloroplast is eventually lost, the genes it donated to the former host's nucleus persist, providing evidence for the lost chloroplast's existence. For example, while diatoms (a heterokontophyte) now have a red algal derived chloroplast, the presence of many green algal genes in the diatom nucleus provide evidence that the diatom ancestor (probably the ancestor of all chromalveolates too) had a green algal derived chloroplast at some point, which was subsequently replaced by the red chloroplast.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-cdf013b5870e41b985c36924a6e402b3", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-cdf013b5870e41b985c36924a6e402b3", "input": "For many years, Sudan had an Islamist regime under the leadership of Hassan al-Turabi. His National Islamic Front first gained influence when strongman General Gaafar al-Nimeiry invited members to serve in his government in 1979. Turabi built a powerful economic base with money from foreign Islamist banking systems, especially those linked with Saudi Arabia. He also recruited and built a cadre of influential loyalists by placing sympathetic students in the university and military academy while serving as minister of education.", "output": ["What type of regime ruled over Sudan for many years?", "Who was the leader of the Islamist regime in Sudan?", "What organization did General Gaafar al-Nimeiry invite members of to serve in his government?", "How did Turabi build a strong economic base?", "Where did Turabi place students sympathetic to his views?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: For many years, Sudan had an Islamist regime under the leadership of Hassan al-Turabi. His National Islamic Front first gained influence when strongman General Gaafar al-Nimeiry invited members to serve in his government in 1979. Turabi built a powerful economic base with money from foreign Islamist banking systems, especially those linked with Saudi Arabia. He also recruited and built a cadre of influential loyalists by placing sympathetic students in the university and military academy while serving as minister of education.\nOutput: ", "bloom_target": "What type of regime ruled over Sudan for many years?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: For many years, Sudan had an Islamist regime under the leadership of Hassan al-Turabi. His National Islamic Front first gained influence when strongman General Gaafar al-Nimeiry invited members to serve in his government in 1979. Turabi built a powerful economic base with money from foreign Islamist banking systems, especially those linked with Saudi Arabia. He also recruited and built a cadre of influential loyalists by placing sympathetic students in the university and military academy while serving as minister of education.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-026958e31c764d0089e05804933d6f90", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-026958e31c764d0089e05804933d6f90", "input": "Computational complexity theory is a branch of the theory of computation in theoretical computer science that focuses on classifying computational problems according to their inherent difficulty, and relating those classes to each other. A computational problem is understood to be a task that is in principle amenable to being solved by a computer, which is equivalent to stating that the problem may be solved by mechanical application of mathematical steps, such as an algorithm.", "output": ["What branch of theoretical computer science deals with broadly classifying computational problems by difficulty and class of relationship?", "By what main attribute are computational problems classified utilizing computational complexity theory?", "What is the term for a task that generally lends itself to being solved by a computer?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Computational complexity theory is a branch of the theory of computation in theoretical computer science that focuses on classifying computational problems according to their inherent difficulty, and relating those classes to each other. A computational problem is understood to be a task that is in principle amenable to being solved by a computer, which is equivalent to stating that the problem may be solved by mechanical application of mathematical steps, such as an algorithm.\nOutput: ", "bloom_target": "By what main attribute are computational problems classified utilizing computational complexity theory?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Computational complexity theory is a branch of the theory of computation in theoretical computer science that focuses on classifying computational problems according to their inherent difficulty, and relating those classes to each other. A computational problem is understood to be a task that is in principle amenable to being solved by a computer, which is equivalent to stating that the problem may be solved by mechanical application of mathematical steps, such as an algorithm.\nOutput:  The", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-36d5b1d0df7c46258150791a6c7857ee", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-36d5b1d0df7c46258150791a6c7857ee", "input": "In connectionless mode each packet includes complete addressing information. The packets are routed individually, sometimes resulting in different paths and out-of-order delivery. Each packet is labeled with a destination address, source address, and port numbers. It may also be labeled with the sequence number of the packet. This precludes the need for a dedicated path to help the packet find its way to its destination, but means that much more information is needed in the packet header, which is therefore larger, and this information needs to be looked up in power-hungry content-addressable memory. Each packet is dispatched and may go via different routes; potentially, the system has to do as much work for every packet as the connection-oriented system has to do in connection set-up, but with less information as to the application's requirements. At the destination, the original message/data is reassembled in the correct order, based on the packet sequence number. Thus a virtual connection, also known as a virtual circuit or byte stream is provided to the end-user by a transport layer protocol, although intermediate network nodes only provides a connectionless network layer service.", "output": ["What does each packet includ in connectionless mode", "How are the packets routed", "What is included with each packet label", "What happens to the packet at the destination"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In connectionless mode each packet includes complete addressing information. The packets are routed individually, sometimes resulting in different paths and out-of-order delivery. Each packet is labeled with a destination address, source address, and port numbers. It may also be labeled with the sequence number of the packet. This precludes the need for a dedicated path to help the packet find its way to its destination, but means that much more information is needed in the packet header, which is therefore larger, and this information needs to be looked up in power-hungry content-addressable memory. Each packet is dispatched and may go via different routes; potentially, the system has to do as much work for every packet as the connection-oriented system has to do in connection set-up, but with less information as to the application's requirements. At the destination, the original message/data is reassembled in the correct order, based on the packet sequence number. Thus a virtual connection, also known as a virtual circuit or byte stream is provided to the end-user by a transport layer protocol, although intermediate network nodes only provides a connectionless network layer service.\nOutput: ", "bloom_target": "What does each packet includ in connectionless mode", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In connectionless mode each packet includes complete addressing information. The packets are routed individually, sometimes resulting in different paths and out-of-order delivery. Each packet is labeled with a destination address, source address, and port numbers. It may also be labeled with the sequence number of the packet. This precludes the need for a dedicated path to help the packet find its way to its destination, but means that much more information is needed in the packet header, which is therefore larger, and this information needs to be looked up in power-hungry content-addressable memory. Each packet is dispatched and may go via different routes; potentially, the system has to do as much work for every packet as the connection-oriented system has to do in connection set-up, but with less information as to the application's requirements. At the destination, the original message/data is reassembled in the correct order, based on the packet sequence number. Thus a virtual connection, also known as a virtual circuit or byte stream is provided to the end-user by a transport layer protocol, although intermediate network nodes only provides a connectionless network layer service.\nOutput:  The", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-c355a4efe4df4f73953d6b4cdb74ac30", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-c355a4efe4df4f73953d6b4cdb74ac30", "input": "Even before Washington returned, Dinwiddie had sent a company of 40 men under William Trent to that point, where in the early months of 1754 they began construction of a small stockaded fort. Governor Duquesne sent additional French forces under Claude-Pierre Pecaudy de Contrec\u0153ur to relieve Saint-Pierre during the same period, and Contrec\u0153ur led 500 men south from Fort Venango on April 5, 1754. When these forces arrived at the fort on April 16, Contrec\u0153ur generously allowed Trent's small company to withdraw. He purchased their construction tools to continue building what became Fort Duquesne.", "output": ["How many men did Duquesne send to relieve  Saint-Pierre ?", "When did British begin to build fort under William Trent?", "What was the fort that was being built to be named?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Even before Washington returned, Dinwiddie had sent a company of 40 men under William Trent to that point, where in the early months of 1754 they began construction of a small stockaded fort. Governor Duquesne sent additional French forces under Claude-Pierre Pecaudy de Contrec\u0153ur to relieve Saint-Pierre during the same period, and Contrec\u0153ur led 500 men south from Fort Venango on April 5, 1754. When these forces arrived at the fort on April 16, Contrec\u0153ur generously allowed Trent's small company to withdraw. He purchased their construction tools to continue building what became Fort Duquesne.\nOutput: ", "bloom_target": "How many men did Duquesne send to relieve  Saint-Pierre ?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Even before Washington returned, Dinwiddie had sent a company of 40 men under William Trent to that point, where in the early months of 1754 they began construction of a small stockaded fort. Governor Duquesne sent additional French forces under Claude-Pierre Pecaudy de Contrec\u0153ur to relieve Saint-Pierre during the same period, and Contrec\u0153ur led 500 men south from Fort Venango on April 5, 1754. When these forces arrived at the fort on April 16, Contrec\u0153ur generously allowed Trent's small company to withdraw. He purchased their construction tools to continue building what became Fort Duquesne.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-c3bafe7ef67e44028fdbd640bb796d9d", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-c3bafe7ef67e44028fdbd640bb796d9d", "input": "In many parts of the United States, after the 1954 decision in the landmark court case Brown v. Board of Education of Topeka that demanded United States schools desegregate 'with all deliberate speed', local families organized a wave of private 'Christian academies'. In much of the U.S. South, many white students migrated to the academies, while public schools became in turn more heavily concentrated with African-American students (see List of private schools in Mississippi). The academic content of the academies was usually College Preparatory. Since the 1970s, many of these 'segregation academies' have shut down, although some continue to operate.[citation needed]", "output": ["What court case desegregated schools in the United States?", "What is a derogatory term for the Christian academies that arose in the wake of school desegregation?", "In what part of the United States did many students migrate to Christian academies during the desegregation period?", "What was the primary race of students attending Christian academies after the Brown decision?", "School desegregation in the United States led to an increased number of students of what ethnicity in public schools?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In many parts of the United States, after the 1954 decision in the landmark court case Brown v. Board of Education of Topeka that demanded United States schools desegregate 'with all deliberate speed', local families organized a wave of private 'Christian academies'. In much of the U.S. South, many white students migrated to the academies, while public schools became in turn more heavily concentrated with African-American students (see List of private schools in Mississippi). The academic content of the academies was usually College Preparatory. Since the 1970s, many of these 'segregation academies' have shut down, although some continue to operate.[citation needed]\nOutput: ", "bloom_target": "What is a derogatory term for the Christian academies that arose in the wake of school desegregation?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In many parts of the United States, after the 1954 decision in the landmark court case Brown v. Board of Education of Topeka that demanded United States schools desegregate 'with all deliberate speed', local families organized a wave of private 'Christian academies'. In much of the U.S. South, many white students migrated to the academies, while public schools became in turn more heavily concentrated with African-American students (see List of private schools in Mississippi). The academic content of the academies was usually College Preparatory. Since the 1970s, many of these 'segregation academies' have shut down, although some continue to operate.[citation needed]\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-89834cb17dfc42b38e45e317b57e4427", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-89834cb17dfc42b38e45e317b57e4427", "input": "A third type of conjectures concerns aspects of the distribution of primes. It is conjectured that there are infinitely many twin primes, pairs of primes with difference 2 (twin prime conjecture). Polignac's conjecture is a strengthening of that conjecture, it states that for every positive integer n, there are infinitely many pairs of consecutive primes that differ by 2n. It is conjectured there are infinitely many primes of the form n2 + 1. These conjectures are special cases of the broad Schinzel's hypothesis H. Brocard's conjecture says that there are always at least four primes between the squares of consecutive primes greater than 2. Legendre's conjecture states that there is a prime number between n2 and (n + 1)2 for every positive integer n. It is implied by the stronger Cram\u00e9r's conjecture.", "output": ["What conjecture holds that there is an infinite amount of twin primes?", "What is a twin prime?", "Which conjecture holds that for any positive integer n, there is an infinite amount of pairs of consecutive primes differing by 2n?", "Of what form is the infinite amount of primes that comprise the special cases of Schinzel's hypothesis?", "What conjecture holds that there are always a minimum of 4 primes  between the squares of consecutive primes greater than 2?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: A third type of conjectures concerns aspects of the distribution of primes. It is conjectured that there are infinitely many twin primes, pairs of primes with difference 2 (twin prime conjecture). Polignac's conjecture is a strengthening of that conjecture, it states that for every positive integer n, there are infinitely many pairs of consecutive primes that differ by 2n. It is conjectured there are infinitely many primes of the form n2 + 1. These conjectures are special cases of the broad Schinzel's hypothesis H. Brocard's conjecture says that there are always at least four primes between the squares of consecutive primes greater than 2. Legendre's conjecture states that there is a prime number between n2 and (n + 1)2 for every positive integer n. It is implied by the stronger Cram\u00e9r's conjecture.\nOutput: ", "bloom_target": "What is a twin prime?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: A third type of conjectures concerns aspects of the distribution of primes. It is conjectured that there are infinitely many twin primes, pairs of primes with difference 2 (twin prime conjecture). Polignac's conjecture is a strengthening of that conjecture, it states that for every positive integer n, there are infinitely many pairs of consecutive primes that differ by 2n. It is conjectured there are infinitely many primes of the form n2 + 1. These conjectures are special cases of the broad Schinzel's hypothesis H. Brocard's conjecture says that there are always at least four primes between the squares of consecutive primes greater than 2. Legendre's conjecture states that there is a prime number between n2 and (n + 1)2 for every positive integer n. It is implied by the stronger Cram\u00e9r's conjecture.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-864d57e1b2df464e8a7f9ea34233fd34", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-864d57e1b2df464e8a7f9ea34233fd34", "input": "In business, notable alumni include Microsoft CEO Satya Nadella, Oracle Corporation founder and the third richest man in America Larry Ellison, Goldman Sachs and MF Global CEO as well as former Governor of New Jersey Jon Corzine, McKinsey & Company founder and author of the first management accounting textbook James O. McKinsey, Arley D. Cathey, Bloomberg L.P. CEO Daniel Doctoroff, Credit Suisse CEO Brady Dougan, Morningstar, Inc. founder and CEO Joe Mansueto, Chicago Cubs owner and chairman Thomas S. Ricketts, and NBA commissioner Adam Silver.", "output": ["What Microsoft CEO is also an alumni of the University of Chicago?", "Who was the founder of the Oracle Corporation?", "Who is the third riches man in America?", "What Goldman Sachs CEO is also an alumni of the University of Chicago?", "Who founded McKinsey & Company?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In business, notable alumni include Microsoft CEO Satya Nadella, Oracle Corporation founder and the third richest man in America Larry Ellison, Goldman Sachs and MF Global CEO as well as former Governor of New Jersey Jon Corzine, McKinsey & Company founder and author of the first management accounting textbook James O. McKinsey, Arley D. Cathey, Bloomberg L.P. CEO Daniel Doctoroff, Credit Suisse CEO Brady Dougan, Morningstar, Inc. founder and CEO Joe Mansueto, Chicago Cubs owner and chairman Thomas S. Ricketts, and NBA commissioner Adam Silver.\nOutput: ", "bloom_target": "What Goldman Sachs CEO is also an alumni of the University of Chicago?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In business, notable alumni include Microsoft CEO Satya Nadella, Oracle Corporation founder and the third richest man in America Larry Ellison, Goldman Sachs and MF Global CEO as well as former Governor of New Jersey Jon Corzine, McKinsey & Company founder and author of the first management accounting textbook James O. McKinsey, Arley D. Cathey, Bloomberg L.P. CEO Daniel Doctoroff, Credit Suisse CEO Brady Dougan, Morningstar, Inc. founder and CEO Joe Mansueto, Chicago Cubs owner and chairman Thomas S. Ricketts, and NBA commissioner Adam Silver.\nOutput:  Who", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-faf6707e756040ab81a109f189ae607d", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-faf6707e756040ab81a109f189ae607d", "input": "Historically, Victoria has been the base for the manufacturing plants of the major car brands Ford, Toyota and Holden; however, closure announcements by all three companies in the 21st century will mean that Australia will no longer be a base for the global car industry, with Toyota's statement in February 2014 outlining a closure year of 2017. Holden's announcement occurred in May 2013, followed by Ford's decision in December of the same year (Ford's Victorian plants\u2014in Broadmeadows and Geelong\u2014will close in October 2016).", "output": ["What type of manufacturing plant is Victoria soon losing?", "When has Toyota said it will close its Victoria plant?", "When did Holden announce that it will close its Victoria plant?", "When will Ford's manufacturing plants close?", "What brand of car is manufactured in Broadmeadows?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Historically, Victoria has been the base for the manufacturing plants of the major car brands Ford, Toyota and Holden; however, closure announcements by all three companies in the 21st century will mean that Australia will no longer be a base for the global car industry, with Toyota's statement in February 2014 outlining a closure year of 2017. Holden's announcement occurred in May 2013, followed by Ford's decision in December of the same year (Ford's Victorian plants\u2014in Broadmeadows and Geelong\u2014will close in October 2016).\nOutput: ", "bloom_target": "When will Ford's manufacturing plants close?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Historically, Victoria has been the base for the manufacturing plants of the major car brands Ford, Toyota and Holden; however, closure announcements by all three companies in the 21st century will mean that Australia will no longer be a base for the global car industry, with Toyota's statement in February 2014 outlining a closure year of 2017. Holden's announcement occurred in May 2013, followed by Ford's decision in December of the same year (Ford's Victorian plants\u2014in Broadmeadows and Geelong\u2014will close in October 2016).\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-7d2c9e1c5ee54e9687a60fac83f2cb61", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-7d2c9e1c5ee54e9687a60fac83f2cb61", "input": "France took control of Algeria in 1830 but began in earnest to rebuild its worldwide empire after 1850, concentrating chiefly in North and West Africa, as well as South-East Asia, with other conquests in Central and East Africa, as well as the South Pacific. Republicans, at first hostile to empire, only became supportive when Germany started to build her own colonial empire. As it developed, the new empire took on roles of trade with France, supplying raw materials and purchasing manufactured items, as well as lending prestige to the motherland and spreading French civilization and language as well as Catholicism. It also provided crucial manpower in both World Wars.", "output": ["When did France take control of Algeria?", "When did France begin in earnest to rebuild its global empire?", "What religion did the French spread along with their imperialism?", "Where did France focus its efforts to rebuild its empire?", "When did French Republicans back building the French empire?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: France took control of Algeria in 1830 but began in earnest to rebuild its worldwide empire after 1850, concentrating chiefly in North and West Africa, as well as South-East Asia, with other conquests in Central and East Africa, as well as the South Pacific. Republicans, at first hostile to empire, only became supportive when Germany started to build her own colonial empire. As it developed, the new empire took on roles of trade with France, supplying raw materials and purchasing manufactured items, as well as lending prestige to the motherland and spreading French civilization and language as well as Catholicism. It also provided crucial manpower in both World Wars.\nOutput: ", "bloom_target": "When did France begin in earnest to rebuild its global empire?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: France took control of Algeria in 1830 but began in earnest to rebuild its worldwide empire after 1850, concentrating chiefly in North and West Africa, as well as South-East Asia, with other conquests in Central and East Africa, as well as the South Pacific. Republicans, at first hostile to empire, only became supportive when Germany started to build her own colonial empire. As it developed, the new empire took on roles of trade with France, supplying raw materials and purchasing manufactured items, as well as lending prestige to the motherland and spreading French civilization and language as well as Catholicism. It also provided crucial manpower in both World Wars.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-3122c731e449441595d2fd7101d905b8", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-3122c731e449441595d2fd7101d905b8", "input": "At the end of World War I, the Rhineland was subject to the Treaty of Versailles. This decreed that it would be occupied by the allies, until 1935 and after that, it would be a demilitarised zone, with the German army forbidden to enter. The Treaty of Versailles and this particular provision, in general, caused much resentment in Germany and is often cited as helping Adolf Hitler's rise to power. The allies left the Rhineland, in 1930 and the German army re-occupied it in 1936, which was enormously popular in Germany. Although the allies could probably have prevented the re-occupation, Britain and France were not inclined to do so, a feature of their policy of appeasement to Hitler.", "output": ["When was Rhineland subject to the Treaty of Versailles?", "When would the occupation of allies leave Rhineland?", "After 1935, who would be forbidden to enter Rhineland?", "What do some believe the Treaty of Versailles assisted in?", "When did the German army reoccupy Rhineland?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: At the end of World War I, the Rhineland was subject to the Treaty of Versailles. This decreed that it would be occupied by the allies, until 1935 and after that, it would be a demilitarised zone, with the German army forbidden to enter. The Treaty of Versailles and this particular provision, in general, caused much resentment in Germany and is often cited as helping Adolf Hitler's rise to power. The allies left the Rhineland, in 1930 and the German army re-occupied it in 1936, which was enormously popular in Germany. Although the allies could probably have prevented the re-occupation, Britain and France were not inclined to do so, a feature of their policy of appeasement to Hitler.\nOutput: ", "bloom_target": "What do some believe the Treaty of Versailles assisted in?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: At the end of World War I, the Rhineland was subject to the Treaty of Versailles. This decreed that it would be occupied by the allies, until 1935 and after that, it would be a demilitarised zone, with the German army forbidden to enter. The Treaty of Versailles and this particular provision, in general, caused much resentment in Germany and is often cited as helping Adolf Hitler's rise to power. The allies left the Rhineland, in 1930 and the German army re-occupied it in 1936, which was enormously popular in Germany. Although the allies could probably have prevented the re-occupation, Britain and France were not inclined to do so, a feature of their policy of appeasement to Hitler.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-e8b27059e481408886f1acc4b142badd", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-e8b27059e481408886f1acc4b142badd", "input": "A piece of paper was later found on which Luther had written his last statement. The statement was in Latin, apart from 'We are beggars,' which was in German.", "output": ["What was later discovered written by Luther?", "In what language was most of the statement written?", "What portion of Luther's last statement was in German?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: A piece of paper was later found on which Luther had written his last statement. The statement was in Latin, apart from 'We are beggars,' which was in German.\nOutput: ", "bloom_target": "What was later discovered written by Luther?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: A piece of paper was later found on which Luther had written his last statement. The statement was in Latin, apart from 'We are beggars,' which was in German.\nOutput:  Luther", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-afc2f58a5b7f47c5ab02cb27bbb10115", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-afc2f58a5b7f47c5ab02cb27bbb10115", "input": "Internet2 is a not-for-profit United States computer networking consortium led by members from the research and education communities, industry, and government. The Internet2 community, in partnership with Qwest, built the first Internet2 Network, called Abilene, in 1998 and was a prime investor in the National LambdaRail (NLR) project. In 2006, Internet2 announced a partnership with Level 3 Communications to launch a brand new nationwide network, boosting its capacity from 10 Gbit/s to 100 Gbit/s. In October, 2007, Internet2 officially retired Abilene and now refers to its new, higher capacity network as the Internet2 Network.", "output": ["what is Internet2", "Who did internet2 partner with", "What was the first internet2 network named", "ableine was retired and the new platform is called"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Internet2 is a not-for-profit United States computer networking consortium led by members from the research and education communities, industry, and government. The Internet2 community, in partnership with Qwest, built the first Internet2 Network, called Abilene, in 1998 and was a prime investor in the National LambdaRail (NLR) project. In 2006, Internet2 announced a partnership with Level 3 Communications to launch a brand new nationwide network, boosting its capacity from 10 Gbit/s to 100 Gbit/s. In October, 2007, Internet2 officially retired Abilene and now refers to its new, higher capacity network as the Internet2 Network.\nOutput: ", "bloom_target": "what is Internet2", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Internet2 is a not-for-profit United States computer networking consortium led by members from the research and education communities, industry, and government. The Internet2 community, in partnership with Qwest, built the first Internet2 Network, called Abilene, in 1998 and was a prime investor in the National LambdaRail (NLR) project. In 2006, Internet2 announced a partnership with Level 3 Communications to launch a brand new nationwide network, boosting its capacity from 10 Gbit/s to 100 Gbit/s. In October, 2007, Internet2 officially retired Abilene and now refers to its new, higher capacity network as the Internet2 Network.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-2ed461380dc24964a29c13dc1c3011d8", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-2ed461380dc24964a29c13dc1c3011d8", "input": "In 1939 Chinese Nationalist soldiers took the mausoleum from its position at the 'Lord's Enclosure' (Mongolian: Edsen Khoroo) in Mongolia to protect it from Japanese troops. It was taken through Communist-held territory in Yan'an some 900 km on carts to safety at a Buddhist monastery, the Dongshan Dafo Dian, where it remained for ten years. In 1949, as Communist troops advanced, the Nationalist soldiers moved it another 200 km farther west to the famous Tibetan monastery of Kumbum Monastery or Ta'er Shi near Xining, which soon fell under Communist control. In early 1954, Genghis Khan's bier and relics were returned to the Lord's Enclosure in Mongolia. By 1956 a new temple was erected there to house them. In 1968 during the Cultural Revolution, Red Guards destroyed almost everything of value. The 'relics' were remade in the 1970s and a great marble statue of Genghis was completed in 1989.", "output": ["What is the Mongolian name for the original place of the Genghis Khan mausoleum?", "Which Buddhist monastery hosted the Genghis Khan mausoleum during the Japanese occupation?", "Where did the Chinese Nationalists move the mausoleum away from advancing Chinese Communist forces?", "When was the mausoleum returned to the Lord's Enclosure in Mongolia?", "Who destroyed the most valuable relics in the mausoleum during the Cultural Revolution?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In 1939 Chinese Nationalist soldiers took the mausoleum from its position at the 'Lord's Enclosure' (Mongolian: Edsen Khoroo) in Mongolia to protect it from Japanese troops. It was taken through Communist-held territory in Yan'an some 900 km on carts to safety at a Buddhist monastery, the Dongshan Dafo Dian, where it remained for ten years. In 1949, as Communist troops advanced, the Nationalist soldiers moved it another 200 km farther west to the famous Tibetan monastery of Kumbum Monastery or Ta'er Shi near Xining, which soon fell under Communist control. In early 1954, Genghis Khan's bier and relics were returned to the Lord's Enclosure in Mongolia. By 1956 a new temple was erected there to house them. In 1968 during the Cultural Revolution, Red Guards destroyed almost everything of value. The 'relics' were remade in the 1970s and a great marble statue of Genghis was completed in 1989.\nOutput: ", "bloom_target": "Which Buddhist monastery hosted the Genghis Khan mausoleum during the Japanese occupation?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In 1939 Chinese Nationalist soldiers took the mausoleum from its position at the 'Lord's Enclosure' (Mongolian: Edsen Khoroo) in Mongolia to protect it from Japanese troops. It was taken through Communist-held territory in Yan'an some 900 km on carts to safety at a Buddhist monastery, the Dongshan Dafo Dian, where it remained for ten years. In 1949, as Communist troops advanced, the Nationalist soldiers moved it another 200 km farther west to the famous Tibetan monastery of Kumbum Monastery or Ta'er Shi near Xining, which soon fell under Communist control. In early 1954, Genghis Khan's bier and relics were returned to the Lord's Enclosure in Mongolia. By 1956 a new temple was erected there to house them. In 1968 during the Cultural Revolution, Red Guards destroyed almost everything of value. The 'relics' were remade in the 1970s and a great marble statue of Genghis was completed in 1989.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-00ad08e4a6944c7a9a082b2019371ffd", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-00ad08e4a6944c7a9a082b2019371ffd", "input": "A steam turbine consists of one or more rotors (rotating discs) mounted on a drive shaft, alternating with a series of stators (static discs) fixed to the turbine casing. The rotors have a propeller-like arrangement of blades at the outer edge. Steam acts upon these blades, producing rotary motion. The stator consists of a similar, but fixed, series of blades that serve to redirect the steam flow onto the next rotor stage. A steam turbine often exhausts into a surface condenser that provides a vacuum. The stages of a steam turbine are typically arranged to extract the maximum potential work from a specific velocity and pressure of steam, giving rise to a series of variably sized high- and low-pressure stages. Turbines are only efficient if they rotate at relatively high speed, therefore they are usually connected to reduction gearing to drive lower speed applications, such as a ship's propeller. In the vast majority of large electric generating stations, turbines are directly connected to generators with no reduction gearing. Typical speeds are 3600 revolutions per minute (RPM) in the USA with 60 Hertz power, 3000 RPM in Europe and other countries with 50 Hertz electric power systems. In nuclear power applications the turbines typically run at half these speeds, 1800 RPM and 1500 RPM. A turbine rotor is also only capable of providing power when rotating in one direction. Therefore, a reversing stage or gearbox is usually required where power is required in the opposite direction.[citation needed]", "output": ["What is another term for rotors?", "In a steam turbine, what are rotors mounted on?", "What is another way of referring to stators?", "What are stators attached to?", "In the United States, what is a usual turbine speed with 60 Hertz of power?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: A steam turbine consists of one or more rotors (rotating discs) mounted on a drive shaft, alternating with a series of stators (static discs) fixed to the turbine casing. The rotors have a propeller-like arrangement of blades at the outer edge. Steam acts upon these blades, producing rotary motion. The stator consists of a similar, but fixed, series of blades that serve to redirect the steam flow onto the next rotor stage. A steam turbine often exhausts into a surface condenser that provides a vacuum. The stages of a steam turbine are typically arranged to extract the maximum potential work from a specific velocity and pressure of steam, giving rise to a series of variably sized high- and low-pressure stages. Turbines are only efficient if they rotate at relatively high speed, therefore they are usually connected to reduction gearing to drive lower speed applications, such as a ship's propeller. In the vast majority of large electric generating stations, turbines are directly connected to generators with no reduction gearing. Typical speeds are 3600 revolutions per minute (RPM) in the USA with 60 Hertz power, 3000 RPM in Europe and other countries with 50 Hertz electric power systems. In nuclear power applications the turbines typically run at half these speeds, 1800 RPM and 1500 RPM. A turbine rotor is also only capable of providing power when rotating in one direction. Therefore, a reversing stage or gearbox is usually required where power is required in the opposite direction.[citation needed]\nOutput: ", "bloom_target": "What is another term for rotors?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: A steam turbine consists of one or more rotors (rotating discs) mounted on a drive shaft, alternating with a series of stators (static discs) fixed to the turbine casing. The rotors have a propeller-like arrangement of blades at the outer edge. Steam acts upon these blades, producing rotary motion. The stator consists of a similar, but fixed, series of blades that serve to redirect the steam flow onto the next rotor stage. A steam turbine often exhausts into a surface condenser that provides a vacuum. The stages of a steam turbine are typically arranged to extract the maximum potential work from a specific velocity and pressure of steam, giving rise to a series of variably sized high- and low-pressure stages. Turbines are only efficient if they rotate at relatively high speed, therefore they are usually connected to reduction gearing to drive lower speed applications, such as a ship's propeller. In the vast majority of large electric generating stations, turbines are directly connected to generators with no reduction gearing. Typical speeds are 3600 revolutions per minute (RPM) in the USA with 60 Hertz power, 3000 RPM in Europe and other countries with 50 Hertz electric power systems. In nuclear power applications the turbines typically run at half these speeds, 1800 RPM and 1500 RPM. A turbine rotor is also only capable of providing power when rotating in one direction. Therefore, a reversing stage or gearbox is usually required where power is required in the opposite direction.[citation needed]\nOutput:  The", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-5df5c742fc5e4453a5dfaf4d33520036", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-5df5c742fc5e4453a5dfaf4d33520036", "input": "In 1900, Tesla was granted patents for a 'system of transmitting electrical energy' and 'an electrical transmitter.' When Guglielmo Marconi made his famous first-ever transatlantic radio transmission in 1901, Tesla quipped that it was done with 17 Tesla patents, though there is little to support this claim. This was the beginning of years of patent battles over radio with Tesla's patents being upheld in 1903, followed by a reverse decision in favor of Marconi in 1904. In 1943, a Supreme Court of the United States decision restored the prior patents of Tesla, Oliver Lodge, and John Stone. The court declared that their decision had no bearing on Marconi's claim as the first to achieve radio transmission, just that since Marconi's claim to certain patents were questionable, he could not claim infringement on those same patents (there are claims the high court was trying to nullify a World War I claim against the U.S. government by the Marconi Company via simply restoring Tesla's prior patent).", "output": ["When did Tesla attain his electrical transmitter patent?", "Who first sent radio waves across the Atlantic?", "When was Marconi's radio demonstration?", "When were Tesla's patents restored?", "A decision made by what entity restored Tesla's patents?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In 1900, Tesla was granted patents for a 'system of transmitting electrical energy' and 'an electrical transmitter.' When Guglielmo Marconi made his famous first-ever transatlantic radio transmission in 1901, Tesla quipped that it was done with 17 Tesla patents, though there is little to support this claim. This was the beginning of years of patent battles over radio with Tesla's patents being upheld in 1903, followed by a reverse decision in favor of Marconi in 1904. In 1943, a Supreme Court of the United States decision restored the prior patents of Tesla, Oliver Lodge, and John Stone. The court declared that their decision had no bearing on Marconi's claim as the first to achieve radio transmission, just that since Marconi's claim to certain patents were questionable, he could not claim infringement on those same patents (there are claims the high court was trying to nullify a World War I claim against the U.S. government by the Marconi Company via simply restoring Tesla's prior patent).\nOutput: ", "bloom_target": "When were Tesla's patents restored?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In 1900, Tesla was granted patents for a 'system of transmitting electrical energy' and 'an electrical transmitter.' When Guglielmo Marconi made his famous first-ever transatlantic radio transmission in 1901, Tesla quipped that it was done with 17 Tesla patents, though there is little to support this claim. This was the beginning of years of patent battles over radio with Tesla's patents being upheld in 1903, followed by a reverse decision in favor of Marconi in 1904. In 1943, a Supreme Court of the United States decision restored the prior patents of Tesla, Oliver Lodge, and John Stone. The court declared that their decision had no bearing on Marconi's claim as the first to achieve radio transmission, just that since Marconi's claim to certain patents were questionable, he could not claim infringement on those same patents (there are claims the high court was trying to nullify a World War I claim against the U.S. government by the Marconi Company via simply restoring Tesla's prior patent).\nOutput:  The", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-720260054da94c56b87ae5180af237f4", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-720260054da94c56b87ae5180af237f4", "input": "In most jurisdictions (such as the United States), pharmacists are regulated separately from physicians. These jurisdictions also usually specify that only pharmacists may supply scheduled pharmaceuticals to the public, and that pharmacists cannot form business partnerships with physicians or give them 'kickback' payments. However, the American Medical Association (AMA) Code of Ethics provides that physicians may dispense drugs within their office practices as long as there is no patient exploitation and patients have the right to a written prescription that can be filled elsewhere. 7 to 10 percent of American physicians practices reportedly dispense drugs on their own.", "output": ["How are pharmacists regulated in most jurisdictions?", "Who do most jurisdictions say can give scheduled drugs to the public?", "What body has stated that physicians can also dispense drugs under specific conditions?", "What is the estimate of how many physicians give out drugs on their own?", "What are pharmacists forbidden to do?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In most jurisdictions (such as the United States), pharmacists are regulated separately from physicians. These jurisdictions also usually specify that only pharmacists may supply scheduled pharmaceuticals to the public, and that pharmacists cannot form business partnerships with physicians or give them 'kickback' payments. However, the American Medical Association (AMA) Code of Ethics provides that physicians may dispense drugs within their office practices as long as there is no patient exploitation and patients have the right to a written prescription that can be filled elsewhere. 7 to 10 percent of American physicians practices reportedly dispense drugs on their own.\nOutput: ", "bloom_target": "How are pharmacists regulated in most jurisdictions?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In most jurisdictions (such as the United States), pharmacists are regulated separately from physicians. These jurisdictions also usually specify that only pharmacists may supply scheduled pharmaceuticals to the public, and that pharmacists cannot form business partnerships with physicians or give them 'kickback' payments. However, the American Medical Association (AMA) Code of Ethics provides that physicians may dispense drugs within their office practices as long as there is no patient exploitation and patients have the right to a written prescription that can be filled elsewhere. 7 to 10 percent of American physicians practices reportedly dispense drugs on their own.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-0643b49809a94ae3bf8cc5f764f62945", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-0643b49809a94ae3bf8cc5f764f62945", "input": "After the 1940s, the Gothic style on campus began to give way to modern styles. In 1955, Eero Saarinen was contracted to develop a second master plan, which led to the construction of buildings both north and south of the Midway, including the Laird Bell Law Quadrangle (a complex designed by Saarinen); a series of arts buildings; a building designed by Ludwig Mies van der Rohe for the university's School of Social Service Administration;, a building which is to become the home of the Harris School of Public Policy Studies by Edward Durrell Stone, and the Regenstein Library, the largest building on campus, a brutalist structure designed by Walter Netsch of the Chicago firm Skidmore, Owings & Merrill. Another master plan, designed in 1999 and updated in 2004, produced the Gerald Ratner Athletics Center (2003), the Max Palevsky Residential Commons (2001), South Campus Residence Hall and dining commons (2009), a new children's hospital, and other construction, expansions, and restorations. In 2011, the university completed the glass dome-shaped Joe and Rika Mansueto Library, which provides a grand reading room for the university library and prevents the need for an off-campus book depository.", "output": ["During what decade did the campus start to look more modern?", "Who was assigned to design a second master plan?", "What administration did Ludwig Mies van der Rohe designa buiding?", "What public policy school found it's home in the building that Ludwig Mies van der Rohe designed?", "When was the Gerald Ratner Athletics Center constructed?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: After the 1940s, the Gothic style on campus began to give way to modern styles. In 1955, Eero Saarinen was contracted to develop a second master plan, which led to the construction of buildings both north and south of the Midway, including the Laird Bell Law Quadrangle (a complex designed by Saarinen); a series of arts buildings; a building designed by Ludwig Mies van der Rohe for the university's School of Social Service Administration;, a building which is to become the home of the Harris School of Public Policy Studies by Edward Durrell Stone, and the Regenstein Library, the largest building on campus, a brutalist structure designed by Walter Netsch of the Chicago firm Skidmore, Owings & Merrill. Another master plan, designed in 1999 and updated in 2004, produced the Gerald Ratner Athletics Center (2003), the Max Palevsky Residential Commons (2001), South Campus Residence Hall and dining commons (2009), a new children's hospital, and other construction, expansions, and restorations. In 2011, the university completed the glass dome-shaped Joe and Rika Mansueto Library, which provides a grand reading room for the university library and prevents the need for an off-campus book depository.\nOutput: ", "bloom_target": "Who was assigned to design a second master plan?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: After the 1940s, the Gothic style on campus began to give way to modern styles. In 1955, Eero Saarinen was contracted to develop a second master plan, which led to the construction of buildings both north and south of the Midway, including the Laird Bell Law Quadrangle (a complex designed by Saarinen); a series of arts buildings; a building designed by Ludwig Mies van der Rohe for the university's School of Social Service Administration;, a building which is to become the home of the Harris School of Public Policy Studies by Edward Durrell Stone, and the Regenstein Library, the largest building on campus, a brutalist structure designed by Walter Netsch of the Chicago firm Skidmore, Owings & Merrill. Another master plan, designed in 1999 and updated in 2004, produced the Gerald Ratner Athletics Center (2003), the Max Palevsky Residential Commons (2001), South Campus Residence Hall and dining commons (2009), a new children's hospital, and other construction, expansions, and restorations. In 2011, the university completed the glass dome-shaped Joe and Rika Mansueto Library, which provides a grand reading room for the university library and prevents the need for an off-campus book depository.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-4b2a625c74a44ca8ad9d76254f92247f", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-4b2a625c74a44ca8ad9d76254f92247f", "input": "In 1891 Scottish chemist James Dewar was able to produce enough liquid oxygen to study. The first commercially viable process for producing liquid oxygen was independently developed in 1895 by German engineer Carl von Linde and British engineer William Hampson. Both men lowered the temperature of air until it liquefied and then distilled the component gases by boiling them off one at a time and capturing them. Later, in 1901, oxyacetylene welding was demonstrated for the first time by burning a mixture of acetylene and compressed O\n2. This method of welding and cutting metal later became common.", "output": ["What chemist managed to make enough liquid oxygen to use for study?", "In what year did Dewar experiment on liquid oxygen?", "When was liquid oxygen developed for commercial use?", "What welding process was demonstrated in 1901?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In 1891 Scottish chemist James Dewar was able to produce enough liquid oxygen to study. The first commercially viable process for producing liquid oxygen was independently developed in 1895 by German engineer Carl von Linde and British engineer William Hampson. Both men lowered the temperature of air until it liquefied and then distilled the component gases by boiling them off one at a time and capturing them. Later, in 1901, oxyacetylene welding was demonstrated for the first time by burning a mixture of acetylene and compressed O\n2. This method of welding and cutting metal later became common.\nOutput: ", "bloom_target": "What chemist managed to make enough liquid oxygen to use for study?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In 1891 Scottish chemist James Dewar was able to produce enough liquid oxygen to study. The first commercially viable process for producing liquid oxygen was independently developed in 1895 by German engineer Carl von Linde and British engineer William Hampson. Both men lowered the temperature of air until it liquefied and then distilled the component gases by boiling them off one at a time and capturing them. Later, in 1901, oxyacetylene welding was demonstrated for the first time by burning a mixture of acetylene and compressed O\n2. This method of welding and cutting metal later became common.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-01835e65beeb4dbbbc4c8fd9c2609253", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-01835e65beeb4dbbbc4c8fd9c2609253", "input": "Construction is the process of constructing a building or infrastructure. Construction differs from manufacturing in that manufacturing typically involves mass production of similar items without a designated purchaser, while construction typically takes place on location for a known client. Construction as an industry comprises six to nine percent of the gross domestic product of developed countries. Construction starts with planning,[citation needed] design, and financing and continues until the project is built and ready for use.", "output": ["What is the process of constructing a building or infrastructure?", "What typically involves mass production of similar items without a designated purchaser?", "What percentile of gross domestic product is construction comprised of?", "What three things are needed for construction to take place?", "Construction takes place on location for who?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Construction is the process of constructing a building or infrastructure. Construction differs from manufacturing in that manufacturing typically involves mass production of similar items without a designated purchaser, while construction typically takes place on location for a known client. Construction as an industry comprises six to nine percent of the gross domestic product of developed countries. Construction starts with planning,[citation needed] design, and financing and continues until the project is built and ready for use.\nOutput: ", "bloom_target": "What percentile of gross domestic product is construction comprised of?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Construction is the process of constructing a building or infrastructure. Construction differs from manufacturing in that manufacturing typically involves mass production of similar items without a designated purchaser, while construction typically takes place on location for a known client. Construction as an industry comprises six to nine percent of the gross domestic product of developed countries. Construction starts with planning,[citation needed] design, and financing and continues until the project is built and ready for use.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-aea60f0bcd58459ea4f97c1c3101856f", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-aea60f0bcd58459ea4f97c1c3101856f", "input": "A variety of alternatives to the Y. pestis have been put forward. Twigg suggested that the cause was a form of anthrax, and Norman Cantor (2001) thought it may have been a combination of anthrax and other pandemics. Scott and Duncan have argued that the pandemic was a form of infectious disease that characterise as hemorrhagic plague similar to Ebola. Archaeologist Barney Sloane has argued that there is insufficient evidence of the extinction of a large number of rats in the archaeological record of the medieval waterfront in London and that the plague spread too quickly to support the thesis that the Y. pestis was spread from fleas on rats; he argues that transmission must have been person to person. However, no single alternative solution has achieved widespread acceptance. Many scholars arguing for the Y. pestis as the major agent of the pandemic suggest that its extent and symptoms can be explained by a combination of bubonic plague with other diseases, including typhus, smallpox and respiratory infections. In addition to the bubonic infection, others point to additional septicemic (a type of 'blood poisoning') and pneumonic (an airborne plague that attacks the lungs before the rest of the body) forms of the plague, which lengthen the duration of outbreaks throughout the seasons and help account for its high mortality rate and additional recorded symptoms. In 2014, scientists with Public Health England announced the results of an examination of 25 bodies exhumed from the Clerkenwell area of London, as well as of wills registered in London during the period, which supported the pneumonic hypothesis.", "output": ["What does Graham Twigg propose about the spread of disease?", "What was Norman Cantor's theory about the plague?", "Which diseases do many scientists believe contributed to plague pandemic?", "What is septicemia?", "How many bodies did Public Health England exhume?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: A variety of alternatives to the Y. pestis have been put forward. Twigg suggested that the cause was a form of anthrax, and Norman Cantor (2001) thought it may have been a combination of anthrax and other pandemics. Scott and Duncan have argued that the pandemic was a form of infectious disease that characterise as hemorrhagic plague similar to Ebola. Archaeologist Barney Sloane has argued that there is insufficient evidence of the extinction of a large number of rats in the archaeological record of the medieval waterfront in London and that the plague spread too quickly to support the thesis that the Y. pestis was spread from fleas on rats; he argues that transmission must have been person to person. However, no single alternative solution has achieved widespread acceptance. Many scholars arguing for the Y. pestis as the major agent of the pandemic suggest that its extent and symptoms can be explained by a combination of bubonic plague with other diseases, including typhus, smallpox and respiratory infections. In addition to the bubonic infection, others point to additional septicemic (a type of 'blood poisoning') and pneumonic (an airborne plague that attacks the lungs before the rest of the body) forms of the plague, which lengthen the duration of outbreaks throughout the seasons and help account for its high mortality rate and additional recorded symptoms. In 2014, scientists with Public Health England announced the results of an examination of 25 bodies exhumed from the Clerkenwell area of London, as well as of wills registered in London during the period, which supported the pneumonic hypothesis.\nOutput: ", "bloom_target": "What is septicemia?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: A variety of alternatives to the Y. pestis have been put forward. Twigg suggested that the cause was a form of anthrax, and Norman Cantor (2001) thought it may have been a combination of anthrax and other pandemics. Scott and Duncan have argued that the pandemic was a form of infectious disease that characterise as hemorrhagic plague similar to Ebola. Archaeologist Barney Sloane has argued that there is insufficient evidence of the extinction of a large number of rats in the archaeological record of the medieval waterfront in London and that the plague spread too quickly to support the thesis that the Y. pestis was spread from fleas on rats; he argues that transmission must have been person to person. However, no single alternative solution has achieved widespread acceptance. Many scholars arguing for the Y. pestis as the major agent of the pandemic suggest that its extent and symptoms can be explained by a combination of bubonic plague with other diseases, including typhus, smallpox and respiratory infections. In addition to the bubonic infection, others point to additional septicemic (a type of 'blood poisoning') and pneumonic (an airborne plague that attacks the lungs before the rest of the body) forms of the plague, which lengthen the duration of outbreaks throughout the seasons and help account for its high mortality rate and additional recorded symptoms. In 2014, scientists with Public Health England announced the results of an examination of 25 bodies exhumed from the Clerkenwell area of London, as well as of wills registered in London during the period, which supported the pneumonic hypothesis.\nOutput:  The", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-eb8ee99193cf4bc8b25ed8647e38676e", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-eb8ee99193cf4bc8b25ed8647e38676e", "input": "Fresno is the largest U.S. city not directly linked to an Interstate highway. When the Interstate Highway System was created in the 1950s, the decision was made to build what is now Interstate 5 on the west side of the Central Valley, and thus bypass many of the population centers in the region, instead of upgrading what is now State Route 99. Due to rapidly raising population and traffic in cities along SR 99, as well as the desirability of Federal funding, much discussion has been made to upgrade it to interstate standards and eventually incorporate it into the interstate system, most likely as Interstate 9. Major improvements to signage, lane width, median separation, vertical clearance, and other concerns are currently underway.", "output": ["Which is the largest city not connected to an interstate highway?", "In what year was the Interstate Highway System created?", "Which State Route has been in discussion to upgrade to interstate standards?", "What are the factors that are contributing to the desire to have SR 99 improved to be of interstate standards?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Fresno is the largest U.S. city not directly linked to an Interstate highway. When the Interstate Highway System was created in the 1950s, the decision was made to build what is now Interstate 5 on the west side of the Central Valley, and thus bypass many of the population centers in the region, instead of upgrading what is now State Route 99. Due to rapidly raising population and traffic in cities along SR 99, as well as the desirability of Federal funding, much discussion has been made to upgrade it to interstate standards and eventually incorporate it into the interstate system, most likely as Interstate 9. Major improvements to signage, lane width, median separation, vertical clearance, and other concerns are currently underway.\nOutput: ", "bloom_target": "In what year was the Interstate Highway System created?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Fresno is the largest U.S. city not directly linked to an Interstate highway. When the Interstate Highway System was created in the 1950s, the decision was made to build what is now Interstate 5 on the west side of the Central Valley, and thus bypass many of the population centers in the region, instead of upgrading what is now State Route 99. Due to rapidly raising population and traffic in cities along SR 99, as well as the desirability of Federal funding, much discussion has been made to upgrade it to interstate standards and eventually incorporate it into the interstate system, most likely as Interstate 9. Major improvements to signage, lane width, median separation, vertical clearance, and other concerns are currently underway.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-701581aaf4614917bbdfc08f989e4c0d", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-701581aaf4614917bbdfc08f989e4c0d", "input": "In Europe, the North American theater of the Seven Years' War usually is not given a separate name. The entire international conflict is known as the Seven Years' War. 'Seven Years' refers to events in Europe, from the official declaration of war in 1756 to the signing of the peace treaty in 1763. These dates do not correspond with the fighting on mainland North America, where the fighting between the two colonial powers was largely concluded in six years, from the Battle of Jumonville Glen in 1754 to the capture of Montreal in 1760.", "output": ["What time framd does the Seven Years War cover?", "How long did the fighting last in Seven Years War?", "When was Montreal captured?", "What was first battle in 1754?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In Europe, the North American theater of the Seven Years' War usually is not given a separate name. The entire international conflict is known as the Seven Years' War. 'Seven Years' refers to events in Europe, from the official declaration of war in 1756 to the signing of the peace treaty in 1763. These dates do not correspond with the fighting on mainland North America, where the fighting between the two colonial powers was largely concluded in six years, from the Battle of Jumonville Glen in 1754 to the capture of Montreal in 1760.\nOutput: ", "bloom_target": "How long did the fighting last in Seven Years War?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In Europe, the North American theater of the Seven Years' War usually is not given a separate name. The entire international conflict is known as the Seven Years' War. 'Seven Years' refers to events in Europe, from the official declaration of war in 1756 to the signing of the peace treaty in 1763. These dates do not correspond with the fighting on mainland North America, where the fighting between the two colonial powers was largely concluded in six years, from the Battle of Jumonville Glen in 1754 to the capture of Montreal in 1760.\nOutput:  The", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-cc158faba686473d82c4210018dee822", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-cc158faba686473d82c4210018dee822", "input": "As northwest Europe slowly began to warm up from 22,000 years ago onward, frozen subsoil and expanded alpine glaciers began to thaw and fall-winter snow covers melted in spring. Much of the discharge was routed to the Rhine and its downstream extension. Rapid warming and changes of vegetation, to open forest, began about 13,000 BP. By 9000 BP, Europe was fully forested. With globally shrinking ice-cover, ocean water levels rose and the English Channel and North Sea re-inundated. Meltwater, adding to the ocean and land subsidence, drowned the former coasts of Europe transgressionally.", "output": ["When did Europe slowly begin to warm up from the last Ice Age?", "What did frozen subsoil and expanded alpine glaciers begin to do?", "Where did the discharge from glaciers go in Europe in the last Ice Age?", "When did rapid warming begin and help vegetation?", "When was Europe fully forested and recovered from the last Ice Age?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: As northwest Europe slowly began to warm up from 22,000 years ago onward, frozen subsoil and expanded alpine glaciers began to thaw and fall-winter snow covers melted in spring. Much of the discharge was routed to the Rhine and its downstream extension. Rapid warming and changes of vegetation, to open forest, began about 13,000 BP. By 9000 BP, Europe was fully forested. With globally shrinking ice-cover, ocean water levels rose and the English Channel and North Sea re-inundated. Meltwater, adding to the ocean and land subsidence, drowned the former coasts of Europe transgressionally.\nOutput: ", "bloom_target": "What did frozen subsoil and expanded alpine glaciers begin to do?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: As northwest Europe slowly began to warm up from 22,000 years ago onward, frozen subsoil and expanded alpine glaciers began to thaw and fall-winter snow covers melted in spring. Much of the discharge was routed to the Rhine and its downstream extension. Rapid warming and changes of vegetation, to open forest, began about 13,000 BP. By 9000 BP, Europe was fully forested. With globally shrinking ice-cover, ocean water levels rose and the English Channel and North Sea re-inundated. Meltwater, adding to the ocean and land subsidence, drowned the former coasts of Europe transgressionally.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-3ea833ba65fe49809612106720989269", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-3ea833ba65fe49809612106720989269", "input": "Anti-inflammatory drugs are often used to control the effects of inflammation. Glucocorticoids are the most powerful of these drugs; however, these drugs can have many undesirable side effects, such as central obesity, hyperglycemia, osteoporosis, and their use must be tightly controlled. Lower doses of anti-inflammatory drugs are often used in conjunction with cytotoxic or immunosuppressive drugs such as methotrexate or azathioprine. Cytotoxic drugs inhibit the immune response by killing dividing cells such as activated T cells. However, the killing is indiscriminate and other constantly dividing cells and their organs are affected, which causes toxic side effects. Immunosuppressive drugs such as cyclosporin prevent T cells from responding to signals correctly by inhibiting signal transduction pathways.", "output": ["What are the most powerful class of anti-inflammatory drugs?", "Low doses of anti-inflammatories are sometimes used with what classes of drugs?", "What are two examples of cytotoxic or immunosuppressive drugs?", "What is an example of an immunosuppressive drug that prevents T cell activity by altering signal transduction pathways?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Anti-inflammatory drugs are often used to control the effects of inflammation. Glucocorticoids are the most powerful of these drugs; however, these drugs can have many undesirable side effects, such as central obesity, hyperglycemia, osteoporosis, and their use must be tightly controlled. Lower doses of anti-inflammatory drugs are often used in conjunction with cytotoxic or immunosuppressive drugs such as methotrexate or azathioprine. Cytotoxic drugs inhibit the immune response by killing dividing cells such as activated T cells. However, the killing is indiscriminate and other constantly dividing cells and their organs are affected, which causes toxic side effects. Immunosuppressive drugs such as cyclosporin prevent T cells from responding to signals correctly by inhibiting signal transduction pathways.\nOutput: ", "bloom_target": "What is an example of an immunosuppressive drug that prevents T cell activity by altering signal transduction pathways?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Anti-inflammatory drugs are often used to control the effects of inflammation. Glucocorticoids are the most powerful of these drugs; however, these drugs can have many undesirable side effects, such as central obesity, hyperglycemia, osteoporosis, and their use must be tightly controlled. Lower doses of anti-inflammatory drugs are often used in conjunction with cytotoxic or immunosuppressive drugs such as methotrexate or azathioprine. Cytotoxic drugs inhibit the immune response by killing dividing cells such as activated T cells. However, the killing is indiscriminate and other constantly dividing cells and their organs are affected, which causes toxic side effects. Immunosuppressive drugs such as cyclosporin prevent T cells from responding to signals correctly by inhibiting signal transduction pathways.\nOutput:  Which", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-8632f4911cab4ee8ade281bb3e355436", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-8632f4911cab4ee8ade281bb3e355436", "input": "In the years after these rumors, neither Tesla nor Edison won the prize (although Edison did receive one of 38 possible bids in 1915 and Tesla did receive one of 38 possible bids in 1937).", "output": ["How many possible bids for the prize were there in 1915?", "Who received a bid in 1915?", "In what year did Tesla receive a Nobel Prize bid?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In the years after these rumors, neither Tesla nor Edison won the prize (although Edison did receive one of 38 possible bids in 1915 and Tesla did receive one of 38 possible bids in 1937).\nOutput: ", "bloom_target": "Who received a bid in 1915?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In the years after these rumors, neither Tesla nor Edison won the prize (although Edison did receive one of 38 possible bids in 1915 and Tesla did receive one of 38 possible bids in 1937).\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-646591a33855490fa3aa89319d069f77", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-646591a33855490fa3aa89319d069f77", "input": "In economics, notable Nobel Memorial Prize in Economic Sciences winners Milton Friedman, a major advisor to Republican U.S. President Ronald Reagan and Conservative British Prime Minister Margaret Thatcher, George Stigler, Nobel laureate and proponent of regulatory capture theory, Gary Becker, an important contributor to the family economics branch of economics, Herbert A. Simon, responsible for the modern interpretation of the concept of organizational decision-making, Paul Samuelson, the first American to win the Nobel Memorial Prize in Economic Sciences, and Eugene Fama, known for his work on portfolio theory, asset pricing and stock market behaviour, are all graduates. American economist, social theorist, political philosopher, and author Thomas Sowell is also an alumnus.", "output": ["What Nobel Memorial Prize in Economic Sciences winner is also a university alumni member?", "What British Prime minister advisor is also a university alumni member?", "Who was the first American to win the Nobel Memorial Prize in Economic Sciences?", "What university alumni member was known for his work on portfolio theory?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In economics, notable Nobel Memorial Prize in Economic Sciences winners Milton Friedman, a major advisor to Republican U.S. President Ronald Reagan and Conservative British Prime Minister Margaret Thatcher, George Stigler, Nobel laureate and proponent of regulatory capture theory, Gary Becker, an important contributor to the family economics branch of economics, Herbert A. Simon, responsible for the modern interpretation of the concept of organizational decision-making, Paul Samuelson, the first American to win the Nobel Memorial Prize in Economic Sciences, and Eugene Fama, known for his work on portfolio theory, asset pricing and stock market behaviour, are all graduates. American economist, social theorist, political philosopher, and author Thomas Sowell is also an alumnus.\nOutput: ", "bloom_target": "What British Prime minister advisor is also a university alumni member?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In economics, notable Nobel Memorial Prize in Economic Sciences winners Milton Friedman, a major advisor to Republican U.S. President Ronald Reagan and Conservative British Prime Minister Margaret Thatcher, George Stigler, Nobel laureate and proponent of regulatory capture theory, Gary Becker, an important contributor to the family economics branch of economics, Herbert A. Simon, responsible for the modern interpretation of the concept of organizational decision-making, Paul Samuelson, the first American to win the Nobel Memorial Prize in Economic Sciences, and Eugene Fama, known for his work on portfolio theory, asset pricing and stock market behaviour, are all graduates. American economist, social theorist, political philosopher, and author Thomas Sowell is also an alumnus.\nOutput:  Which", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-881ca88141e64abfb6546fb208e0cec4", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-881ca88141e64abfb6546fb208e0cec4", "input": "Child labour is common in Kenya. Most working children are active in agriculture. In 2006, UNICEF estimated that up to 30% of girls in the coastal areas of Malindi, Mombasa, Kilifi, and Diani were subject to prostitution. Most of the prostitutes in Kenya are aged 9\u201318. The Ministry of Gender and Child Affairs employed 400 child protection officers in 2009. The causes of child labour include poverty, the lack of access to education and weak government institutions. Kenya has ratified Convention No. 81 on labour inspection in industries and Convention No. 129 on labour inspection in agriculture.", "output": ["Where are most working children working?", "What percent of girls are in prostution in Kenyas coastal areas?", "What is the age range of most prostitutes in Kenya?", "What are some of the causes of child labor?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Child labour is common in Kenya. Most working children are active in agriculture. In 2006, UNICEF estimated that up to 30% of girls in the coastal areas of Malindi, Mombasa, Kilifi, and Diani were subject to prostitution. Most of the prostitutes in Kenya are aged 9\u201318. The Ministry of Gender and Child Affairs employed 400 child protection officers in 2009. The causes of child labour include poverty, the lack of access to education and weak government institutions. Kenya has ratified Convention No. 81 on labour inspection in industries and Convention No. 129 on labour inspection in agriculture.\nOutput: ", "bloom_target": "Where are most working children working?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Child labour is common in Kenya. Most working children are active in agriculture. In 2006, UNICEF estimated that up to 30% of girls in the coastal areas of Malindi, Mombasa, Kilifi, and Diani were subject to prostitution. Most of the prostitutes in Kenya are aged 9\u201318. The Ministry of Gender and Child Affairs employed 400 child protection officers in 2009. The causes of child labour include poverty, the lack of access to education and weak government institutions. Kenya has ratified Convention No. 81 on labour inspection in industries and Convention No. 129 on labour inspection in agriculture.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-93aa8855b74f45329b444cecb8431f74", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-93aa8855b74f45329b444cecb8431f74", "input": "Contrary to popular belief, Genghis Khan did not conquer all the areas ultimately part of the Mongol Empire. At the time of his death, the Mongol Empire stretched from the Caspian Sea to the Sea of Japan. The empire's expansion continued for a generation or more after Genghis's death in 1227. Under Genghis's successor \u00d6gedei Khan the speed of expansion reached its peak. Mongol armies pushed into Persia, finished off the Western Xia and the remnants of the Khwarezmids, and came into conflict with the imperial Song dynasty of China, starting a war that lasted until 1279 and that concluded with the Mongols gaining control of all of China. They also pushed further into Russia and eastern Europe.", "output": ["What sea bordered Genghis Khan's empire to the east when he died?", "What body of water sat to the west of the Mongol Empire when Genghis Khan died?", "Who led the most rapid expansion of the Mongol Empire?", "What year was the conflict with the Song dynasty concluded?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Contrary to popular belief, Genghis Khan did not conquer all the areas ultimately part of the Mongol Empire. At the time of his death, the Mongol Empire stretched from the Caspian Sea to the Sea of Japan. The empire's expansion continued for a generation or more after Genghis's death in 1227. Under Genghis's successor \u00d6gedei Khan the speed of expansion reached its peak. Mongol armies pushed into Persia, finished off the Western Xia and the remnants of the Khwarezmids, and came into conflict with the imperial Song dynasty of China, starting a war that lasted until 1279 and that concluded with the Mongols gaining control of all of China. They also pushed further into Russia and eastern Europe.\nOutput: ", "bloom_target": "What sea bordered Genghis Khan's empire to the east when he died?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Contrary to popular belief, Genghis Khan did not conquer all the areas ultimately part of the Mongol Empire. At the time of his death, the Mongol Empire stretched from the Caspian Sea to the Sea of Japan. The empire's expansion continued for a generation or more after Genghis's death in 1227. Under Genghis's successor \u00d6gedei Khan the speed of expansion reached its peak. Mongol armies pushed into Persia, finished off the Western Xia and the remnants of the Khwarezmids, and came into conflict with the imperial Song dynasty of China, starting a war that lasted until 1279 and that concluded with the Mongols gaining control of all of China. They also pushed further into Russia and eastern Europe.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-5e6915b871a84542a3a95cd3ca980df5", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-5e6915b871a84542a3a95cd3ca980df5", "input": "Between 1991 and 2000, the total area of forest lost in the Amazon rose from 415,000 to 587,000 square kilometres (160,000 to 227,000 sq mi), with most of the lost forest becoming pasture for cattle. Seventy percent of formerly forested land in the Amazon, and 91% of land deforested since 1970, is used for livestock pasture. Currently, Brazil is the second-largest global producer of soybeans after the United States. New research however, conducted by Leydimere Oliveira et al., has shown that the more rainforest is logged in the Amazon, the less precipitation reaches the area and so the lower the yield per hectare becomes. So despite the popular perception, there has been no economical advantage for Brazil from logging rainforest zones and converting these to pastoral fields.", "output": ["How many square kilometres of the Amazon forest was lost by 1991?", "In the year 2000 how many square kilometres of the Amazon forest had been lost?", "What is most of the cleared land in the Amazon region used for?", "Where is Brazil ranked globally in soybean production?", "What percentage of the land cleared in the Amazon is used for growing livestock?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Between 1991 and 2000, the total area of forest lost in the Amazon rose from 415,000 to 587,000 square kilometres (160,000 to 227,000 sq mi), with most of the lost forest becoming pasture for cattle. Seventy percent of formerly forested land in the Amazon, and 91% of land deforested since 1970, is used for livestock pasture. Currently, Brazil is the second-largest global producer of soybeans after the United States. New research however, conducted by Leydimere Oliveira et al., has shown that the more rainforest is logged in the Amazon, the less precipitation reaches the area and so the lower the yield per hectare becomes. So despite the popular perception, there has been no economical advantage for Brazil from logging rainforest zones and converting these to pastoral fields.\nOutput: ", "bloom_target": "What is most of the cleared land in the Amazon region used for?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Between 1991 and 2000, the total area of forest lost in the Amazon rose from 415,000 to 587,000 square kilometres (160,000 to 227,000 sq mi), with most of the lost forest becoming pasture for cattle. Seventy percent of formerly forested land in the Amazon, and 91% of land deforested since 1970, is used for livestock pasture. Currently, Brazil is the second-largest global producer of soybeans after the United States. New research however, conducted by Leydimere Oliveira et al., has shown that the more rainforest is logged in the Amazon, the less precipitation reaches the area and so the lower the yield per hectare becomes. So despite the popular perception, there has been no economical advantage for Brazil from logging rainforest zones and converting these to pastoral fields.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-e976da3cd95f4261b89051a9e7db6787", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-e976da3cd95f4261b89051a9e7db6787", "input": "In the early 1970s, ABC completed its transition to color; the decade as a whole would mark a turning point for ABC, as it began to pass CBS and NBC in the ratings to become the first place network. It also began to use behavioral and demographic data to better determine what types of sponsors to sell advertising slots to and provide programming that would appeal towards certain audiences. ABC's gains in audience share were greatly helped by the fact that several smaller markets had grown large enough to allow full-time affiliations from all three networks.", "output": ["In what decade did ABC finish transitioning to color?", "The 1970s allowed which network to move in to first place in the ratings?", "What kind of data did ABC begin using in the 1970s to better target ads and programming for certain audiences?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In the early 1970s, ABC completed its transition to color; the decade as a whole would mark a turning point for ABC, as it began to pass CBS and NBC in the ratings to become the first place network. It also began to use behavioral and demographic data to better determine what types of sponsors to sell advertising slots to and provide programming that would appeal towards certain audiences. ABC's gains in audience share were greatly helped by the fact that several smaller markets had grown large enough to allow full-time affiliations from all three networks.\nOutput: ", "bloom_target": "The 1970s allowed which network to move in to first place in the ratings?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In the early 1970s, ABC completed its transition to color; the decade as a whole would mark a turning point for ABC, as it began to pass CBS and NBC in the ratings to become the first place network. It also began to use behavioral and demographic data to better determine what types of sponsors to sell advertising slots to and provide programming that would appeal towards certain audiences. ABC's gains in audience share were greatly helped by the fact that several smaller markets had grown large enough to allow full-time affiliations from all three networks.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-2ef67878b9bb416fb5cbf62272a50eb6", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-2ef67878b9bb416fb5cbf62272a50eb6", "input": "Frederick William, Elector of Brandenburg, invited Huguenots to settle in his realms, and a number of their descendants rose to positions of prominence in Prussia. Several prominent German military, cultural, and political figures were ethnic Huguenot, including poet Theodor Fontane, General Hermann von Fran\u00e7ois, the hero of the First World War Battle of Tannenberg, Luftwaffe General and fighter ace Adolf Galland, Luftwaffe flying ace Hans-Joachim Marseille, and famed U-boat captain Lothar von Arnauld de la Peri\u00e8re. The last Prime Minister of the (East) German Democratic Republic, Lothar de Maizi\u00e8re, is also a descendant of a Huguenot family, as is the German Federal Minister of the Interior, Thomas de Maizi\u00e8re.", "output": ["What German ruler invited Huguenot immigration?", "What German poet was descended from Huguenots?", "What German general and fighter pilot was of Huguenot ancestry?", "Who was the final Prime Minister of East Germany?", "Thomas de Maiziere serves what role in the German cabinet?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Frederick William, Elector of Brandenburg, invited Huguenots to settle in his realms, and a number of their descendants rose to positions of prominence in Prussia. Several prominent German military, cultural, and political figures were ethnic Huguenot, including poet Theodor Fontane, General Hermann von Fran\u00e7ois, the hero of the First World War Battle of Tannenberg, Luftwaffe General and fighter ace Adolf Galland, Luftwaffe flying ace Hans-Joachim Marseille, and famed U-boat captain Lothar von Arnauld de la Peri\u00e8re. The last Prime Minister of the (East) German Democratic Republic, Lothar de Maizi\u00e8re, is also a descendant of a Huguenot family, as is the German Federal Minister of the Interior, Thomas de Maizi\u00e8re.\nOutput: ", "bloom_target": "Who was the final Prime Minister of East Germany?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Frederick William, Elector of Brandenburg, invited Huguenots to settle in his realms, and a number of their descendants rose to positions of prominence in Prussia. Several prominent German military, cultural, and political figures were ethnic Huguenot, including poet Theodor Fontane, General Hermann von Fran\u00e7ois, the hero of the First World War Battle of Tannenberg, Luftwaffe General and fighter ace Adolf Galland, Luftwaffe flying ace Hans-Joachim Marseille, and famed U-boat captain Lothar von Arnauld de la Peri\u00e8re. The last Prime Minister of the (East) German Democratic Republic, Lothar de Maizi\u00e8re, is also a descendant of a Huguenot family, as is the German Federal Minister of the Interior, Thomas de Maizi\u00e8re.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-a16405438cbd4fc28fdb9d99b96f9eef", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-a16405438cbd4fc28fdb9d99b96f9eef", "input": "Evolution of the adaptive immune system occurred in an ancestor of the jawed vertebrates. Many of the classical molecules of the adaptive immune system (e.g., immunoglobulins and T cell receptors) exist only in jawed vertebrates. However, a distinct lymphocyte-derived molecule has been discovered in primitive jawless vertebrates, such as the lamprey and hagfish. These animals possess a large array of molecules called Variable lymphocyte receptors (VLRs) that, like the antigen receptors of jawed vertebrates, are produced from only a small number (one or two) of genes. These molecules are believed to bind pathogenic antigens in a similar way to antibodies, and with the same degree of specificity.", "output": ["What molecules of the adaptive immune system only exist in jawed vertebrates?", "What are two examples of primitive jawless vertebrates?", "Primitive jawless vertebrates possess an array of receptors referred to as what?", "Evolution of what part of the immune system occurred in the evolutionary ancestor of jawed vertebrates?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Evolution of the adaptive immune system occurred in an ancestor of the jawed vertebrates. Many of the classical molecules of the adaptive immune system (e.g., immunoglobulins and T cell receptors) exist only in jawed vertebrates. However, a distinct lymphocyte-derived molecule has been discovered in primitive jawless vertebrates, such as the lamprey and hagfish. These animals possess a large array of molecules called Variable lymphocyte receptors (VLRs) that, like the antigen receptors of jawed vertebrates, are produced from only a small number (one or two) of genes. These molecules are believed to bind pathogenic antigens in a similar way to antibodies, and with the same degree of specificity.\nOutput: ", "bloom_target": "Primitive jawless vertebrates possess an array of receptors referred to as what?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Evolution of the adaptive immune system occurred in an ancestor of the jawed vertebrates. Many of the classical molecules of the adaptive immune system (e.g., immunoglobulins and T cell receptors) exist only in jawed vertebrates. However, a distinct lymphocyte-derived molecule has been discovered in primitive jawless vertebrates, such as the lamprey and hagfish. These animals possess a large array of molecules called Variable lymphocyte receptors (VLRs) that, like the antigen receptors of jawed vertebrates, are produced from only a small number (one or two) of genes. These molecules are believed to bind pathogenic antigens in a similar way to antibodies, and with the same degree of specificity.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-8c2a7294c6b34b3583d2f59f9cffd4b2", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-8c2a7294c6b34b3583d2f59f9cffd4b2", "input": "During the divestment from South Africa movement in the late 1980s, student activists erected a symbolic 'shantytown' on Harvard Yard and blockaded a speech given by South African Vice Consul Duke Kent-Brown. The Harvard Management Company repeatedly refused to divest, stating that 'operating expenses must not be subject to financially unrealistic strictures or carping by the unsophisticated or by special interest groups.' However, the university did eventually reduce its South African holdings by $230 million (out of $400 million) in response to the pressure.", "output": ["When was the divestment from South Africa movement?", "What South African Vice Consul did Harvard students blockade the speech of?", "By how much did Harvard management reduce its South Africa holdings in response to pressure?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: During the divestment from South Africa movement in the late 1980s, student activists erected a symbolic 'shantytown' on Harvard Yard and blockaded a speech given by South African Vice Consul Duke Kent-Brown. The Harvard Management Company repeatedly refused to divest, stating that 'operating expenses must not be subject to financially unrealistic strictures or carping by the unsophisticated or by special interest groups.' However, the university did eventually reduce its South African holdings by $230 million (out of $400 million) in response to the pressure.\nOutput: ", "bloom_target": "When was the divestment from South Africa movement?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: During the divestment from South Africa movement in the late 1980s, student activists erected a symbolic 'shantytown' on Harvard Yard and blockaded a speech given by South African Vice Consul Duke Kent-Brown. The Harvard Management Company repeatedly refused to divest, stating that 'operating expenses must not be subject to financially unrealistic strictures or carping by the unsophisticated or by special interest groups.' However, the university did eventually reduce its South African holdings by $230 million (out of $400 million) in response to the pressure.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-63ac864aa59a454f86b4da6b26df468e", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-63ac864aa59a454f86b4da6b26df468e", "input": "In particular, this norm gets smaller when a number is multiplied by p, in sharp contrast to the usual absolute value (also referred to as the infinite prime). While completing Q (roughly, filling the gaps) with respect to the absolute value yields the field of real numbers, completing with respect to the p-adic norm |\u2212|p yields the field of p-adic numbers. These are essentially all possible ways to complete Q, by Ostrowski's theorem. Certain arithmetic questions related to Q or more general global fields may be transferred back and forth to the completed (or local) fields. This local-global principle again underlines the importance of primes to number theory.", "output": ["What happens to the norm when a number is multiplied by p?", "To what may general global fields be transferred to or from?", "Completing Q with respect to what will produce the field of real numbers?", "What principle highlights the significance of primes in number theory"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In particular, this norm gets smaller when a number is multiplied by p, in sharp contrast to the usual absolute value (also referred to as the infinite prime). While completing Q (roughly, filling the gaps) with respect to the absolute value yields the field of real numbers, completing with respect to the p-adic norm |\u2212|p yields the field of p-adic numbers. These are essentially all possible ways to complete Q, by Ostrowski's theorem. Certain arithmetic questions related to Q or more general global fields may be transferred back and forth to the completed (or local) fields. This local-global principle again underlines the importance of primes to number theory.\nOutput: ", "bloom_target": "What happens to the norm when a number is multiplied by p?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In particular, this norm gets smaller when a number is multiplied by p, in sharp contrast to the usual absolute value (also referred to as the infinite prime). While completing Q (roughly, filling the gaps) with respect to the absolute value yields the field of real numbers, completing with respect to the p-adic norm |\u2212|p yields the field of p-adic numbers. These are essentially all possible ways to complete Q, by Ostrowski's theorem. Certain arithmetic questions related to Q or more general global fields may be transferred back and forth to the completed (or local) fields. This local-global principle again underlines the importance of primes to number theory.\nOutput:  The", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-c5a5b7b440134793a3e330252c613d9e", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-c5a5b7b440134793a3e330252c613d9e", "input": "Islamism is a controversial concept not just because it posits a political role for Islam but also because its supporters believe their views merely reflect Islam, while the contrary idea that Islam is, or can be, apolitical is an error. Scholars and observers who do not believe that Islam is merely a political ideology include Fred Halliday, John Esposito and Muslim intellectuals like Javed Ahmad Ghamidi. Hayri Abaza argues the failure to distinguish between Islam and Islamism leads many in the West to support illiberal Islamic regimes, to the detriment of progressive moderates who seek to separate religion from politics.", "output": ["What type of role that Islamism seeks makes it a somewhat controversial concept?", "What do supporters of Islamism believe their views reflect?", "The idea that Islam can be apolitical isn't able to be embraced by whom?", "What does the inability to separate Islam from Islamism lead many in the West to support?", "What do progressive moderates of Islam seek to separate?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Islamism is a controversial concept not just because it posits a political role for Islam but also because its supporters believe their views merely reflect Islam, while the contrary idea that Islam is, or can be, apolitical is an error. Scholars and observers who do not believe that Islam is merely a political ideology include Fred Halliday, John Esposito and Muslim intellectuals like Javed Ahmad Ghamidi. Hayri Abaza argues the failure to distinguish between Islam and Islamism leads many in the West to support illiberal Islamic regimes, to the detriment of progressive moderates who seek to separate religion from politics.\nOutput: ", "bloom_target": "What type of role that Islamism seeks makes it a somewhat controversial concept?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Islamism is a controversial concept not just because it posits a political role for Islam but also because its supporters believe their views merely reflect Islam, while the contrary idea that Islam is, or can be, apolitical is an error. Scholars and observers who do not believe that Islam is merely a political ideology include Fred Halliday, John Esposito and Muslim intellectuals like Javed Ahmad Ghamidi. Hayri Abaza argues the failure to distinguish between Islam and Islamism leads many in the West to support illiberal Islamic regimes, to the detriment of progressive moderates who seek to separate religion from politics.\nOutput:  Islam", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-520bd692bde6447baa01eb2fcd843ffe", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-520bd692bde6447baa01eb2fcd843ffe", "input": "In 2010 the Amazon rainforest experienced another severe drought, in some ways more extreme than the 2005 drought. The affected region was approximate 1,160,000 square miles (3,000,000 km2) of rainforest, compared to 734,000 square miles (1,900,000 km2) in 2005. The 2010 drought had three epicenters where vegetation died off, whereas in 2005 the drought was focused on the southwestern part. The findings were published in the journal Science. In a typical year the Amazon absorbs 1.5 gigatons of carbon dioxide; during 2005 instead 5 gigatons were released and in 2010 8 gigatons were released.", "output": ["In what year did the Amazon experience a drought that may have been more extreme than in 2005?", "How many square miles large was the region impacted by the 2010 drought?", "How many areas were impacted by the the death of vegetation in the 2010 drought?", "The southern part of the Amazon forest was mainly impacted by drought in what year?", "How many tons of carbon are absorbed the Amazon in a typical year?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In 2010 the Amazon rainforest experienced another severe drought, in some ways more extreme than the 2005 drought. The affected region was approximate 1,160,000 square miles (3,000,000 km2) of rainforest, compared to 734,000 square miles (1,900,000 km2) in 2005. The 2010 drought had three epicenters where vegetation died off, whereas in 2005 the drought was focused on the southwestern part. The findings were published in the journal Science. In a typical year the Amazon absorbs 1.5 gigatons of carbon dioxide; during 2005 instead 5 gigatons were released and in 2010 8 gigatons were released.\nOutput: ", "bloom_target": "How many tons of carbon are absorbed the Amazon in a typical year?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In 2010 the Amazon rainforest experienced another severe drought, in some ways more extreme than the 2005 drought. The affected region was approximate 1,160,000 square miles (3,000,000 km2) of rainforest, compared to 734,000 square miles (1,900,000 km2) in 2005. The 2010 drought had three epicenters where vegetation died off, whereas in 2005 the drought was focused on the southwestern part. The findings were published in the journal Science. In a typical year the Amazon absorbs 1.5 gigatons of carbon dioxide; during 2005 instead 5 gigatons were released and in 2010 8 gigatons were released.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-e6830f1a489c406bafeda34b0732d988", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-e6830f1a489c406bafeda34b0732d988", "input": "In contrast, during wake periods differentiated effector cells, such as cytotoxic natural killer cells and CTLs (cytotoxic T lymphocytes), peak in order to elicit an effective response against any intruding pathogens. As well during awake active times, anti-inflammatory molecules, such as cortisol and catecholamines, peak. There are two theories as to why the pro-inflammatory state is reserved for sleep time. First, inflammation would cause serious cognitive and physical impairments if it were to occur during wake times. Second, inflammation may occur during sleep times due to the presence of melatonin. Inflammation causes a great deal of oxidative stress and the presence of melatonin during sleep times could actively counteract free radical production during this time.", "output": ["What are examples of differentiated effector cells that peak during wake periods?", "What are two anti-inflammatory molecules that peak during awake hours?", "Inflammation occurs during sleep times because of the presence of what molecule?", "Melatonin during sleep can actively counteract the production of what?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In contrast, during wake periods differentiated effector cells, such as cytotoxic natural killer cells and CTLs (cytotoxic T lymphocytes), peak in order to elicit an effective response against any intruding pathogens. As well during awake active times, anti-inflammatory molecules, such as cortisol and catecholamines, peak. There are two theories as to why the pro-inflammatory state is reserved for sleep time. First, inflammation would cause serious cognitive and physical impairments if it were to occur during wake times. Second, inflammation may occur during sleep times due to the presence of melatonin. Inflammation causes a great deal of oxidative stress and the presence of melatonin during sleep times could actively counteract free radical production during this time.\nOutput: ", "bloom_target": "What are examples of differentiated effector cells that peak during wake periods?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In contrast, during wake periods differentiated effector cells, such as cytotoxic natural killer cells and CTLs (cytotoxic T lymphocytes), peak in order to elicit an effective response against any intruding pathogens. As well during awake active times, anti-inflammatory molecules, such as cortisol and catecholamines, peak. There are two theories as to why the pro-inflammatory state is reserved for sleep time. First, inflammation would cause serious cognitive and physical impairments if it were to occur during wake times. Second, inflammation may occur during sleep times due to the presence of melatonin. Inflammation causes a great deal of oxidative stress and the presence of melatonin during sleep times could actively counteract free radical production during this time.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-44892bf3efe245d2a59be24bf397ac54", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-44892bf3efe245d2a59be24bf397ac54", "input": "'The Islamic State', formerly known as the 'Islamic State of Iraq and the Levant' and before that as the 'Islamic State of Iraq', (and called the acronym Daesh by its many detractors), is a Wahhabi/Salafi jihadist extremist militant group which is led by and mainly composed of Sunni Arabs from Iraq and Syria. In 2014, the group proclaimed itself a caliphate, with religious, political and military authority over all Muslims worldwide. As of March 2015[update], it had control over territory occupied by ten million people in Iraq and Syria, and has nominal control over small areas of Libya, Nigeria and Afghanistan. (While a self-described state, it lacks international recognition.) The group also operates or has affiliates in other parts of the world, including North Africa and South Asia.", "output": ["What type of group is The Islamic State?", "Who leads The Islamic State?", "How many people did the Islamic State control the territory of as of March 2015?", "What does the Islamic State lack from the international community?", "What did the Islamic State proclaim itself in 2014?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: 'The Islamic State', formerly known as the 'Islamic State of Iraq and the Levant' and before that as the 'Islamic State of Iraq', (and called the acronym Daesh by its many detractors), is a Wahhabi/Salafi jihadist extremist militant group which is led by and mainly composed of Sunni Arabs from Iraq and Syria. In 2014, the group proclaimed itself a caliphate, with religious, political and military authority over all Muslims worldwide. As of March 2015[update], it had control over territory occupied by ten million people in Iraq and Syria, and has nominal control over small areas of Libya, Nigeria and Afghanistan. (While a self-described state, it lacks international recognition.) The group also operates or has affiliates in other parts of the world, including North Africa and South Asia.\nOutput: ", "bloom_target": "What type of group is The Islamic State?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: 'The Islamic State', formerly known as the 'Islamic State of Iraq and the Levant' and before that as the 'Islamic State of Iraq', (and called the acronym Daesh by its many detractors), is a Wahhabi/Salafi jihadist extremist militant group which is led by and mainly composed of Sunni Arabs from Iraq and Syria. In 2014, the group proclaimed itself a caliphate, with religious, political and military authority over all Muslims worldwide. As of March 2015[update], it had control over territory occupied by ten million people in Iraq and Syria, and has nominal control over small areas of Libya, Nigeria and Afghanistan. (While a self-described state, it lacks international recognition.) The group also operates or has affiliates in other parts of the world, including North Africa and South Asia.\nOutput:  The", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-7297b7a6d92642b9b3bee111d3e52e75", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-7297b7a6d92642b9b3bee111d3e52e75", "input": "In 2000, ABC launched a web-based promotional campaign focused around its circle logo, also called 'the dot', in which comic book character Little Dot prompted visitors to 'download the dot', a program which would cause the ABC logo to fly around the screen and settle in the bottom-right corner. The network hired the Troika Design Group to design and produce its 2001\u201302 identity, which continued using the black-and-yellow coloring of the logo and featured dots and stripes in various promotional and identification spots.", "output": ["In 2000, ABC started an internet based campaign focused on what?", "Who was hired to produce ABC's 2001-02 identity?", "What colors was the 2001 ABC logo?", "What is the nickname for ABC's logo from the 2000 campaign?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In 2000, ABC launched a web-based promotional campaign focused around its circle logo, also called 'the dot', in which comic book character Little Dot prompted visitors to 'download the dot', a program which would cause the ABC logo to fly around the screen and settle in the bottom-right corner. The network hired the Troika Design Group to design and produce its 2001\u201302 identity, which continued using the black-and-yellow coloring of the logo and featured dots and stripes in various promotional and identification spots.\nOutput: ", "bloom_target": "Who was hired to produce ABC's 2001-02 identity?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: In 2000, ABC launched a web-based promotional campaign focused around its circle logo, also called 'the dot', in which comic book character Little Dot prompted visitors to 'download the dot', a program which would cause the ABC logo to fly around the screen and settle in the bottom-right corner. The network hired the Troika Design Group to design and produce its 2001\u201302 identity, which continued using the black-and-yellow coloring of the logo and featured dots and stripes in various promotional and identification spots.\nOutput:  The", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-720fd1309b674c738dcf520d3f2a4039", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-720fd1309b674c738dcf520d3f2a4039", "input": "For a precise definition of what it means to solve a problem using a given amount of time and space, a computational model such as the deterministic Turing machine is used. The time required by a deterministic Turing machine M on input x is the total number of state transitions, or steps, the machine makes before it halts and outputs the answer ('yes' or 'no'). A Turing machine M is said to operate within time f(n), if the time required by M on each input of length n is at most f(n). A decision problem A can be solved in time f(n) if there exists a Turing machine operating in time f(n) that solves the problem. Since complexity theory is interested in classifying problems based on their difficulty, one defines sets of problems based on some criteria. For instance, the set of problems solvable within time f(n) on a deterministic Turing machine is then denoted by DTIME(f(n)).", "output": ["The time required to output an answer on a deterministic Turing machine is expressed as what?", "Complexity theory classifies problems based on what primary attribute?", "What is the expression used to identify any given series of problems capable of being solved within time on a deterministic Turing machine?", "What is the most critical resource measured to in assessing the determination of a Turing machine's ability to solve any given set of problems?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: For a precise definition of what it means to solve a problem using a given amount of time and space, a computational model such as the deterministic Turing machine is used. The time required by a deterministic Turing machine M on input x is the total number of state transitions, or steps, the machine makes before it halts and outputs the answer ('yes' or 'no'). A Turing machine M is said to operate within time f(n), if the time required by M on each input of length n is at most f(n). A decision problem A can be solved in time f(n) if there exists a Turing machine operating in time f(n) that solves the problem. Since complexity theory is interested in classifying problems based on their difficulty, one defines sets of problems based on some criteria. For instance, the set of problems solvable within time f(n) on a deterministic Turing machine is then denoted by DTIME(f(n)).\nOutput: ", "bloom_target": "What is the expression used to identify any given series of problems capable of being solved within time on a deterministic Turing machine?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: For a precise definition of what it means to solve a problem using a given amount of time and space, a computational model such as the deterministic Turing machine is used. The time required by a deterministic Turing machine M on input x is the total number of state transitions, or steps, the machine makes before it halts and outputs the answer ('yes' or 'no'). A Turing machine M is said to operate within time f(n), if the time required by M on each input of length n is at most f(n). A decision problem A can be solved in time f(n) if there exists a Turing machine operating in time f(n) that solves the problem. Since complexity theory is interested in classifying problems based on their difficulty, one defines sets of problems based on some criteria. For instance, the set of problems solvable within time f(n) on a deterministic Turing machine is then denoted by DTIME(f(n)).\nOutput:  The", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-366d246c3c684bab943af0a520b3f3cc", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-366d246c3c684bab943af0a520b3f3cc", "input": "Aristotle provided a philosophical discussion of the concept of a force as an integral part of Aristotelian cosmology. In Aristotle's view, the terrestrial sphere contained four elements that come to rest at different 'natural places' therein. Aristotle believed that motionless objects on Earth, those composed mostly of the elements earth and water, to be in their natural place on the ground and that they will stay that way if left alone. He distinguished between the innate tendency of objects to find their 'natural place' (e.g., for heavy bodies to fall), which led to 'natural motion', and unnatural or forced motion, which required continued application of a force. This theory, based on the everyday experience of how objects move, such as the constant application of a force needed to keep a cart moving, had conceptual trouble accounting for the behavior of projectiles, such as the flight of arrows. The place where the archer moves the projectile was at the start of the flight, and while the projectile sailed through the air, no discernible efficient cause acts on it. Aristotle was aware of this problem and proposed that the air displaced through the projectile's path carries the projectile to its target. This explanation demands a continuum like air for change of place in general.", "output": ["Who provided a philosophical discussion of force?", "What was the concept of force an integral part of?", "How many elements did Aristotle believe the terrestrial sphere to be made up of?", "Where did Aristotle believe the natural place for earth and water elements?", "What did Aristotle refer to forced motion as?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Aristotle provided a philosophical discussion of the concept of a force as an integral part of Aristotelian cosmology. In Aristotle's view, the terrestrial sphere contained four elements that come to rest at different 'natural places' therein. Aristotle believed that motionless objects on Earth, those composed mostly of the elements earth and water, to be in their natural place on the ground and that they will stay that way if left alone. He distinguished between the innate tendency of objects to find their 'natural place' (e.g., for heavy bodies to fall), which led to 'natural motion', and unnatural or forced motion, which required continued application of a force. This theory, based on the everyday experience of how objects move, such as the constant application of a force needed to keep a cart moving, had conceptual trouble accounting for the behavior of projectiles, such as the flight of arrows. The place where the archer moves the projectile was at the start of the flight, and while the projectile sailed through the air, no discernible efficient cause acts on it. Aristotle was aware of this problem and proposed that the air displaced through the projectile's path carries the projectile to its target. This explanation demands a continuum like air for change of place in general.\nOutput: ", "bloom_target": "What was the concept of force an integral part of?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Aristotle provided a philosophical discussion of the concept of a force as an integral part of Aristotelian cosmology. In Aristotle's view, the terrestrial sphere contained four elements that come to rest at different 'natural places' therein. Aristotle believed that motionless objects on Earth, those composed mostly of the elements earth and water, to be in their natural place on the ground and that they will stay that way if left alone. He distinguished between the innate tendency of objects to find their 'natural place' (e.g., for heavy bodies to fall), which led to 'natural motion', and unnatural or forced motion, which required continued application of a force. This theory, based on the everyday experience of how objects move, such as the constant application of a force needed to keep a cart moving, had conceptual trouble accounting for the behavior of projectiles, such as the flight of arrows. The place where the archer moves the projectile was at the start of the flight, and while the projectile sailed through the air, no discernible efficient cause acts on it. Aristotle was aware of this problem and proposed that the air displaced through the projectile's path carries the projectile to its target. This explanation demands a continuum like air for change of place in general.\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-034ffd5e88da4bf1b1e18dc2de4cd292", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-034ffd5e88da4bf1b1e18dc2de4cd292", "input": "Historically, the Methodist Church has supported the temperance movement. John Wesley warned against the dangers of drinking in his famous sermon, 'The Use of Money,' and in his letter to an alcoholic. At one time, Methodist ministers had to take a pledge not to drink and encouraged their congregations to do the same. Today the United Methodist Church states that it 'affirms our long-standing support of abstinence from alcohol as a faithful witness to God's liberating and redeeming love for persons.' In fact, the United Methodist Church uses unfermented grape juice in the sacrament of Holy Communion, thus 'expressing pastoral concern for recovering alcoholics, enabling the participation of children and youth, and supporting the church's witness of abstinence.' Moreover, in 2011 and 2012, The United Methodist Church's General Board of Church and Society called on all United Methodists to abstain from alcohol for Lent.", "output": ["Historically, which movement has the Methodist Church supported?", "When did the UMC's General Board of Church and Society call on all United Methodists to abstain from alcohol for Lent?", "What was the name of Wesley's famous sermon in which he warned against the dangers of drinking?", "What does the United Methodist Church use in the sacrament of the Holy Communion?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Historically, the Methodist Church has supported the temperance movement. John Wesley warned against the dangers of drinking in his famous sermon, 'The Use of Money,' and in his letter to an alcoholic. At one time, Methodist ministers had to take a pledge not to drink and encouraged their congregations to do the same. Today the United Methodist Church states that it 'affirms our long-standing support of abstinence from alcohol as a faithful witness to God's liberating and redeeming love for persons.' In fact, the United Methodist Church uses unfermented grape juice in the sacrament of Holy Communion, thus 'expressing pastoral concern for recovering alcoholics, enabling the participation of children and youth, and supporting the church's witness of abstinence.' Moreover, in 2011 and 2012, The United Methodist Church's General Board of Church and Society called on all United Methodists to abstain from alcohol for Lent.\nOutput: ", "bloom_target": "When did the UMC's General Board of Church and Society call on all United Methodists to abstain from alcohol for Lent?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: Historically, the Methodist Church has supported the temperance movement. John Wesley warned against the dangers of drinking in his famous sermon, 'The Use of Money,' and in his letter to an alcoholic. At one time, Methodist ministers had to take a pledge not to drink and encouraged their congregations to do the same. Today the United Methodist Church states that it 'affirms our long-standing support of abstinence from alcohol as a faithful witness to God's liberating and redeeming love for persons.' In fact, the United Methodist Church uses unfermented grape juice in the sacrament of Holy Communion, thus 'expressing pastoral concern for recovering alcoholics, enabling the participation of children and youth, and supporting the church's witness of abstinence.' Moreover, in 2011 and 2012, The United Methodist Church's General Board of Church and Society called on all United Methodists to abstain from alcohol for Lent.\nOutput:  The", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
{"id": "task1609-8cec6e9db15a4155b7ebb487e4956740", "Task": "task1609_xquad_en_question_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Generation"], "Reasoning": [], "Definition": ["Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage."], "Positive Examples": [{"input": "There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.", "output": "How many segments of virus DNA does the human genome contain?", "explanation": "The paragraph states the number of segments of virus DNA does the human genome contain, hence the question asked above is appropriate."}, {"input": "When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.", "output": "When does the mice behavior change?", "explanation": "As the paragraph states that mice behavior changes when kept at high population densities, the question mentioned above can be an expected output."}, {"input": "Piedmont, or mountain, glaciers are found in many parts of the world. In North America they are distributed along the mountain ranges of the Pacific Coast from central California northward. They abound in the Andes range in South America and are familiar and greatly admired spectacles in the Alps.", "output": "What mountain ranges are present in South America?", "explanation": "The task identified a question whose answer is Andes that is present in the text. Therefore, this serves as a good example."}], "Negative Examples": [{"input": "My cherished toy as a new teenager was my bicycle. We lived in a soi with lots of pavement and no cars. I was head of the bike gang and we would set up ramps for jumps and design obstacle courses for racing.", "output": "What was the cherished car of the boy?", "explanation": "The paragraph claims that the cherished toy of boy was a bicyle but not car, hence, this question is a wrong output."}, {"input": "When I was a little boy in elementary school, the neighborhood kids and I all looked forward to playing so many games in my backyard during the long summer holidays between grades.", "output": "When did the boy miss school?", "explanation": "The text did not mention about the times when the boy has missed the school, hence, this is not a valid question."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1609-8cec6e9db15a4155b7ebb487e4956740", "input": "However, his religious views remain uncertain due to other statements that he made. For example, in his article, 'A Machine to End War', published in 1937, Tesla stated:", "output": ["What article was published in 1937?", "Because of certain statements what was the believed state of his religious views?", "In the article's title, what did the machine hope to end?"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: However, his religious views remain uncertain due to other statements that he made. For example, in his article, 'A Machine to End War', published in 1937, Tesla stated:\nOutput: ", "bloom_target": "In the article's title, what did the machine hope to end?", "bloom_response": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\n\n Positive Example 1 -\nInput: There are a million times more viruses on the planet than stars in the universe. Viruses also harbor the majority of genetic diversity on Earth. Scientists are finding evidence of viruses as a planetary force, influencing the global climate and geochemical cycles. They have also profoundly shaped the evolution of their hosts. The human genome, for example, contains 100,000 segments of virus DNA.\n Output: How many segments of virus DNA does the human genome contain?\n\n Positive Example 2 -\nInput: When mice are kept at high population densities, their behaviour changes in a number of ways. Aggressive activity within populations of mice rises as density increases.\n Output: When does the mice behavior change?\n\nNow complete the following example -\nInput: However, his religious views remain uncertain due to other statements that he made. For example, in his article, 'A Machine to End War', published in 1937, Tesla stated:\nOutput:  What", "prediction": "Definition: Given a paragraph, your job is to generate a question that can be answered from the passage"}
