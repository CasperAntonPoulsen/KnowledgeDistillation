{"id": "task413-cb95090900cd4ac28cbd471a4478078f", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-cb95090900cd4ac28cbd471a4478078f", "input": "You would join the process because you want action and debate .", "output": ["You would challenge the opposition because you worship action and adventure .", "You would join the RSS because you want action and popularity .", "You would join the army because you want action and adventure.", "You would test the franchise because you allow action and adventure ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would join the process because you want action and debate .\nOutput: ", "bloom_target": "You would challenge the opposition because you worship action and adventure .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would join the process because you want action and debate .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-eeac8af11fe54e7586d408bafa66bfe3", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-eeac8af11fe54e7586d408bafa66bfe3", "input": "You would wake up in the morning because you want to dig school .", "output": ["You would wake up in the morning because you want to attend school.", "You would wake up in the den because you want to call school .", "You would wake up in the m because you want to attend school .", "You would wake up in the morning because you use to pick school ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would wake up in the morning because you want to dig school .\nOutput: ", "bloom_target": "You would wake up in the m because you want to attend school .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would wake up in the morning because you want to dig school .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-4b425a2aa5fd4f4aaf2cb6796edd5c4c", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-4b425a2aa5fd4f4aaf2cb6796edd5c4c", "input": "The world is sweeping out of fossil fuels .", "output": ["The world is shooting out of fossil fuels .", "The world is breaking out of fossil fuels .", "The world is evolving out of fossil fuels .", "The world is running out of fossil fuels."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: The world is sweeping out of fossil fuels .\nOutput: ", "bloom_target": "The world is shooting out of fossil fuels .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: The world is sweeping out of fossil fuels .\nOutput:  The", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-eaa14c72077c4d3f9fa2359400a4b1b1", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-eaa14c72077c4d3f9fa2359400a4b1b1", "input": "You would listen to music because you want have a brilliant time .", "output": ["You would listen to music because you want have a certain time .", "You would listen to music because you want have a mindless time .", "You would listen to music because you want have a good time.", "You would introduce to music because you want have a good time ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would listen to music because you want have a brilliant time .\nOutput: ", "bloom_target": "You would introduce to music because you want have a good time .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would listen to music because you want have a brilliant time .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-7c95564706074fbcb17612ff2d7b2f5d", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-7c95564706074fbcb17612ff2d7b2f5d", "input": "You are privileged to bear a tail in movies .", "output": ["You are likely to find a horse in movies.", "You are likely to choose a horse in movies .", "You are fun to channel a student in movies .", "You are likely to approach a horse in movies ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are privileged to bear a tail in movies .\nOutput: ", "bloom_target": "You are fun to channel a student in movies .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are privileged to bear a tail in movies .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-456c25ca0ef6474cb7f709689a9c30d9", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-456c25ca0ef6474cb7f709689a9c30d9", "input": "I cooked my finger in the oven .", "output": ["I inserted my profile in the DB .", "I cooked my food in the oven.", "I cut my foot in the oven .", "I cooked my tongue in the oven ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: I cooked my finger in the oven .\nOutput: ", "bloom_target": "I inserted my profile in the DB .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: I cooked my finger in the oven .\nOutput:  I", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-c0492849f5f84cd09c0db9b382cb0f39", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-c0492849f5f84cd09c0db9b382cb0f39", "input": "You can use a hardware element to have tools .", "output": ["You can walk a hardware store to spot tools .", "You can use a hardware store to buy tools.", "You can share a hardware mod to buy tools .", "You can use a hardware tab to manipulate tools ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can use a hardware element to have tools .\nOutput: ", "bloom_target": "You can walk a hardware store to spot tools .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can use a hardware element to have tools .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-68f8090bd67c40dda5e7b661f756576e", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-68f8090bd67c40dda5e7b661f756576e", "input": "You can use a talk doll to encourage family conversation .", "output": ["You can use a spelling assessment to encourage family conversation .", "You can modify a board game to raise family responsibility .", "You can use a board game to encourage family conversation.", "You can sell a board game to encourage team conversation ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can use a talk doll to encourage family conversation .\nOutput: ", "bloom_target": "You can sell a board game to encourage team conversation .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can use a talk doll to encourage family conversation .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-22a8c32cc23447f3b8b1d09c0ff8828c", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-22a8c32cc23447f3b8b1d09c0ff8828c", "input": "You would lead to a performance because you bought tickets .", "output": ["You would see to a performance because you bought tickets .", "You would advance to a performance because you hated tickets .", "You would go to a performance because you bought tickets.", "You would go to a performance because you bought toys ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would lead to a performance because you bought tickets .\nOutput: ", "bloom_target": "You would go to a performance because you bought tickets.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would lead to a performance because you bought tickets .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-52ac768190ff4a5a8aaf4df470b7738e", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-52ac768190ff4a5a8aaf4df470b7738e", "input": "Sometimes knees , after being injured , can be reconstructed for better efficacy .", "output": ["Sometimes knees , after being injured , can be reconstructed for better storage .", "Sometimes knees , after being injured , can be reconstructed for better productivity .", "Sometimes knees, after being injured, can be reconstructed for better use.", "Sometimes knees , after being injured , can be reconstructed for better distribution ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Sometimes knees , after being injured , can be reconstructed for better efficacy .\nOutput: ", "bloom_target": "Sometimes knees, after being injured, can be reconstructed for better use.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Sometimes knees , after being injured , can be reconstructed for better efficacy .\nOutput:  Sometimes", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-fe812e56b82e4547a1c97711a25caa5a", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-fe812e56b82e4547a1c97711a25caa5a", "input": "It is a good idea to quench your thirst when you are salty .", "output": ["It is a cute idea to quench your thirst when you are thirsty .", "It is a good idea to quench your lust when you are thirsty .", "It is a good idea to quench your thirst when you are thirsty.", "It is a good idea to quench your thirst when you are interested ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: It is a good idea to quench your thirst when you are salty .\nOutput: ", "bloom_target": "It is a cute idea to quench your thirst when you are thirsty .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: It is a good idea to quench your thirst when you are salty .\nOutput:  It", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-9c398ff4bc694706978d8f2f4e236dec", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-9c398ff4bc694706978d8f2f4e236dec", "input": "You can use a chess set to play a strategic interface .", "output": ["You can use a chess set to play a strategic thing .", "You can use a chess set to play a strategic form .", "You can use a chess set to play a strategic game.", "You can use a chess set to play a strategic draw ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can use a chess set to play a strategic interface .\nOutput: ", "bloom_target": "You can use a chess set to play a strategic form .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can use a chess set to play a strategic interface .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-7cb00cd8bb2b496b9e60c94d0d0fcac7", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-7cb00cd8bb2b496b9e60c94d0d0fcac7", "input": "You would pay bill because you have a pocket full of money .", "output": ["You would pay cash because you have a pocket full of money.", "You would pay cash because you have a pocket full of candy .", "You would win cash because you have a camera full of money .", "You would pay cash because you have a pocket full of pot ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would pay bill because you have a pocket full of money .\nOutput: ", "bloom_target": "You would pay cash because you have a pocket full of candy .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would pay bill because you have a pocket full of money .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-68613f889bcd4c09bc272bea4882fb4d", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-68613f889bcd4c09bc272bea4882fb4d", "input": "You can use a cat to hunt mice.", "output": ["You can use a flash to hunt mice .", "You can whip a cat to hunt mice .", "You can use a cat to walk mice .", "You can use a mask to hunt mice ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can use a cat to hunt mice.\nOutput: ", "bloom_target": "You can use a cat to walk mice .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can use a cat to hunt mice.\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-8d4f3449b4d145db8891e9c312d5fe32", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-8d4f3449b4d145db8891e9c312d5fe32", "input": "At the wedding ceremony, you would exchange gifts.", "output": ["At the wedding plan , you would exchange favors .", "At the coin ceremony , you would grab gifts .", "At the coin ceremony , you would own gifts .", "At the graduating ceremony , you would pay gifts ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: At the wedding ceremony, you would exchange gifts.\nOutput: ", "bloom_target": "At the coin ceremony , you would own gifts .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: At the wedding ceremony, you would exchange gifts.\nOutput:  At", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-3954a104c3034dc7847ce9995fcafa62", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-3954a104c3034dc7847ce9995fcafa62", "input": "You would visit other countries because you want to experience other cultures.", "output": ["You would visit other individuals because you want to experience other cultures .", "You would visit other streets because you fear to overcome other cultures .", "You would visit other countries because you want to experience other talents .", "You would visit other countries because you want to experience other fields ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would visit other countries because you want to experience other cultures.\nOutput: ", "bloom_target": "You would visit other streets because you fear to overcome other cultures .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would visit other countries because you want to experience other cultures.\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-5a171f3dd2f7400e9ad57e2d72a04d75", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-5a171f3dd2f7400e9ad57e2d72a04d75", "input": "You are likely to generate a key model in a hardware way .", "output": ["You are likely to find a whole chain in a hardware store .", "You are likely to find a key chain in a hardware store.", "You are likely to work a key delay in a hardware situation .", "You are likely to find a decent chain in a hardware store ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to generate a key model in a hardware way .\nOutput: ", "bloom_target": "You are likely to find a key chain in a hardware store.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to generate a key model in a hardware way .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-b29bb48965cd4a99a1a0e62da0ff2cc9", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-b29bb48965cd4a99a1a0e62da0ff2cc9", "input": "A snowball is used to defend oneself against an alert .", "output": ["A chain is used to store oneself against an attack .", "A projectile is used to ensure oneself against an attack .", "A weapon is used to defend oneself against an attack.", "A club is used to defend oneself against an A ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: A snowball is used to defend oneself against an alert .\nOutput: ", "bloom_target": "A chain is used to store oneself against an attack .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: A snowball is used to defend oneself against an alert .\nOutput:  A", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-7c2b02aa350a4916a51bafc5f9d32d76", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-7c2b02aa350a4916a51bafc5f9d32d76", "input": "You are likely to move a doll in a girl 's bedroom .", "output": ["You are mistaken to find a python in a girl 's bedroom .", "You are likely to avoid a doll in a girl 's bedroom .", "You are disgusted to find a doll in a girl 's son .", "You are likely to find a doll in a girl's bedroom."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to move a doll in a girl 's bedroom .\nOutput: ", "bloom_target": "You are likely to find a doll in a girl's bedroom.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to move a doll in a girl 's bedroom .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-cb895106c9cc4ebc9087cf11f572bd0c", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-cb895106c9cc4ebc9087cf11f572bd0c", "input": "You are likely to say a container drum in a bow .", "output": ["You are likely to find a container drum in a factory.", "You are likely to find a container brand in a factory .", "You are likely to eat a container drum in a laboratory .", "You are liable to crash a container boat in a factory ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to say a container drum in a bow .\nOutput: ", "bloom_target": "You are likely to find a container drum in a factory.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to say a container drum in a bow .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-b64cb737230e43509313e71cd928ab78", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-b64cb737230e43509313e71cd928ab78", "input": "You can use a boat to cross water.", "output": ["You can use a boat to attract water .", "You can begin a family to cross crossing .", "You can use a monkey to cross water .", "You can use a hoop to cross water ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can use a boat to cross water.\nOutput: ", "bloom_target": "You can use a hoop to cross water .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can use a boat to cross water.\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-51be7369b65b4da8b8873f39cc2fe080", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-51be7369b65b4da8b8873f39cc2fe080", "input": "Some oven is made of glass .", "output": ["Some bottles is conceived of glass .", "Some bottles is made of glass.", "Some property is makes of glass .", "Some skeleton is handmade of glass ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Some oven is made of glass .\nOutput: ", "bloom_target": "Some bottles is conceived of glass .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Some oven is made of glass .\nOutput:  Some", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-36b0f8a750e4462891c5c51af707f73e", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-36b0f8a750e4462891c5c51af707f73e", "input": "You would read a draft because you want to gain information .", "output": ["You would leak a bomb because you want to deflect information .", "You would hear a book because you want to aggregate information .", "You would read a book because you choose to appreciate information .", "You would read a book because you want to gain information."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would read a draft because you want to gain information .\nOutput: ", "bloom_target": "You would leak a bomb because you want to deflect information .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would read a draft because you want to gain information .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-1aac5b7d01414a139390fe7be265c3fd", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-1aac5b7d01414a139390fe7be265c3fd", "input": "A flight bag should be small enough to fit under an airplane seat.", "output": ["A sling bag should be small enough to fit under an toilet seat .", "A gift bag should be small enough to fit under an airplane seat .", "A traveling bag should be small enough to fit under an airplane seat .", "A pillow bag should be small enough to fit under an airplane seat ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: A flight bag should be small enough to fit under an airplane seat.\nOutput: ", "bloom_target": "A traveling bag should be small enough to fit under an airplane seat .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: A flight bag should be small enough to fit under an airplane seat.\nOutput:  A", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-dc9e1c4fb6704c569a329f900b25f498", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-dc9e1c4fb6704c569a329f900b25f498", "input": "Organized organization members can recreate a sports game .", "output": ["Organized FW members can contribute a sports game .", "Organized pool members can invite a sports game .", "Organized uild members can compete a sports game .", "Organized crime members can fix a sports game."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Organized organization members can recreate a sports game .\nOutput: ", "bloom_target": "Organized crime members can fix a sports game.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Organized organization members can recreate a sports game .\nOutput:  Organ", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-beeaffdeeeb14d7ebde9eb5311bff075", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-beeaffdeeeb14d7ebde9eb5311bff075", "input": "You can buy corn at a farmer's market.", "output": ["You can buy corn at a farmer 's plant .", "You can package corn at a printer 's work .", "You can fix corn at a worker 's grill .", "You can buy corn at a farmer 's sal ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can buy corn at a farmer's market.\nOutput: ", "bloom_target": "You can buy corn at a farmer 's plant .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can buy corn at a farmer's market.\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-adc7f14ab4b24c9e93b7e38c7a9833a5", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-adc7f14ab4b24c9e93b7e38c7a9833a5", "input": "Your closest followers can protect your honour .", "output": ["Your closest friends can break your heart.", "Your closest aides can break your grasp .", "Your closest coworkers can break your net .", "Your closest supporters can rebuild your heart ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Your closest followers can protect your honour .\nOutput: ", "bloom_target": "Your closest friends can break your heart.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Your closest followers can protect your honour .\nOutput:  Your", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-4c046f7b660b4896a814c0a24d7df389", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-4c046f7b660b4896a814c0a24d7df389", "input": "If you work to dominate ice combat then you should enjoy it .", "output": ["If you want to eat ice cream then you should enjoy it.", "If you want to clean ice cream then you should enjoy it .", "If you want to eat ice cream then you should split it .", "If you want to eat ice cream then you should follow it ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you work to dominate ice combat then you should enjoy it .\nOutput: ", "bloom_target": "If you want to eat ice cream then you should enjoy it.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you work to dominate ice combat then you should enjoy it .\nOutput:  If", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-b4acfe745d714154b79cd71ec4a44b23", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-b4acfe745d714154b79cd71ec4a44b23", "input": "You are helpful to find a time table in a house .", "output": ["You are likely to find a kitchen table in a house.", "You are delighted to find a hunting table in a house .", "You are likely to find a kitchen cook in a house .", "You are likely to find a kitchen element in a house ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are helpful to find a time table in a house .\nOutput: ", "bloom_target": "You are delighted to find a hunting table in a house .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are helpful to find a time table in a house .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-c4b007e989024c179682c6da4c2e1641", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-c4b007e989024c179682c6da4c2e1641", "input": "If you want to clean the shed then you should Get help .", "output": ["If you dig to clean the house then you should hire help .", "If you qualify to rescue the house then you should dispatch help .", "If you want to clean the house then you should hire help.", "If you arrange to advertise the house then you should hire money ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you want to clean the shed then you should Get help .\nOutput: ", "bloom_target": "If you qualify to rescue the house then you should dispatch help .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you want to clean the shed then you should Get help .\nOutput:  If", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-512111a0090949ea85425530694a8434", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-512111a0090949ea85425530694a8434", "input": "A playlist can help you find your way .", "output": ["A map can help you find your way.", "A map can cover you find your way .", "A specialist can help you find your way .", "A map can direct you find your way ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: A playlist can help you find your way .\nOutput: ", "bloom_target": "A map can help you find your way.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: A playlist can help you find your way .\nOutput:  A", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-187321d3ce2d4ddd9567d510207b524e", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-187321d3ce2d4ddd9567d510207b524e", "input": "running in the rain is for getting somewhere when you do n't have a car .", "output": ["Walking in the rain is for getting somewhere when you don't have a car.", "Walking in the loop is for getting somewhere when you do n't have a car .", "Walking in the week is for getting somewhere when you do n't have a car .", "Walking in the state is for getting somewhere when you do n't have a car ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: running in the rain is for getting somewhere when you do n't have a car .\nOutput: ", "bloom_target": "Walking in the week is for getting somewhere when you do n't have a car .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: running in the rain is for getting somewhere when you do n't have a car .\nOutput:  running", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-36cf5632b6614e2c9fdc74d664e70f19", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-36cf5632b6614e2c9fdc74d664e70f19", "input": "You can use a line to handle geometric figures .", "output": ["You can use a line to make geometric floats .", "You can use a device to make geometric laws .", "You can use a line to make geometric applications .", "You can use a line to make geometric figures."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can use a line to handle geometric figures .\nOutput: ", "bloom_target": "You can use a line to make geometric figures.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can use a line to handle geometric figures .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-6bad6aa0d31c4b66b309a5e616915960", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-6bad6aa0d31c4b66b309a5e616915960", "input": "You would junk a computer because you want to target email .", "output": ["You would use a computer because you want to send email.", "You would use a attachment because you want to send email .", "You would welcome a computer because you want to join email .", "You would raise a fax because you want to send email ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would junk a computer because you want to target email .\nOutput: ", "bloom_target": "You would raise a fax because you want to send email .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would junk a computer because you want to target email .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-e9effce04aec4fd5a5baf97b53716d75", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-e9effce04aec4fd5a5baf97b53716d75", "input": "If you want to go on the stage then you should ask the director.", "output": ["If you intention to go on the stage then you should ask the director .", "If you want to dub on the fight then you should ask the director .", "If you say to go on the holiday then you should accept the director .", "If you want to quit on the attempt then you should ask the director ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you want to go on the stage then you should ask the director.\nOutput: ", "bloom_target": "If you say to go on the holiday then you should accept the director .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you want to go on the stage then you should ask the director.\nOutput:  If", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-bdaa0b781ddc4c7cada0cc39fae95a37", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-bdaa0b781ddc4c7cada0cc39fae95a37", "input": "Every industry must be clean to ensure credit .", "output": ["Every server must be clean to ensure production .", "Every bed must be clean to ensure professionalism .", "Every house must be clean to ensure safety.", "Every house be be clean to ensure dignity ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Every industry must be clean to ensure credit .\nOutput: ", "bloom_target": "Every house be be clean to ensure dignity .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Every industry must be clean to ensure credit .\nOutput:  Every", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-4a3aabb581ae4957a017e29132770615", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-4a3aabb581ae4957a017e29132770615", "input": "A sounding alarm would make you want to stand up .", "output": ["A ch alarm would make you want to stand up .", "A break alarm would make you want to stand up .", "A water alarm would make you want to stand up .", "A fire alarm would make you want to stand up."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: A sounding alarm would make you want to stand up .\nOutput: ", "bloom_target": "A ch alarm would make you want to stand up .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: A sounding alarm would make you want to stand up .\nOutput:  A", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-ce78be82a71a457f8eeca48d13583d44", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-ce78be82a71a457f8eeca48d13583d44", "input": "Weapons get into a bad person sometimes .", "output": ["People get into a dreadful mood sometimes .", "People get into a bad mood sometimes.", "People get into a bad mood anymore .", "People get into a bad mood maybe ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Weapons get into a bad person sometimes .\nOutput: ", "bloom_target": "People get into a bad mood anymore .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Weapons get into a bad person sometimes .\nOutput:  Weapons", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-0c9cb3c675e444f4ab18d02e25a40af5", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-0c9cb3c675e444f4ab18d02e25a40af5", "input": "A bottle can be made of plastic.", "output": ["A bottle can be conceived of instead .", "A file can be assembly of list .", "A bottle can be made of Plastic .", "A poem can be approved of better ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: A bottle can be made of plastic.\nOutput: ", "bloom_target": "A bottle can be made of Plastic .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: A bottle can be made of plastic.\nOutput:  A", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-6872f40cf520400080162ede257ba9ac", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-6872f40cf520400080162ede257ba9ac", "input": "Pain can conceal a cold .", "output": ["People can catch a cold.", "Spring can catch a cold .", "things can catch a cold .", "Work can confuse a cold ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Pain can conceal a cold .\nOutput: ", "bloom_target": "Work can confuse a cold .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Pain can conceal a cold .\nOutput:  Pain", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-a5cd6914b68848358dad17ce1809dcf4", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-a5cd6914b68848358dad17ce1809dcf4", "input": "You use it to put etchup on your food .", "output": ["You use it to put iodine on your food .", "You paste it to put type on your folder .", "You use it to put cooking on your food .", "You use it to put salt on your food."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You use it to put etchup on your food .\nOutput: ", "bloom_target": "You paste it to put type on your folder .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You use it to put etchup on your food .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-dc8ba4b478d1421490375ee58bed5f2b", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-dc8ba4b478d1421490375ee58bed5f2b", "input": "You are likely to find a poem in a poetry book.", "output": ["You are likely to approach a poem in a poetry book .", "You are likely to recall a poem in a poetry book .", "You are likely to find a snippet in a poetry book .", "You are likely to find a role in a poetry book ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to find a poem in a poetry book.\nOutput: ", "bloom_target": "You are likely to find a snippet in a poetry book .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to find a poem in a poetry book.\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-27ec8347758d4d2fbd1415c6f888eba3", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-27ec8347758d4d2fbd1415c6f888eba3", "input": "If you want to talk then you should flap your lips and make attempt .", "output": ["If you want to bleed then you should flap your tires and make noise .", "If you excuse to talk then you should flap your lips and beat noise .", "If you see to talk then you should flap your lips and report noise .", "If you want to talk then you should flap your lips and make noise."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you want to talk then you should flap your lips and make attempt .\nOutput: ", "bloom_target": "If you want to bleed then you should flap your tires and make noise .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you want to talk then you should flap your lips and make attempt .\nOutput:  If", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-301c5ff30dcc41a89fd5921257e1a32f", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-301c5ff30dcc41a89fd5921257e1a32f", "input": "You are likely to find a jack around in hole .", "output": ["You are likely to sprinkle a snake around in hole .", "You are OK to loop a snake around in home .", "You are likely to find a snake around in hole.", "You are likely to awaken a snake around in hole ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to find a jack around in hole .\nOutput: ", "bloom_target": "You are likely to awaken a snake around in hole .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to find a jack around in hole .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-9fffa84cc11848c68ae5be2883881341", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-9fffa84cc11848c68ae5be2883881341", "input": "You are likely to find a pair in a palm tree .", "output": ["You are distressed to find a monkey in a sleeping carriage .", "You are likely to find a monkey in a palm tree.", "You are likely to confuse a monkey in a palm tree .", "You are likely to reach a monkey in a palm tree ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to find a pair in a palm tree .\nOutput: ", "bloom_target": "You are likely to reach a monkey in a palm tree .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to find a pair in a palm tree .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-86d5eabdc6bf4f77a62a8497397bf07d", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-86d5eabdc6bf4f77a62a8497397bf07d", "input": "You can use a Bitcoin store to purchase merchandise .", "output": ["You can use a card store to purchase merchandise .", "You can integrate a food store to share merchandise .", "You can customize a food store to feed merchandise .", "You can use a food store to purchase merchandise."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can use a Bitcoin store to purchase merchandise .\nOutput: ", "bloom_target": "You can use a food store to purchase merchandise.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can use a Bitcoin store to purchase merchandise .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-e02ea12103e44b0493a6bf75b893e3a3", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-e02ea12103e44b0493a6bf75b893e3a3", "input": "You can rob a place to eat to have lunch .", "output": ["You can collect a place to eat to have lunch .", "You can snag a place to eat to have lunch .", "You can use a place to eat to have lunch.", "You can expect a place to eat to have lunch ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can rob a place to eat to have lunch .\nOutput: ", "bloom_target": "You can expect a place to eat to have lunch .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can rob a place to eat to have lunch .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-bce00a4d60c6439fb9b62f5cdacd1c2d", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-bce00a4d60c6439fb9b62f5cdacd1c2d", "input": "You would go on the assignment because you want to collect information .", "output": ["You would go on the internet because you kill to collect information .", "You would go on the internet because you decide to collect information .", "You would give on the internet because you feel to collect gold .", "You would go on the internet because you want to collect information."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would go on the assignment because you want to collect information .\nOutput: ", "bloom_target": "You would give on the internet because you feel to collect gold .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would go on the assignment because you want to collect information .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-365cdd05270f4b4da5af47b0dfc12cf7", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-365cdd05270f4b4da5af47b0dfc12cf7", "input": "You are safe to find a nest in a map website .", "output": ["You are motivated to find a recommendation in a map website .", "You are responsible to find a world in a map website .", "You are likely to find a map in a map website.", "You are likely to explore a map in a source website ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are safe to find a nest in a map website .\nOutput: ", "bloom_target": "You are motivated to find a recommendation in a map website .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are safe to find a nest in a map website .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-66dc6aa47b244f06ae63d93b5281a6b1", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-66dc6aa47b244f06ae63d93b5281a6b1", "input": "You are determined to rub a hat in a chalk box .", "output": ["You are OK to sniff a hat in a broom box .", "You are likely to find a hat in a hat box.", "You are likely to order a hat in a hat box .", "You are likely to stay a hat in a hat story ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are determined to rub a hat in a chalk box .\nOutput: ", "bloom_target": "You are likely to find a hat in a hat box.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are determined to rub a hat in a chalk box .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-c29203b69e5a48dabd89e3d1f2c588a0", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-c29203b69e5a48dabd89e3d1f2c588a0", "input": "A ramp is used for reaching high places .", "output": ["A bird is used for migrating high places .", "A method is used for reaching high places .", "A sail is used for reaching high places .", "A ladder is used for reaching high places."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: A ramp is used for reaching high places .\nOutput: ", "bloom_target": "A bird is used for migrating high places .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: A ramp is used for reaching high places .\nOutput:  A", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-b8090a943ed94adc916797a09fdbc924", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-b8090a943ed94adc916797a09fdbc924", "input": "You would remember a phone number because you dare to call it ofter .", "output": ["You would remember a phone friend because you want to call it ofter .", "You would remember a phone number because you want to call it ofter.", "You would remember a phone hotline because you want to call it ofter .", "You would remember a phone booth because you want to call it ofter ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would remember a phone number because you dare to call it ofter .\nOutput: ", "bloom_target": "You would remember a phone booth because you want to call it ofter .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would remember a phone number because you dare to call it ofter .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-69725880281641af85dd5b73b3c768f8", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-69725880281641af85dd5b73b3c768f8", "input": "Water is for quenching steam .", "output": ["Training is for quenching interest .", "Water is for quenching interest .", "Water is for quenching thirst.", "stress is for quenching conflict ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Water is for quenching steam .\nOutput: ", "bloom_target": "Water is for quenching thirst.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Water is for quenching steam .\nOutput:  Water", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-78f3bb7166d14b9ca43f584f40323110", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-78f3bb7166d14b9ca43f584f40323110", "input": "Those suffering from persistent nightmares should refuse help from a therapist .", "output": ["Those suffering from persistent nightmares should seek help from a therapist.", "Those suffering from persistent nightmares should suggest help from a therapist .", "Those suffering from persistent overdoses should invoke help from a therapist .", "Those suffering from persistent comments should bring help from a therapist ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Those suffering from persistent nightmares should refuse help from a therapist .\nOutput: ", "bloom_target": "Those suffering from persistent nightmares should suggest help from a therapist .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Those suffering from persistent nightmares should refuse help from a therapist .\nOutput:  Those", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-044921a5df6643d1976acdc7c013d60e", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-044921a5df6643d1976acdc7c013d60e", "input": "You are likely to find a mutant in a snakes bone .", "output": ["You are likely to find a mouse in a snakes belly.", "You are likely to find a pit in a snakes domain .", "You are likely to bat a trout in a snakes belly .", "You are likely to find a ball in a snakes group ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to find a mutant in a snakes bone .\nOutput: ", "bloom_target": "You are likely to bat a trout in a snakes belly .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to find a mutant in a snakes bone .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-59e156e44e1548f1acee068cfd1f6e5d", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-59e156e44e1548f1acee068cfd1f6e5d", "input": "Wolves , like all newcomers , can see with their eyes .", "output": ["Wolves , like all lions , can see with their eyes .", "Wolves , like all experts , can see with their eyes .", "Wolves, like all mammals, can see with their eyes.", "Wolves , like all believers , can see with their eyes ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Wolves , like all newcomers , can see with their eyes .\nOutput: ", "bloom_target": "Wolves, like all mammals, can see with their eyes.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Wolves , like all newcomers , can see with their eyes .\nOutput:  Wol", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-6884e59dc23a4d56a2d99aeb38eec541", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-6884e59dc23a4d56a2d99aeb38eec541", "input": "If you want to report on the printer then you should use a q .", "output": ["If you want to try on the printer then you should use a stick .", "If you want to print on the printer then you should use a computer.", "If you want to tape on the printer then you should use a computer .", "If you want to layer on the printer then you should use a virtual ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you want to report on the printer then you should use a q .\nOutput: ", "bloom_target": "If you want to layer on the printer then you should use a virtual .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you want to report on the printer then you should use a q .\nOutput:  If", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-f42cb13006c64296bdcab04f947bc986", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-f42cb13006c64296bdcab04f947bc986", "input": "You are likely to care a mill around in warm waters .", "output": ["You are likely to snap a towel around in warm waters .", "You are delusional to grow a shark around in warm waters .", "You are likely to save a neck around in warm waters .", "You are likely to find a shark around in warm waters."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to care a mill around in warm waters .\nOutput: ", "bloom_target": "You are likely to snap a towel around in warm waters .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to care a mill around in warm waters .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-35250889657149d8926b40c63c9ca040", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-35250889657149d8926b40c63c9ca040", "input": "Making bread is for break with meals .", "output": ["Making bread is for light with meals .", "Making bread is for heat with meals .", "Making suggestion is for example with meals .", "Making bread is for use with meals."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Making bread is for break with meals .\nOutput: ", "bloom_target": "Making bread is for use with meals.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Making bread is for break with meals .\nOutput:  Making", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-16fdcebbe7964ebfb394358be0eb3ce4", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-16fdcebbe7964ebfb394358be0eb3ce4", "input": "The effect of flying in an airplane is missing quickly .", "output": ["The air of flying in an airplane is collapsing quickly .", "The effect of flying in an airplane is building quickly .", "The effect of flying in an airplane is traveling quickly.", "The annoyance of flying in an airplane is expanding quickly ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: The effect of flying in an airplane is missing quickly .\nOutput: ", "bloom_target": "The annoyance of flying in an airplane is expanding quickly .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: The effect of flying in an airplane is missing quickly .\nOutput:  The", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-d3a7aa9122704e6899384a0b84c8ad39", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-d3a7aa9122704e6899384a0b84c8ad39", "input": "You would request a movie because you read the book .", "output": ["You would see a movie because you read the book.", "You would solve a jam because you read the book .", "You would remain a pro because you read the book .", "You would despise a movie because you read the book ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would request a movie because you read the book .\nOutput: ", "bloom_target": "You would remain a pro because you read the book .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would request a movie because you read the book .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-92450e93861c45919341656642c8d103", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-92450e93861c45919341656642c8d103", "input": "You would read the board because you want to trade opinions about the news .", "output": ["You would split the newspaper because you want to read opinions about the thing .", "You would read the newspaper because you want to read opinions about the news.", "You would answer the newspaper because you want to read opinions about the writing .", "You would read the majority because you want to base opinions about the news ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would read the board because you want to trade opinions about the news .\nOutput: ", "bloom_target": "You would split the newspaper because you want to read opinions about the thing .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would read the board because you want to trade opinions about the news .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-bf399b55398d4db78105db0534b9db94", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-bf399b55398d4db78105db0534b9db94", "input": "You would write a story because you have a spontaneous idea .", "output": ["You would write a story because you have a detailed idea .", "You would write a story because you have a weak idea .", "You would write a story because you have a sweet idea .", "You would write a story because you have a good idea."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would write a story because you have a spontaneous idea .\nOutput: ", "bloom_target": "You would write a story because you have a weak idea .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would write a story because you have a spontaneous idea .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-dbcb27d9751f48b4a6d2c7c72a204734", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-dbcb27d9751f48b4a6d2c7c72a204734", "input": "You are likely to distribute a plastic in cars .", "output": ["You are likely to find a plastic in cars.", "You are likely to perceive a plastic in cars .", "You are likely to shoot a plastic in cars .", "You are likely to abuse a plastic in cars ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to distribute a plastic in cars .\nOutput: ", "bloom_target": "You are likely to find a plastic in cars.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to distribute a plastic in cars .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-3a0f834aaf6a40869c31ea4d466ab877", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-3a0f834aaf6a40869c31ea4d466ab877", "input": "You would take in a film because life is boring at the moment.", "output": ["You would take in a film because life is small at the moment .", "You would take in a client because change is high at the moment .", "You would take in a film because life is boring at the store .", "You would take in a turtle because black is boring at the sun ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would take in a film because life is boring at the moment.\nOutput: ", "bloom_target": "You would take in a film because life is boring at the store .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would take in a film because life is boring at the moment.\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-25c0acb2aad14167aa10e642b7a2c0d0", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-25c0acb2aad14167aa10e642b7a2c0d0", "input": "If you want to slide around a lake then you should find a path .", "output": ["If you want to route around a lake then you should find a path .", "If you want to walk around a lake then you should find a path.", "If you want to eat around a lake then you should find a shop .", "If you want to paint around a lake then you should find a path ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you want to slide around a lake then you should find a path .\nOutput: ", "bloom_target": "If you want to route around a lake then you should find a path .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you want to slide around a lake then you should find a path .\nOutput:  If", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-b71633f752224064a1f0de3fa8d1b5ac", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-b71633f752224064a1f0de3fa8d1b5ac", "input": "You are likely to signal a bus station in the mall .", "output": ["You are likely to lease a bus station in the plan .", "You are likely to find a bus station in the city.", "You are correct to find a hash lock in the area .", "You are likely to accommodate a bus station in the past ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to signal a bus station in the mall .\nOutput: ", "bloom_target": "You are correct to find a hash lock in the area .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to signal a bus station in the mall .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-8a32916174894e898010158216f13ad2", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-8a32916174894e898010158216f13ad2", "input": "You can use a captain to command a ship.", "output": ["You can use a wizard to flip a note .", "You can dispatch a tracker to command a ship .", "You can solder a capacitor to command a ship .", "You can used a subclass to command a ship ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can use a captain to command a ship.\nOutput: ", "bloom_target": "You can dispatch a tracker to command a ship .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can use a captain to command a ship.\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-50a0232c82d54d8bba039ff4bcd6eecf", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-50a0232c82d54d8bba039ff4bcd6eecf", "input": "You are likely to unlock shoes in front door .", "output": ["You are likely to saw shoes in front door .", "You are likely to fold shoes in front door .", "You are likely to find shoes in front door.", "You are likely to load shoes in front door ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to unlock shoes in front door .\nOutput: ", "bloom_target": "You are likely to load shoes in front door .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to unlock shoes in front door .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-30934c5556e549728a1aaa8d99fcc909", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-30934c5556e549728a1aaa8d99fcc909", "input": "You would buy presents for others because you want to please them.", "output": ["You would buy pills for others because you manage to multiply them .", "You would buy presents for others because you use to please them .", "You would mail presents for others because you want to please them .", "You would buy motorcycles for others because you want to chase them ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would buy presents for others because you want to please them.\nOutput: ", "bloom_target": "You would buy pills for others because you manage to multiply them .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would buy presents for others because you want to please them.\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-ae3d9ff6b6c4480ba5206bf2cb823e29", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-ae3d9ff6b6c4480ba5206bf2cb823e29", "input": "You are likely to call a name list in future .", "output": ["You are likely to find a wine list in restaurant.", "You are likely to refuse a wine cut in restaurant .", "You are likely to pull a wine press in restaurant .", "You are likely to meet a director list in summer ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to call a name list in future .\nOutput: ", "bloom_target": "You are likely to pull a wine press in restaurant .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to call a name list in future .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-2ca7a268015049089e121e4714b34042", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-2ca7a268015049089e121e4714b34042", "input": "The future is a planet to dance .", "output": ["The ocean is a place to dance .", "The yard is a place to dance .", "The life is a Place to dance .", "The future is a place to dance."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: The future is a planet to dance .\nOutput: ", "bloom_target": "The future is a place to dance.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: The future is a planet to dance .\nOutput:  The", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-1ce23358365f4936842eede23cecfcd1", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-1ce23358365f4936842eede23cecfcd1", "input": "The president can govern the nation.", "output": ["The president can form the force .", "The president can flip the cash .", "The president can negotiate the bailout .", "The president can govern the department ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: The president can govern the nation.\nOutput: ", "bloom_target": "The president can flip the cash .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: The president can govern the nation.\nOutput:  The", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-e11bb92300304f4c8730993009d6d08f", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-e11bb92300304f4c8730993009d6d08f", "input": "You can stop the car outside the boat shop .", "output": ["You can stop the car outside the pet shop.", "You can stop the looting outside the phone shop .", "You can fit the buggy outside the pet shop .", "You can stop the car outside the med shop ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can stop the car outside the boat shop .\nOutput: ", "bloom_target": "You can stop the car outside the med shop .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can stop the car outside the boat shop .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-89b978f8ac754dc7b27926162b346784", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-89b978f8ac754dc7b27926162b346784", "input": "You can watch zombies on tv or at the park .", "output": ["You can obtain games on tv or at the park .", "You can end games on tv or at the blog .", "You can throw games on tv or at the park .", "You can watch games on tv or at the park."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can watch zombies on tv or at the park .\nOutput: ", "bloom_target": "You can end games on tv or at the blog .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can watch zombies on tv or at the park .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-ccb0ec50987d40d8a6854541b9d52d3a", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-ccb0ec50987d40d8a6854541b9d52d3a", "input": "You can publish a launch token to set off a replication .", "output": ["You can use a launch platform to set off a laser .", "You can use a computing platform to set off a state .", "You can use a launch platform to set off a rocket.", "You can use a burning platform to set off a rocket ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can publish a launch token to set off a replication .\nOutput: ", "bloom_target": "You can use a computing platform to set off a state .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can publish a launch token to set off a replication .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-d440c5868bec4340b286ae81fdb5f1ae", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-d440c5868bec4340b286ae81fdb5f1ae", "input": "If you want to impanel a jury then you should become a lawyer.", "output": ["If you win to impanel a jury then you should become a king .", "If you learn to impanel a jury then you should become a player .", "If you venture to impanel a jury then you should gather a resource .", "If you require to impanel a jury then you should kill a respondent ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you want to impanel a jury then you should become a lawyer.\nOutput: ", "bloom_target": "If you require to impanel a jury then you should kill a respondent .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you want to impanel a jury then you should become a lawyer.\nOutput:  If", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-fce884ed434846ab86f0f93927d86f32", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-fce884ed434846ab86f0f93927d86f32", "input": "If you strive to wait tables then you should get hired at a restaurant .", "output": ["If you crave to wait tables then you should get hired at a restaurant .", "If you want to wait others then you should get hired at a restaurant .", "If you want to wait tables then you should get hired at a restaurant.", "If you adore to wait tables then you should get hired at a restaurant ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you strive to wait tables then you should get hired at a restaurant .\nOutput: ", "bloom_target": "If you adore to wait tables then you should get hired at a restaurant .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you strive to wait tables then you should get hired at a restaurant .\nOutput:  If", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-8ddd7eec9baf4f4a92af83f216aa764b", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-8ddd7eec9baf4f4a92af83f216aa764b", "input": "You would roam outside for an evening because you want to see the turtles .", "output": ["You would scream outside for an evening because you want to see the stars .", "You would go outside for an evening because you want to see the stars.", "You would cozy outside for an evening because you want to see the stars .", "You would sweat outside for an evening because you want to see the stars ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would roam outside for an evening because you want to see the turtles .\nOutput: ", "bloom_target": "You would sweat outside for an evening because you want to see the stars .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would roam outside for an evening because you want to see the turtles .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-d4defb0fcbc649b08379f41a6c73bbe2", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-d4defb0fcbc649b08379f41a6c73bbe2", "input": "You can use a cloth to hold a stick .", "output": ["You can use a sidebar to maintain a slideshow .", "You can use a finger to hold a flower .", "You can use a finger to hold a stick.", "You can use a felt to hold a stick ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can use a cloth to hold a stick .\nOutput: ", "bloom_target": "You can use a sidebar to maintain a slideshow .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You can use a cloth to hold a stick .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-8dd8a4b4c44240229e1e290f8e901dea", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-8dd8a4b4c44240229e1e290f8e901dea", "input": "You are likely to find a snake in the desert.", "output": ["You are difficult to find a snake in the colony .", "You are likely to slip a snake in the milk .", "You are likely to witness a snake in the wind .", "You are mistaken to fight a snake in the desert ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to find a snake in the desert.\nOutput: ", "bloom_target": "You are difficult to find a snake in the colony .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to find a snake in the desert.\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-0858af84a57b4b81b939e82a236aae90", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-0858af84a57b4b81b939e82a236aae90", "input": "American hair would make you want to go to purchase a haircut .", "output": ["brown hair would make you want to go to wear a haircut .", "Long hair would make you want to go to get a haircut.", "Long hair would make you urge to go to get a kid .", "Random hair would make you want to go to require a haircut ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: American hair would make you want to go to purchase a haircut .\nOutput: ", "bloom_target": "Long hair would make you want to go to get a haircut.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: American hair would make you want to go to purchase a haircut .\nOutput:  American", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-4e4525734fde478a8e63831d6cacab93", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-4e4525734fde478a8e63831d6cacab93", "input": "People can resolve their fate .", "output": ["People can decide their fate.", "Events can continue their fate .", "women can assert their assertion .", "People can protest their fate ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: People can resolve their fate .\nOutput: ", "bloom_target": "People can protest their fate .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: People can resolve their fate .\nOutput:  People", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-eb258ec9855149f7805a33d8d7500628", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-eb258ec9855149f7805a33d8d7500628", "input": "If you want to look a result then you should turn a cookbook .", "output": ["If you want to devise a recipe then you should raid a cookbook .", "If you want to handle a horse then you should enter a cookbook .", "If you want to invent a drug then you should examine a cookbook .", "If you want to follow a recipe then you should read a cookbook."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you want to look a result then you should turn a cookbook .\nOutput: ", "bloom_target": "If you want to handle a horse then you should enter a cookbook .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you want to look a result then you should turn a cookbook .\nOutput:  If", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-2f498f1ef99c4c76b60f5a5915d88458", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-2f498f1ef99c4c76b60f5a5915d88458", "input": "A home is a slave to sunlight outside .", "output": ["A tent is a place to sleep outside.", "A tent is a trailer to block outside .", "A flag is a place to measure outside .", "A trailer is a place to sleep freely ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: A home is a slave to sunlight outside .\nOutput: ", "bloom_target": "A tent is a place to sleep outside.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: A home is a slave to sunlight outside .\nOutput:  A", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-8b99ed1a66644999b933b4af6e6004c3", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-8b99ed1a66644999b933b4af6e6004c3", "input": "You are likely to find an arm in a hangar .", "output": ["You are likely to send an explanation in a hangar .", "You are likely to trust an airplane in a hangar .", "You are glad to snag an eagle in a hangar .", "You are likely to find an airplane in a hangar."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to find an arm in a hangar .\nOutput: ", "bloom_target": "You are glad to snag an eagle in a hangar .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You are likely to find an arm in a hangar .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-adc6b4b7cc5141c382695e43f8b05c12", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-adc6b4b7cc5141c382695e43f8b05c12", "input": "If you Want to add up numbers then you should float a sequence .", "output": ["If you want to tie up numbers then you should remember a system .", "If you want to start up numbers then you should learn a color .", "If you strive to add up numbers then you should lift a cross .", "If you want to add up numbers then you should make a column."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you Want to add up numbers then you should float a sequence .\nOutput: ", "bloom_target": "If you strive to add up numbers then you should lift a cross .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you Want to add up numbers then you should float a sequence .\nOutput:  If", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-6599d119d6a644b792fa1652cadbe4fc", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-6599d119d6a644b792fa1652cadbe4fc", "input": "You would play a house with your birds because it aids boredom .", "output": ["You would limit a game with your friends because it prevents confrontation .", "You would game a game with your friends because it prevents bribery .", "You would play a game with your friends because it prevents boredom.", "You would sign a game with your friends because it prevents defeat ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would play a house with your birds because it aids boredom .\nOutput: ", "bloom_target": "You would play a game with your friends because it prevents boredom.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would play a house with your birds because it aids boredom .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-79bb43e863354264b7daf06b18154c07", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-79bb43e863354264b7daf06b18154c07", "input": "Inside ishes are grown in pots .", "output": ["Inside flowers are grown in pots.", "Asian flowers are grown in pots .", "Cool flowers are grown in pots .", "External flowers are grown in pots ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Inside ishes are grown in pots .\nOutput: ", "bloom_target": "External flowers are grown in pots .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Inside ishes are grown in pots .\nOutput:  Inside", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-d6759e81f6a649e3a7fd1a150a280ba9", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-d6759e81f6a649e3a7fd1a150a280ba9", "input": "The effect of cleaning clothing is to remove stains.", "output": ["The price of cleaning litter is to remove stains .", "The effect of cleaning wood is to remove acids .", "The course of cleaning software is to remove lines .", "The one of cleaning clean is to remove smells ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: The effect of cleaning clothing is to remove stains.\nOutput: ", "bloom_target": "The one of cleaning clean is to remove smells .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: The effect of cleaning clothing is to remove stains.\nOutput:  The", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-bc12a3a3a2fc4e26a69806d97bc1c9bc", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-bc12a3a3a2fc4e26a69806d97bc1c9bc", "input": "When the weather is very terrible , we talk of heat .", "output": ["When the weather is very warm, we talk of heat.", "When the weather is very warm , we talk of fun .", "When the storm is very effective , we talk of heat .", "When the weather is very turbulent , we talk of heat ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: When the weather is very terrible , we talk of heat .\nOutput: ", "bloom_target": "When the weather is very warm, we talk of heat.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: When the weather is very terrible , we talk of heat .\nOutput:  When", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-c084a1bc6d284607af02f640a0382a15", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-c084a1bc6d284607af02f640a0382a15", "input": "You would switch a house because you need a place to stay in .", "output": ["You would buy a SUV because you need a jacket to stay in .", "You would buy a pc because you need a keyboard to stay in .", "You would adopt a house because you need a place to stay in .", "You would buy a house because you need a place to stay in."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would switch a house because you need a place to stay in .\nOutput: ", "bloom_target": "You would buy a SUV because you need a jacket to stay in .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You would switch a house because you need a place to stay in .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-cca81cece69745c5b4bbc6232ca5fb36", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-cca81cece69745c5b4bbc6232ca5fb36", "input": "If you fancy to have a rest then you should hide in the bathroom .", "output": ["If you walk to have a rest then you should practice in the bathroom .", "If you want to have a rest then you should hide in the bathroom.", "If you stay to have a rest then you should hide in the bathroom .", "If you move to have a rest then you should hide in the bathroom ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you fancy to have a rest then you should hide in the bathroom .\nOutput: ", "bloom_target": "If you walk to have a rest then you should practice in the bathroom .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you fancy to have a rest then you should hide in the bathroom .\nOutput:  If", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-6c307dcd618b4d10ac707d771b1b8b8d", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-6c307dcd618b4d10ac707d771b1b8b8d", "input": "Food is rightly prepared in a stove .", "output": ["Food is often prepared in a kitchen.", "Food is slowly prepared in a manner .", "Food is partially prepared in a kitchen .", "Food is certainly prepared in a vegetable ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Food is rightly prepared in a stove .\nOutput: ", "bloom_target": "Food is partially prepared in a kitchen .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Food is rightly prepared in a stove .\nOutput:  Food", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-427870f18e5d4daab7135c3990d7616f", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-427870f18e5d4daab7135c3990d7616f", "input": "Some as competer in drivers racing .", "output": ["Some girls competer in animal racing .", "Some for competer in ski racing .", "Some pilots competer in airplane racing.", "Some frequently competer in fan racing ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Some as competer in drivers racing .\nOutput: ", "bloom_target": "Some girls competer in animal racing .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Some as competer in drivers racing .\nOutput:  Some", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-91c9812a493040a99c1688e9b41a4b4b", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-91c9812a493040a99c1688e9b41a4b4b", "input": "In general , children often attack their mother more than their community .", "output": ["In general, children often love their mother more than their father.", "In general , children notoriously love their mother more than their father .", "In general , sters often love their mother more than their father .", "In general , agents often love their mother more than their father ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: In general , children often attack their mother more than their community .\nOutput: ", "bloom_target": "In general, children often love their mother more than their father.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: In general , children often attack their mother more than their community .\nOutput:  In", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-9944dc2f7c35425fb6b73961aa756a09", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-9944dc2f7c35425fb6b73961aa756a09", "input": "If you want to accept a table then you should have a window .", "output": ["If you want to shoot a fire then you should have a flame .", "If you want to light a vehicle then you should have a flame .", "If you want to light a hand then you should have a flame .", "If you want to light a fire then you should have a flame."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you want to accept a table then you should have a window .\nOutput: ", "bloom_target": "If you want to light a fire then you should have a flame.", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: If you want to accept a table then you should have a window .\nOutput:  If", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-d8745f013cbe4f9888d4ebbc04d746a8", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-d8745f013cbe4f9888d4ebbc04d746a8", "input": "You may jump out of a window to escape a fear .", "output": ["You may observe out of a window to describe a characteristic .", "You may zip out of a window to spy a gunman .", "You may jump out of a crate to escape a fire .", "You may jump out of a window to escape a fire."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You may jump out of a window to escape a fear .\nOutput: ", "bloom_target": "You may observe out of a window to describe a characteristic .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: You may jump out of a window to escape a fear .\nOutput:  You", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-4b89fd3d4f8a4320b311b2c96d92590d", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-4b89fd3d4f8a4320b311b2c96d92590d", "input": "Clothes are kidnapped in a bar room .", "output": ["Clothes are washed in a laundry room.", "Clothes are hung in a laundry room .", "Clothes are washed in a training room .", "Clothes are washed in a factory room ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Clothes are kidnapped in a bar room .\nOutput: ", "bloom_target": "Clothes are washed in a factory room .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Clothes are kidnapped in a bar room .\nOutput:  Cl", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task413-e80624961f634392a26547f033e7ad0d", "Task": "task413_mickey_en_sentence_perturbation_generation", "Contributors": "['Maitreya Patel']", "Source": ["xcsr"], "URL": ["https://inklab.usc.edu/XCSR/"], "Categories": ["Sentence Perturbation"], "Reasoning": [], "Definition": ["Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true."], "Positive Examples": [{"input": "Stimulating the mind helps the brain to grow essential neural cells .", "output": "Stimulating the learning helps the brain to grow new neural cells .", "explanation": "This is a good example of a change in the input. The created sentence is semantically similar to the input as both are talking about growing essential neural cells and the changes in the sentence follows the commonsense knowledge."}, {"input": "A bath is suspended in a tub .", "output": "A pup is squeezed in a tub .", "explanation": "The output sentence is semantically related to the input sentence as both are talking about something to put inside the tub and follows the commonsense. Hence, the answer is correct."}], "Negative Examples": [{"input": "Country roads shocked me quite .", "output": "Night roads take me to moon .", "explanation": "The output is semantically similar to the input but it does not follow the commonsense as there is no road to the moon from the earth."}, {"input": "You would lucid dream because you believe to cure your loneliness .", "output": "Needing to hear the news would make you cry to listen to the video .", "explanation": "The output has the commonsense but it is not semantically related or similar to the input. Hence, the answer is incorrect."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Commonsense", "Knowledge Base -> Wikidata"], "Instance": {"id": "task413-e80624961f634392a26547f033e7ad0d", "input": "Washing your hands is for good health.", "output": ["Getting your age is for good r .", "icing your hands is for true health .", "Starting your hands is for daily health .", "Washing your fiber is for quality health ."]}, "Instance License": ["MIT"], "bloom_input": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Washing your hands is for good health.\nOutput: ", "bloom_target": "Getting your age is for good r .", "bloom_response": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence. Here, make sure that the changes are semantically related and syntactically similar to the input. And the generated sentence should have high commonsense plausibility, that is to have reasonable probability of it being true.\n\n Positive Example 1 -\nInput: Stimulating the mind helps the brain to grow essential neural cells .\n Output: Stimulating the learning helps the brain to grow new neural cells .\n\n Positive Example 2 -\nInput: A bath is suspended in a tub .\n Output: A pup is squeezed in a tub .\n\nNow complete the following example -\nInput: Washing your hands is for good health.\nOutput:  W", "prediction": "Definition: Given a sentence, generate a new sentence by performing small changes on the sentence"}
{"id": "task770-28cb14c614a342bf8c6595ff3688252f", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-28cb14c614a342bf8c6595ff3688252f", "input": "Kevin Lepage started third , Terry Labonte as fourth and Robby Gordon qualified as fifth .", "output": ["Kevin Lepage started third , Terry Labonte fourth , and Robby Gordon qualified fifth ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Kevin Lepage started third , Terry Labonte as fourth and Robby Gordon qualified as fifth .\nOutput: ", "bloom_target": "Kevin Lepage started third , Terry Labonte fourth , and Robby Gordon qualified fifth .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Kevin Lepage started third , Terry Labonte as fourth and Robby Gordon qualified as fifth .\nOutput:  Kevin", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-5cc74d40009345b79010747d43c88971", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-5cc74d40009345b79010747d43c88971", "input": "Taieri is a former parliamentary electorate in the Otago region of New Zealand , from 1866 to 1911 .", "output": ["Taieri is a former parliamentary electorate in the New Zealand region of Otago , from 1866 to 1911 ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Taieri is a former parliamentary electorate in the Otago region of New Zealand , from 1866 to 1911 .\nOutput: ", "bloom_target": "Taieri is a former parliamentary electorate in the New Zealand region of Otago , from 1866 to 1911 .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Taieri is a former parliamentary electorate in the Otago region of New Zealand , from 1866 to 1911 .\nOutput:  Tai", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-a77dc074fa8446558edf596735f17e8d", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-a77dc074fa8446558edf596735f17e8d", "input": "The first five weapons were delivered in the first half of 1916 , with a total of 57 barrels and 56 cars completed by the end of the war .", "output": ["The first five weapons were delivered in the first half of 1916 . A total of 57 barrels and 56 carriages were completed by the end of the war ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The first five weapons were delivered in the first half of 1916 , with a total of 57 barrels and 56 cars completed by the end of the war .\nOutput: ", "bloom_target": "The first five weapons were delivered in the first half of 1916 . A total of 57 barrels and 56 carriages were completed by the end of the war .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The first five weapons were delivered in the first half of 1916 , with a total of 57 barrels and 56 cars completed by the end of the war .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-0e7ee4e9f09f40e3b930aff0f09741ac", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-0e7ee4e9f09f40e3b930aff0f09741ac", "input": "Shaffer Creek is a tributary of the Raystown Branch Juniata River ( Brush Creek ) in Bedford County , Pennsylvania , United States .", "output": ["Shaffer Creek is an tributary of Brush Creek ( Raystown Branch Juniata River ) in Bedford County , Pennsylvania in the United States ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Shaffer Creek is a tributary of the Raystown Branch Juniata River ( Brush Creek ) in Bedford County , Pennsylvania , United States .\nOutput: ", "bloom_target": "Shaffer Creek is an tributary of Brush Creek ( Raystown Branch Juniata River ) in Bedford County , Pennsylvania in the United States .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Shaffer Creek is a tributary of the Raystown Branch Juniata River ( Brush Creek ) in Bedford County , Pennsylvania , United States .\nOutput:  Sha", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-e1f4250f762f4cd0a5580ba8f2f8b666", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-e1f4250f762f4cd0a5580ba8f2f8b666", "input": "The school is connected with the secondary department of the autistic school Ysgol Plas Brondyffryn , which was built in 2003 .", "output": ["The school is in conjunction with the secondary department of the autistic school Ysgol Plas Brondyffryn , which was built in 2003 ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The school is connected with the secondary department of the autistic school Ysgol Plas Brondyffryn , which was built in 2003 .\nOutput: ", "bloom_target": "The school is in conjunction with the secondary department of the autistic school Ysgol Plas Brondyffryn , which was built in 2003 .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The school is connected with the secondary department of the autistic school Ysgol Plas Brondyffryn , which was built in 2003 .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-c6b6476aaa4a4a3cbef53f6bded130b6", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-c6b6476aaa4a4a3cbef53f6bded130b6", "input": "Pelmus has been exhibited in museums in Romania , France , Germany , Israel , Canada , Austria , Chile and Brazil , including five solo exhibitions .", "output": ["Pelmus has been exhibited in museums of Romania , France , Germany , Israel , Canada , Austria , Chile and Brazil , including five solo exhibitions ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Pelmus has been exhibited in museums in Romania , France , Germany , Israel , Canada , Austria , Chile and Brazil , including five solo exhibitions .\nOutput: ", "bloom_target": "Pelmus has been exhibited in museums of Romania , France , Germany , Israel , Canada , Austria , Chile and Brazil , including five solo exhibitions .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Pelmus has been exhibited in museums in Romania , France , Germany , Israel , Canada , Austria , Chile and Brazil , including five solo exhibitions .\nOutput:  Pel", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-15f6a2097b3e44a7af513f90dcee9ce4", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-15f6a2097b3e44a7af513f90dcee9ce4", "input": "Malhar Joshi should not be confused with the author Waman Gopal Joshi , they were contemporaries .", "output": ["Vaman Malhar Joshi should not be confused with the writer Waman Gopal Joshi . They were contemporaries ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Malhar Joshi should not be confused with the author Waman Gopal Joshi , they were contemporaries .\nOutput: ", "bloom_target": "Vaman Malhar Joshi should not be confused with the writer Waman Gopal Joshi . They were contemporaries .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Malhar Joshi should not be confused with the author Waman Gopal Joshi , they were contemporaries .\nOutput:  Mal", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-1c0577a26e89434389e6fab7ed0d6b28", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-1c0577a26e89434389e6fab7ed0d6b28", "input": "On November 10 , 2015 , ATP editor Josh Meiseles confirmed the official list of 8 players on the final website of the ATP World Tour .", "output": ["On 10 November 2015 , ATP editor Josh Meiseles confirmed the official list of 8 players on the ATP World Tour final website ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: On November 10 , 2015 , ATP editor Josh Meiseles confirmed the official list of 8 players on the final website of the ATP World Tour .\nOutput: ", "bloom_target": "On 10 November 2015 , ATP editor Josh Meiseles confirmed the official list of 8 players on the ATP World Tour final website .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: On November 10 , 2015 , ATP editor Josh Meiseles confirmed the official list of 8 players on the final website of the ATP World Tour .\nOutput:  On", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-7dc941fe5f114cf49aa8560653382cd7", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-7dc941fe5f114cf49aa8560653382cd7", "input": "Most pre- 1976 series produced by CBS or distributed by CBS Films were later distributed by Viacom and Paramount Television .", "output": ["Most of the series produced by CBS before 1976 or distributed by CBS films were later distributed by Viacom and Paramount Television ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Most pre- 1976 series produced by CBS or distributed by CBS Films were later distributed by Viacom and Paramount Television .\nOutput: ", "bloom_target": "Most of the series produced by CBS before 1976 or distributed by CBS films were later distributed by Viacom and Paramount Television .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Most pre- 1976 series produced by CBS or distributed by CBS Films were later distributed by Viacom and Paramount Television .\nOutput:  Most", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-de3ef751ce5b4c65b0bd7fb9d131da67", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-de3ef751ce5b4c65b0bd7fb9d131da67", "input": "It is located to the north of New Square and New Hempstead , east of Viola , south of Spring Valley and west of New City .", "output": ["It is located north of New Square and New Hempstead , east of Viola , south of Spring Valley , and west of New City ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: It is located to the north of New Square and New Hempstead , east of Viola , south of Spring Valley and west of New City .\nOutput: ", "bloom_target": "It is located north of New Square and New Hempstead , east of Viola , south of Spring Valley , and west of New City .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: It is located to the north of New Square and New Hempstead , east of Viola , south of Spring Valley and west of New City .\nOutput:  It", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-3fef9d08bf1e47269609eeb354444b3f", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-3fef9d08bf1e47269609eeb354444b3f", "input": "Dorothy Kate Richmond , Frances Hodgkins , and Gwen Knight was a contemporary of Stewart .", "output": ["Dorothy Kate Richmond , Frances Hodgkins and Gwen Knight was a Stewart contemporary ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Dorothy Kate Richmond , Frances Hodgkins , and Gwen Knight was a contemporary of Stewart .\nOutput: ", "bloom_target": "Dorothy Kate Richmond , Frances Hodgkins and Gwen Knight was a Stewart contemporary .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Dorothy Kate Richmond , Frances Hodgkins , and Gwen Knight was a contemporary of Stewart .\nOutput:  Dorothy", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-6921f6693b074db7a880769da30667ac", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-6921f6693b074db7a880769da30667ac", "input": "Chicago Public School is a DeWitt Clinton School on the north side of Chicago , Illinois .", "output": ["Chicago Public School is a DeWitt Clinton School on the northern side of Chicago , Illinois ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Chicago Public School is a DeWitt Clinton School on the north side of Chicago , Illinois .\nOutput: ", "bloom_target": "Chicago Public School is a DeWitt Clinton School on the northern side of Chicago , Illinois .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Chicago Public School is a DeWitt Clinton School on the north side of Chicago , Illinois .\nOutput:  Chicago", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-248cbf566b4f4176843e4b2973d15f25", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-248cbf566b4f4176843e4b2973d15f25", "input": "As a songwriter , Lars Ankerstjerne has written songs for Nik & Jay , Burhan G and Rasmus Seebach .", "output": ["Lars Ankerstjerne has written as songwriter for Nik 'Jay , Burhan G and Rasmus Seebach songs ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: As a songwriter , Lars Ankerstjerne has written songs for Nik & Jay , Burhan G and Rasmus Seebach .\nOutput: ", "bloom_target": "Lars Ankerstjerne has written as songwriter for Nik 'Jay , Burhan G and Rasmus Seebach songs .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: As a songwriter , Lars Ankerstjerne has written songs for Nik & Jay , Burhan G and Rasmus Seebach .\nOutput:  As", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-c198b7d643bc4d2ea6f6f9a2856e3f85", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-c198b7d643bc4d2ea6f6f9a2856e3f85", "input": "The North Downs Way crosses the Medway Viaduct at the eastern end of the Medway Valley Walk or motorway bridge .", "output": ["The North Downs Way crosses the Medway viaduct at the eastern end of Medway Valley Walk or the Autobahn bridge ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The North Downs Way crosses the Medway Viaduct at the eastern end of the Medway Valley Walk or motorway bridge .\nOutput: ", "bloom_target": "The North Downs Way crosses the Medway viaduct at the eastern end of Medway Valley Walk or the Autobahn bridge .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The North Downs Way crosses the Medway Viaduct at the eastern end of the Medway Valley Walk or motorway bridge .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-2d5cc5510040480eb665313997f0e92f", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-2d5cc5510040480eb665313997f0e92f", "input": "Below is the early version of the album with all the original segues and \" The Sacrifice of Victor \" is slightly longer in the early configuration .", "output": ["Below is the early version of the album with all the original segues . Also , \" The Sacrifice of Victor \" is slightly longer on the early configuration ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Below is the early version of the album with all the original segues and \" The Sacrifice of Victor \" is slightly longer in the early configuration .\nOutput: ", "bloom_target": "Below is the early version of the album with all the original segues . Also , \" The Sacrifice of Victor \" is slightly longer on the early configuration .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Below is the early version of the album with all the original segues and \" The Sacrifice of Victor \" is slightly longer in the early configuration .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-b1b9b0b8d8c34817b109461d998f3be0", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-b1b9b0b8d8c34817b109461d998f3be0", "input": "Of the twelve stories that are included , six were previously published in the author 's first collection , \" evening news \" .", "output": ["Of the twelve stories included , six were previously published in the author 's first collection , \" The Evening News \" ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Of the twelve stories that are included , six were previously published in the author 's first collection , \" evening news \" .\nOutput: ", "bloom_target": "Of the twelve stories included , six were previously published in the author 's first collection , \" The Evening News \" .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Of the twelve stories that are included , six were previously published in the author 's first collection , \" evening news \" .\nOutput:  Of", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-fc0ddd91526642edbf45d852ec3809f0", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-fc0ddd91526642edbf45d852ec3809f0", "input": "The branch codice 2 is updated daily , the branch codice 3 is updated every 6 months .", "output": ["The codice 2 branch is updated daily , and the branch codice 3 is updated every 6 months ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The branch codice 2 is updated daily , the branch codice 3 is updated every 6 months .\nOutput: ", "bloom_target": "The codice 2 branch is updated daily , and the branch codice 3 is updated every 6 months .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The branch codice 2 is updated daily , the branch codice 3 is updated every 6 months .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-e5f94d227a2f410b8ae24946725f1ae8", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-e5f94d227a2f410b8ae24946725f1ae8", "input": "She worked and lived in Stuttgart , Berlin ( Germany ) and in Vienna ( Austria ) .", "output": ["She worked and lived in Germany ( Stuttgart , Berlin ) and in Vienna ( Austria ) ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: She worked and lived in Stuttgart , Berlin ( Germany ) and in Vienna ( Austria ) .\nOutput: ", "bloom_target": "She worked and lived in Germany ( Stuttgart , Berlin ) and in Vienna ( Austria ) .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: She worked and lived in Stuttgart , Berlin ( Germany ) and in Vienna ( Austria ) .\nOutput:  She", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-7f91235d9f834f40aea302c03b60a675", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-7f91235d9f834f40aea302c03b60a675", "input": "He has previously played for Wolverhampton Wanderers , Kidderminster Harriers , Mansfield Town , Lincoln City , Northampton Town , Chesterfield and Gateshead .", "output": ["He formerly played for Wolverhampton Wanderers , Kidderminster Harriers , Mansfield Town , Chesterfield , Northampton Town , Lincoln City and Gateshead ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: He has previously played for Wolverhampton Wanderers , Kidderminster Harriers , Mansfield Town , Lincoln City , Northampton Town , Chesterfield and Gateshead .\nOutput: ", "bloom_target": "He formerly played for Wolverhampton Wanderers , Kidderminster Harriers , Mansfield Town , Chesterfield , Northampton Town , Lincoln City and Gateshead .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: He has previously played for Wolverhampton Wanderers , Kidderminster Harriers , Mansfield Town , Lincoln City , Northampton Town , Chesterfield and Gateshead .\nOutput:  He", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-489fd865283d43a0ad4c2ebd5d64f8e4", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-489fd865283d43a0ad4c2ebd5d64f8e4", "input": "Chris Egan ( Nick Smith ) settles down with his family in Summer Bay , and he and Duncan quickly become friends and get into various crises .", "output": ["Nick Smith ( Chris Egan ) settles in Summer Bay with his family and he and Duncan quickly become friends and get into various scrapes ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Chris Egan ( Nick Smith ) settles down with his family in Summer Bay , and he and Duncan quickly become friends and get into various crises .\nOutput: ", "bloom_target": "Nick Smith ( Chris Egan ) settles in Summer Bay with his family and he and Duncan quickly become friends and get into various scrapes .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Chris Egan ( Nick Smith ) settles down with his family in Summer Bay , and he and Duncan quickly become friends and get into various crises .\nOutput:  Chris", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-84dfba11a2d04511951cc58224038120", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-84dfba11a2d04511951cc58224038120", "input": "Bailey was replaced by Henry Cole as caretaker of Breakheart Hill .", "output": ["Bailey was succeeded as caretaker of Breakheart Hill by Henry Cole ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Bailey was replaced by Henry Cole as caretaker of Breakheart Hill .\nOutput: ", "bloom_target": "Bailey was succeeded as caretaker of Breakheart Hill by Henry Cole .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Bailey was replaced by Henry Cole as caretaker of Breakheart Hill .\nOutput:  Bailey", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-325379183b384b688531dd5f296f9b90", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-325379183b384b688531dd5f296f9b90", "input": "Based on the city of Baltimore , only mentioned , never visited in the show .", "output": ["Based on the city of Baltimore , only mentioned , has never visited in the show ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Based on the city of Baltimore , only mentioned , never visited in the show .\nOutput: ", "bloom_target": "Based on the city of Baltimore , only mentioned , has never visited in the show .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Based on the city of Baltimore , only mentioned , never visited in the show .\nOutput:  Based", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-59fdde37612a47728899798f140bae49", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-59fdde37612a47728899798f140bae49", "input": "It is covered with a natural vegetation of grassland of less fertile red clay bases , and Saltbush bushland on more fertile earths .", "output": ["It is covered with a natural vegetation of grassland of less fertile red clay soils , and saltbush shrubland on more fertile earths ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: It is covered with a natural vegetation of grassland of less fertile red clay bases , and Saltbush bushland on more fertile earths .\nOutput: ", "bloom_target": "It is covered with a natural vegetation of grassland of less fertile red clay soils , and saltbush shrubland on more fertile earths .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: It is covered with a natural vegetation of grassland of less fertile red clay bases , and Saltbush bushland on more fertile earths .\nOutput:  It", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-8bbfbf77b67e4fa9835f07643f82b5c9", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-8bbfbf77b67e4fa9835f07643f82b5c9", "input": "It located in Himachal Pradesh ( Tattapani ) , at the altitude of 650mts , perfect temperature for the healing treatments .", "output": ["It is located in Tattapani ( Himachal Pradesh ) , at an altitude of 650mts , the perfect temperature for the treatments ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: It located in Himachal Pradesh ( Tattapani ) , at the altitude of 650mts , perfect temperature for the healing treatments .\nOutput: ", "bloom_target": "It is located in Tattapani ( Himachal Pradesh ) , at an altitude of 650mts , the perfect temperature for the treatments .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: It located in Himachal Pradesh ( Tattapani ) , at the altitude of 650mts , perfect temperature for the healing treatments .\nOutput:  It", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-35695d85b811408da42172fae045da8e", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-35695d85b811408da42172fae045da8e", "input": "The tsunami was observed along the Pacific coast of Japan from the Izu peninsula to Kyushu and recorded by tides from Hawaii to Alaska .", "output": ["The tsunami was observed along the Pacific coast of Japan from Izu Peninsula to Kyushu , and recorded by tide gauges from Hawaii to Alaska ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The tsunami was observed along the Pacific coast of Japan from the Izu peninsula to Kyushu and recorded by tides from Hawaii to Alaska .\nOutput: ", "bloom_target": "The tsunami was observed along the Pacific coast of Japan from Izu Peninsula to Kyushu , and recorded by tide gauges from Hawaii to Alaska .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The tsunami was observed along the Pacific coast of Japan from the Izu peninsula to Kyushu and recorded by tides from Hawaii to Alaska .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-b738eb9836004e45bf19e18c37da287d", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-b738eb9836004e45bf19e18c37da287d", "input": "The bones of Zrinski and Frankopan were found in Austria in 1907 and brought to Zagreb in 1919 , where they were reburied in the Zagreb Cathedral .", "output": ["The bones of Zrinski and Frankopan were found in 1907 in Austria and brought to Zagreb in 1919 , where they were rebuilt in the Zagreb Cathedral ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The bones of Zrinski and Frankopan were found in Austria in 1907 and brought to Zagreb in 1919 , where they were reburied in the Zagreb Cathedral .\nOutput: ", "bloom_target": "The bones of Zrinski and Frankopan were found in 1907 in Austria and brought to Zagreb in 1919 , where they were rebuilt in the Zagreb Cathedral .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The bones of Zrinski and Frankopan were found in Austria in 1907 and brought to Zagreb in 1919 , where they were reburied in the Zagreb Cathedral .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-5683aeb617c743388f9ee8a3d8704281", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-5683aeb617c743388f9ee8a3d8704281", "input": "The light novels are written by Dojyomaru and are illustrated by Fuyuyuki and published by Overlap Bunko .", "output": ["The light novels are written by Dojyomaru and illustrated by Fuyuyuki , and are published by Overlap Bunko ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The light novels are written by Dojyomaru and are illustrated by Fuyuyuki and published by Overlap Bunko .\nOutput: ", "bloom_target": "The light novels are written by Dojyomaru and illustrated by Fuyuyuki , and are published by Overlap Bunko .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The light novels are written by Dojyomaru and are illustrated by Fuyuyuki and published by Overlap Bunko .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-db6ea381b9e948169f8abd73df3e2488", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-db6ea381b9e948169f8abd73df3e2488", "input": "Debbie Downer is a name of a fictional \" Saturday Night Live \" character that was debuted in 2004 and portrayed by Rachel Dratch .", "output": ["Debbie Downer is a name of a fictional \" Saturday Night Live \" character who debuted in 2004 , and who was portrayed by Rachel Dratch ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Debbie Downer is a name of a fictional \" Saturday Night Live \" character that was debuted in 2004 and portrayed by Rachel Dratch .\nOutput: ", "bloom_target": "Debbie Downer is a name of a fictional \" Saturday Night Live \" character who debuted in 2004 , and who was portrayed by Rachel Dratch .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Debbie Downer is a name of a fictional \" Saturday Night Live \" character that was debuted in 2004 and portrayed by Rachel Dratch .\nOutput:  Deb", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-44f14c04f1cc4fea93b45ac534ad9a6f", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-44f14c04f1cc4fea93b45ac534ad9a6f", "input": "He gained popularity also by replacing Archie Kao and Adam Rodriguez .", "output": ["He also gained popularity by replacing Archie Kao with Adam Rodriguez ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: He gained popularity also by replacing Archie Kao and Adam Rodriguez .\nOutput: ", "bloom_target": "He also gained popularity by replacing Archie Kao with Adam Rodriguez .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: He gained popularity also by replacing Archie Kao and Adam Rodriguez .\nOutput:  He", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-babb3dc958274ea880e1ea523519da62", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-babb3dc958274ea880e1ea523519da62", "input": "Jont\u00e9 Buhl ( born April 4 , 1982 ) is a former Canadian professional football player who played for four years at the Edmonton Eskimos of the Canadian Football League .", "output": ["Jont\u00e9 Buhl ( born April 4 , 1982 ) is a Canadian former professional football cornerback who played four seasons for the Edmonton Eskimos of the Canadian Football League ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Jont\u00e9 Buhl ( born April 4 , 1982 ) is a former Canadian professional football player who played for four years at the Edmonton Eskimos of the Canadian Football League .\nOutput: ", "bloom_target": "Jont\u00e9 Buhl ( born April 4 , 1982 ) is a Canadian former professional football cornerback who played four seasons for the Edmonton Eskimos of the Canadian Football League .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Jont\u00e9 Buhl ( born April 4 , 1982 ) is a former Canadian professional football player who played for four years at the Edmonton Eskimos of the Canadian Football League .\nOutput:  J", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-c0b06ad2990842e2a0277821cd282668", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-c0b06ad2990842e2a0277821cd282668", "input": "In December 2006 , Muspratt was named \" Chicagoan of the Year by John von Rhein and the staff of the \" Chicago Tribune \" in the Classic .", "output": ["In December 2006 , Muspratt was named \" Chicagoan of the Year \" in classical music by John von Rhein and the staff of the \" Chicago Tribune \" ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: In December 2006 , Muspratt was named \" Chicagoan of the Year by John von Rhein and the staff of the \" Chicago Tribune \" in the Classic .\nOutput: ", "bloom_target": "In December 2006 , Muspratt was named \" Chicagoan of the Year \" in classical music by John von Rhein and the staff of the \" Chicago Tribune \" .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: In December 2006 , Muspratt was named \" Chicagoan of the Year by John von Rhein and the staff of the \" Chicago Tribune \" in the Classic .\nOutput:  In", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-cff11252602c42b3ad4e738de85479c1", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-cff11252602c42b3ad4e738de85479c1", "input": "Other car manufacturers that have produced models with suicide doors include Citro\u00ebn , Lancia , Opel , Panhard , Rover , Saab , Saturn , Skoda and Volkswagen .", "output": ["Other car manufacturers which have produced models with suicide doors include Opel , Lancia , Citro\u00ebn , Panhard , Rover , Saab , Saturn , \u0160koda , and Volkswagen ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Other car manufacturers that have produced models with suicide doors include Citro\u00ebn , Lancia , Opel , Panhard , Rover , Saab , Saturn , Skoda and Volkswagen .\nOutput: ", "bloom_target": "Other car manufacturers which have produced models with suicide doors include Opel , Lancia , Citro\u00ebn , Panhard , Rover , Saab , Saturn , \u0160koda , and Volkswagen .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Other car manufacturers that have produced models with suicide doors include Citro\u00ebn , Lancia , Opel , Panhard , Rover , Saab , Saturn , Skoda and Volkswagen .\nOutput:  Other", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-8f4c7dbc65844f719d570abe5015ecfe", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-8f4c7dbc65844f719d570abe5015ecfe", "input": "He spent several more years in New York City as a computer consultant and software engineer , then moved to Atlanta in 1998 .", "output": ["He spent several years in New York City as a computer consultant and software engineer and moved to Atlanta in 1998 ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: He spent several more years in New York City as a computer consultant and software engineer , then moved to Atlanta in 1998 .\nOutput: ", "bloom_target": "He spent several years in New York City as a computer consultant and software engineer and moved to Atlanta in 1998 .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: He spent several more years in New York City as a computer consultant and software engineer , then moved to Atlanta in 1998 .\nOutput:  He", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-5b59c6f2bd664c91b8f4108f33f0cd4b", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-5b59c6f2bd664c91b8f4108f33f0cd4b", "input": "On 7 July 2011 , Russell was traded for Kris Russell on the Columbus Blue Jackets and joined his brother Michael Blunden with the Blue Jackets organization .", "output": ["On July 7 , 2011 , Russell was traded to the Columbus Blue Jackets for Kris Russell . He joined his brother Michael Blunden with the Blue Jackets organization ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: On 7 July 2011 , Russell was traded for Kris Russell on the Columbus Blue Jackets and joined his brother Michael Blunden with the Blue Jackets organization .\nOutput: ", "bloom_target": "On July 7 , 2011 , Russell was traded to the Columbus Blue Jackets for Kris Russell . He joined his brother Michael Blunden with the Blue Jackets organization .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: On 7 July 2011 , Russell was traded for Kris Russell on the Columbus Blue Jackets and joined his brother Michael Blunden with the Blue Jackets organization .\nOutput:  On", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-62291aab558f4900a088eabb8e6ef9d9", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-62291aab558f4900a088eabb8e6ef9d9", "input": "Chad Ochocinco ( born 1978 ; formerly Chad Johnson ) is an American football wide receiver .", "output": ["Chad Ochocinco ( born 1978 ; formerly Chad Johnson ) is an American - American - football receiver ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Chad Ochocinco ( born 1978 ; formerly Chad Johnson ) is an American football wide receiver .\nOutput: ", "bloom_target": "Chad Ochocinco ( born 1978 ; formerly Chad Johnson ) is an American - American - football receiver .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Chad Ochocinco ( born 1978 ; formerly Chad Johnson ) is an American football wide receiver .\nOutput:  Chad", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-62c9b28476484d2484246d21b0a660b8", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-62c9b28476484d2484246d21b0a660b8", "input": "Sussex and Lancing Colleges are recorded as having played a football match in November 1860 , the first by public schools in Brighton .", "output": ["Sussex and Lancing Colleges , which played a football match in November 1860 , are recorded as the first of public schools in Brighton ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Sussex and Lancing Colleges are recorded as having played a football match in November 1860 , the first by public schools in Brighton .\nOutput: ", "bloom_target": "Sussex and Lancing Colleges , which played a football match in November 1860 , are recorded as the first of public schools in Brighton .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Sussex and Lancing Colleges are recorded as having played a football match in November 1860 , the first by public schools in Brighton .\nOutput:  Sussex", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-cf978674d0504664beafc3825f9e1e08", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-cf978674d0504664beafc3825f9e1e08", "input": "Kathryn Lindskoog , an independent Lewis scholar , argued that Hooper 's scholarship is not reliable and that he has made false statements and attributed forged works to Lewis .", "output": ["Kathryn Lindskoog , an independent Lewis scholar , argued that Hooper 's scholarship is not reliable and that he has made false statements and attributed false works ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Kathryn Lindskoog , an independent Lewis scholar , argued that Hooper 's scholarship is not reliable and that he has made false statements and attributed forged works to Lewis .\nOutput: ", "bloom_target": "Kathryn Lindskoog , an independent Lewis scholar , argued that Hooper 's scholarship is not reliable and that he has made false statements and attributed false works .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Kathryn Lindskoog , an independent Lewis scholar , argued that Hooper 's scholarship is not reliable and that he has made false statements and attributed forged works to Lewis .\nOutput:  Kath", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-fbd3f5523abb427d92f4cb83d15d92b5", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-fbd3f5523abb427d92f4cb83d15d92b5", "input": "It has a high school ( \" Cornway Junior College \" ) and a nursery school ( \" Cornway Senior College \" ) .", "output": ["It has a high school ( \" Cornway Junior College \" ) and a preparatory school ( \" Cornway Senior College \" ) ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: It has a high school ( \" Cornway Junior College \" ) and a nursery school ( \" Cornway Senior College \" ) .\nOutput: ", "bloom_target": "It has a high school ( \" Cornway Junior College \" ) and a preparatory school ( \" Cornway Senior College \" ) .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: It has a high school ( \" Cornway Junior College \" ) and a nursery school ( \" Cornway Senior College \" ) .\nOutput:  It", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-37cdf6947bac4805927ed406525b3612", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-37cdf6947bac4805927ed406525b3612", "input": "The isodynamic conjugates of Fermat - points are the isogonal points and vice versa .", "output": ["The isodynamic conjugates of the Fermat points are the isogonal points and vice versa ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The isodynamic conjugates of Fermat - points are the isogonal points and vice versa .\nOutput: ", "bloom_target": "The isodynamic conjugates of the Fermat points are the isogonal points and vice versa .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The isodynamic conjugates of Fermat - points are the isogonal points and vice versa .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-cec36122f8c04ce99b5ba79ae6a30d0d", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-cec36122f8c04ce99b5ba79ae6a30d0d", "input": "In 1951 , he died and retired in 1956 .", "output": ["He died in 1951 and retired in 1956 ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: In 1951 , he died and retired in 1956 .\nOutput: ", "bloom_target": "He died in 1951 and retired in 1956 .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: In 1951 , he died and retired in 1956 .\nOutput:  In", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-e74d814a95eb4998a334781d86628c45", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-e74d814a95eb4998a334781d86628c45", "input": "Seb Janiak is a French photographer and video director of Polish origin .", "output": ["Seb Janiak is the French photographer and video director of Polish origin ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Seb Janiak is a French photographer and video director of Polish origin .\nOutput: ", "bloom_target": "Seb Janiak is the French photographer and video director of Polish origin .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Seb Janiak is a French photographer and video director of Polish origin .\nOutput:  Seb", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-577293331e7b465e81cb755e4a9e6fe2", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-577293331e7b465e81cb755e4a9e6fe2", "input": "The department of dramatic speaking and public arts was originally headed by T. Earl Pardoe .", "output": ["The Dramatic and Public Arts Department was originally headed by T. Earl Pardoe ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The department of dramatic speaking and public arts was originally headed by T. Earl Pardoe .\nOutput: ", "bloom_target": "The Dramatic and Public Arts Department was originally headed by T. Earl Pardoe .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The department of dramatic speaking and public arts was originally headed by T. Earl Pardoe .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-a220e5e6434b408997d1faa1608122a3", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-a220e5e6434b408997d1faa1608122a3", "input": "This was the last AFL Championship to end the season ; the first Super Bowl followed the 1966 season .", "output": ["This was the last AFL Championship to complete the season , the first Super Bowl followed in the 1966 season ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: This was the last AFL Championship to end the season ; the first Super Bowl followed the 1966 season .\nOutput: ", "bloom_target": "This was the last AFL Championship to complete the season , the first Super Bowl followed in the 1966 season .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: This was the last AFL Championship to end the season ; the first Super Bowl followed the 1966 season .\nOutput:  This", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-bdd98d7a66174a0f8209bc63263e3a65", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-bdd98d7a66174a0f8209bc63263e3a65", "input": "The river Oraciu or Orociu is a tributary of the River Pustnic in Romania .", "output": ["The Oraciu River or Orociu River is a tributary of the Pustnic River in Romania ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The river Oraciu or Orociu is a tributary of the River Pustnic in Romania .\nOutput: ", "bloom_target": "The Oraciu River or Orociu River is a tributary of the Pustnic River in Romania .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The river Oraciu or Orociu is a tributary of the River Pustnic in Romania .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-7c4d982253354d71be64412123d4736d", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-7c4d982253354d71be64412123d4736d", "input": "The complex of the Trabzon World Trade Center is close to Trabzon Airport .", "output": ["The complex of World Trade Center Trabzon is situated close to Trabzon Airport ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The complex of the Trabzon World Trade Center is close to Trabzon Airport .\nOutput: ", "bloom_target": "The complex of World Trade Center Trabzon is situated close to Trabzon Airport .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The complex of the Trabzon World Trade Center is close to Trabzon Airport .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-915154c582404157881adaeb41b47bfe", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-915154c582404157881adaeb41b47bfe", "input": "Smaller small cross circuit pylons may have two single arms on one side and one on the other .", "output": ["Smaller small cross pylons may have two single arms on one side and one on the other ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Smaller small cross circuit pylons may have two single arms on one side and one on the other .\nOutput: ", "bloom_target": "Smaller small cross pylons may have two single arms on one side and one on the other .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Smaller small cross circuit pylons may have two single arms on one side and one on the other .\nOutput:  Small", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-e2b1b70cf7a344d78977f7449cf3de0b", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-e2b1b70cf7a344d78977f7449cf3de0b", "input": "Togdheer ( Somali \" Wabi Togdheer \" ) is a seasonal river in the Togdheer River region in the eastern part of Somaliland .", "output": ["Togdheer ( Somali \" Wabi Togdheer \" ) is a seasonal river in the Togdheer River region of eastern Somaliland ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Togdheer ( Somali \" Wabi Togdheer \" ) is a seasonal river in the Togdheer River region in the eastern part of Somaliland .\nOutput: ", "bloom_target": "Togdheer ( Somali \" Wabi Togdheer \" ) is a seasonal river in the Togdheer River region of eastern Somaliland .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Togdheer ( Somali \" Wabi Togdheer \" ) is a seasonal river in the Togdheer River region in the eastern part of Somaliland .\nOutput:  T", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-e840af98aaa64ff588d411d1a60bcc44", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-e840af98aaa64ff588d411d1a60bcc44", "input": "St John married Elizabeth Crowley ( daughter of Ambrose Crowley ) of Greenwich on March 6 , 1725 . Their children were :", "output": ["St John married Elizabeth Crowley ( the daughter of Ambrose Crowley ) of Greenwich on 6 March 1725 . Their children included :"]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: St John married Elizabeth Crowley ( daughter of Ambrose Crowley ) of Greenwich on March 6 , 1725 . Their children were :\nOutput: ", "bloom_target": "St John married Elizabeth Crowley ( the daughter of Ambrose Crowley ) of Greenwich on 6 March 1725 . Their children included :", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: St John married Elizabeth Crowley ( daughter of Ambrose Crowley ) of Greenwich on March 6 , 1725 . Their children were :\nOutput:  St", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-fe24a3e86d644937acf99fe5f606dc38", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-fe24a3e86d644937acf99fe5f606dc38", "input": "The house was purchased in 1858 by Sir David Dundas of Dunira , who sold it to George Dewhurst from Lymm , Cheshire in 1864 .", "output": ["The house was purchased by Sir David Dundas of Dunira in 1858 , who sold it on to George Dewhurst of Lymm , Cheshire , in 1864 ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The house was purchased in 1858 by Sir David Dundas of Dunira , who sold it to George Dewhurst from Lymm , Cheshire in 1864 .\nOutput: ", "bloom_target": "The house was purchased by Sir David Dundas of Dunira in 1858 , who sold it on to George Dewhurst of Lymm , Cheshire , in 1864 .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The house was purchased in 1858 by Sir David Dundas of Dunira , who sold it to George Dewhurst from Lymm , Cheshire in 1864 .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-1e7df489116a43c196fdf30ecad0bbaa", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-1e7df489116a43c196fdf30ecad0bbaa", "input": "Quintus Caecilius Metellus Macedonicus was the second son of Roman politician and general Lucius Caecilius Metellus Diadematus .", "output": ["Quintus Caecilius Metellus Macedonicus was the second son of the Roman politician , General Lucius Caecilius Metellus Diadematus ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Quintus Caecilius Metellus Macedonicus was the second son of Roman politician and general Lucius Caecilius Metellus Diadematus .\nOutput: ", "bloom_target": "Quintus Caecilius Metellus Macedonicus was the second son of the Roman politician , General Lucius Caecilius Metellus Diadematus .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Quintus Caecilius Metellus Macedonicus was the second son of Roman politician and general Lucius Caecilius Metellus Diadematus .\nOutput:  Quint", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-b2d2ef0ef68244e1a63bd9f50090a1a2", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-b2d2ef0ef68244e1a63bd9f50090a1a2", "input": "\" Bonne Citoyenne \" had the misfortune to become damaged in a storm and to be separated from the rest of the French squadron .", "output": ["\" Bonne Citoyenne \" had the misfortune to be damaged in a storm and separated from the rest of the French squadron ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: \" Bonne Citoyenne \" had the misfortune to become damaged in a storm and to be separated from the rest of the French squadron .\nOutput: ", "bloom_target": "\" Bonne Citoyenne \" had the misfortune to be damaged in a storm and separated from the rest of the French squadron .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: \" Bonne Citoyenne \" had the misfortune to become damaged in a storm and to be separated from the rest of the French squadron .\nOutput:  \"", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-4344c38acd54482a89b6c8e5ae387f7d", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-4344c38acd54482a89b6c8e5ae387f7d", "input": "\" The Timber \" was released in North America on February 27 , 2015 , Well Go USA Entertainment released it in Germany on October 6 , 2015 .", "output": ["On February 27 , 2015 , \" The Timber \" was released in North America , Well Go USA Entertainment was released on October 6 , 2015 in Germany ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: \" The Timber \" was released in North America on February 27 , 2015 , Well Go USA Entertainment released it in Germany on October 6 , 2015 .\nOutput: ", "bloom_target": "On February 27 , 2015 , \" The Timber \" was released in North America , Well Go USA Entertainment was released on October 6 , 2015 in Germany .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: \" The Timber \" was released in North America on February 27 , 2015 , Well Go USA Entertainment released it in Germany on October 6 , 2015 .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-4323ef3109a242f382fbf3fb822c55ea", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-4323ef3109a242f382fbf3fb822c55ea", "input": "It is located in Shawnee Township and adjacent to Duchouquet Township in Allen County .", "output": ["It is located in Shawnee Township and is adjacent to Duchouquet Township in Allen County ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: It is located in Shawnee Township and adjacent to Duchouquet Township in Allen County .\nOutput: ", "bloom_target": "It is located in Shawnee Township and is adjacent to Duchouquet Township in Allen County .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: It is located in Shawnee Township and adjacent to Duchouquet Township in Allen County .\nOutput:  It", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-5e24e91533e24ed3a0dca7088cfc412c", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-5e24e91533e24ed3a0dca7088cfc412c", "input": "Ashley was born on 1 November 1986 and is a contemporary dancer from Los Angeles who originally grew up in Arizona .", "output": ["Born on November 1 , 1986 , Ashley is a contemporary dancer from Los Angeles who was originally raised in Arizona ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Ashley was born on 1 November 1986 and is a contemporary dancer from Los Angeles who originally grew up in Arizona .\nOutput: ", "bloom_target": "Born on November 1 , 1986 , Ashley is a contemporary dancer from Los Angeles who was originally raised in Arizona .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Ashley was born on 1 November 1986 and is a contemporary dancer from Los Angeles who originally grew up in Arizona .\nOutput:  Ashley", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-e509a7f7894e44ebb6ffbf54b1a93cb0", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-e509a7f7894e44ebb6ffbf54b1a93cb0", "input": "The Little Jocko River flows across the Saint Lawrence River and the Ottawa River to the Jocko River .", "output": ["The Little Jocko River flows via the Saint Lawrence River and the Ottawa River to the Jocko River ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The Little Jocko River flows across the Saint Lawrence River and the Ottawa River to the Jocko River .\nOutput: ", "bloom_target": "The Little Jocko River flows via the Saint Lawrence River and the Ottawa River to the Jocko River .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The Little Jocko River flows across the Saint Lawrence River and the Ottawa River to the Jocko River .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-a9860be17cd44eefb546a1ea83715b5b", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-a9860be17cd44eefb546a1ea83715b5b", "input": "After his dismissal , he moved from Germany to New Mexico and then to Los Angeles , then to San Francisco .", "output": ["After his discharge he moved from Germany to New Mexico and then to Los Angeles then to San Francisco ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: After his dismissal , he moved from Germany to New Mexico and then to Los Angeles , then to San Francisco .\nOutput: ", "bloom_target": "After his discharge he moved from Germany to New Mexico and then to Los Angeles then to San Francisco .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: After his dismissal , he moved from Germany to New Mexico and then to Los Angeles , then to San Francisco .\nOutput:  After", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-65fd7e5f66714bd6b809bed0c39b85ec", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-65fd7e5f66714bd6b809bed0c39b85ec", "input": "On 9 June 2017 , Barkchi appointed Mubin Ergashev as their manager after Vitaliy Levchenko joined the coaching staff of Krylia Sovetov .", "output": ["On 9 June 2017 , Barkchi Mubin Ergashev appointed her manager after Vitaliy Levchenko joined the Krylia Sovetov coaching staff ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: On 9 June 2017 , Barkchi appointed Mubin Ergashev as their manager after Vitaliy Levchenko joined the coaching staff of Krylia Sovetov .\nOutput: ", "bloom_target": "On 9 June 2017 , Barkchi Mubin Ergashev appointed her manager after Vitaliy Levchenko joined the Krylia Sovetov coaching staff .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: On 9 June 2017 , Barkchi appointed Mubin Ergashev as their manager after Vitaliy Levchenko joined the coaching staff of Krylia Sovetov .\nOutput:  On", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-c04a0bad540041d9907a3760d74dd250", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-c04a0bad540041d9907a3760d74dd250", "input": "This is a list of caves in the United Kingdom , including information about the largest and deepest caves in the UK .", "output": ["This is a list of caves in the United Kingdom , including information on the largest and deepest caves in the UK ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: This is a list of caves in the United Kingdom , including information about the largest and deepest caves in the UK .\nOutput: ", "bloom_target": "This is a list of caves in the United Kingdom , including information on the largest and deepest caves in the UK .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: This is a list of caves in the United Kingdom , including information about the largest and deepest caves in the UK .\nOutput:  This", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-c137880cfd4943548f49e3b038fa1d01", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-c137880cfd4943548f49e3b038fa1d01", "input": "The female lead role was played by Cortez in \" Ali Baba and the Sacred Crown \" , directed by Erminio Salvi .", "output": ["Cortez played the female lead in \" Ali Baba and the Sacred Crown \" , directed by Erminio Salvi ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The female lead role was played by Cortez in \" Ali Baba and the Sacred Crown \" , directed by Erminio Salvi .\nOutput: ", "bloom_target": "Cortez played the female lead in \" Ali Baba and the Sacred Crown \" , directed by Erminio Salvi .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The female lead role was played by Cortez in \" Ali Baba and the Sacred Crown \" , directed by Erminio Salvi .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-a8d838e5a2f14f5c95c16bd68b8b1437", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-a8d838e5a2f14f5c95c16bd68b8b1437", "input": "According to the United States Census Bureau , the city is a total surface area of which has land and , or 1.35 % , is water .", "output": ["According to the United States Census Bureau , the town has a total area of , of which is land and , or 1.35 % , is water ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: According to the United States Census Bureau , the city is a total surface area of which has land and , or 1.35 % , is water .\nOutput: ", "bloom_target": "According to the United States Census Bureau , the town has a total area of , of which is land and , or 1.35 % , is water .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: According to the United States Census Bureau , the city is a total surface area of which has land and , or 1.35 % , is water .\nOutput:  According", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-4e3c25560b0e4accb6caf137fd7bb7c3", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-4e3c25560b0e4accb6caf137fd7bb7c3", "input": "He graduated from the Galatasaray High School in Shumen in 1858 and became a teacher in Istanbul , where he remained until 1864 .", "output": ["In 1858 he graduated from the Galatasaray School in Schumen and became a teacher in Istanbul , where he remained until 1864 ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: He graduated from the Galatasaray High School in Shumen in 1858 and became a teacher in Istanbul , where he remained until 1864 .\nOutput: ", "bloom_target": "In 1858 he graduated from the Galatasaray School in Schumen and became a teacher in Istanbul , where he remained until 1864 .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: He graduated from the Galatasaray High School in Shumen in 1858 and became a teacher in Istanbul , where he remained until 1864 .\nOutput:  He", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-1638304d15b945aba69537497d7af766", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-1638304d15b945aba69537497d7af766", "input": "Werder 's forces invested Belfort and reached the city on 3 November .", "output": ["Werder 's troops invested Belfort and reached the city on November 3 ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Werder 's forces invested Belfort and reached the city on 3 November .\nOutput: ", "bloom_target": "Werder 's troops invested Belfort and reached the city on November 3 .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Werder 's forces invested Belfort and reached the city on 3 November .\nOutput:  Wer", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-c2f38acae2d044d795cd58b88ea4621f", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-c2f38acae2d044d795cd58b88ea4621f", "input": "( Don Wyatt had been in the penguins in 1956 , and both Eddie and Ray had been with Ray in the later Colts / Fortunes . )", "output": ["( Don Wyatt had been in the Penguins in 1956 and both Eddie and Ray had been in the later Colts/Fortunes with Ray . )"]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: ( Don Wyatt had been in the penguins in 1956 , and both Eddie and Ray had been with Ray in the later Colts / Fortunes . )\nOutput: ", "bloom_target": "( Don Wyatt had been in the Penguins in 1956 and both Eddie and Ray had been in the later Colts/Fortunes with Ray . )", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: ( Don Wyatt had been in the penguins in 1956 , and both Eddie and Ray had been with Ray in the later Colts / Fortunes . )\nOutput:  Don", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-d0fbf7f18a6243aebc5d1246aa25706e", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-d0fbf7f18a6243aebc5d1246aa25706e", "input": "Jie\u0163 is a tributary of the Slivei River in Romania .", "output": ["The Jie\u0163 is a tributary of the Slivei River in Romania ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Jie\u0163 is a tributary of the Slivei River in Romania .\nOutput: ", "bloom_target": "The Jie\u0163 is a tributary of the Slivei River in Romania .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Jie\u0163 is a tributary of the Slivei River in Romania .\nOutput:  J", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-fdc95dbca9a44717b657c915ce9c73a3", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-fdc95dbca9a44717b657c915ce9c73a3", "input": "The 2007 -- 08 Kansas State Wildcats Men 's Basketball Team represents Kansas State University at the 2007 -- 08 College - Basketball - Season .", "output": ["The 2007 -- 08 Kansas State Wildcats men 's basketball team represented Kansas State University in the 2007 -- 08 college basketball season ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The 2007 -- 08 Kansas State Wildcats Men 's Basketball Team represents Kansas State University at the 2007 -- 08 College - Basketball - Season .\nOutput: ", "bloom_target": "The 2007 -- 08 Kansas State Wildcats men 's basketball team represented Kansas State University in the 2007 -- 08 college basketball season .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The 2007 -- 08 Kansas State Wildcats Men 's Basketball Team represents Kansas State University at the 2007 -- 08 College - Basketball - Season .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-b10c10156d004d1aa7fee1a4cee8cec2", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-b10c10156d004d1aa7fee1a4cee8cec2", "input": "Earlier in 209 , Sun Quan Sun married Quan \u2019 s younger sister , Lady Sun , to strengthen the alliance between him and Liu Bei .", "output": ["Earlier in 209 , Sun Quan married Sun Quan 's younger sister Lady Sun to strengthen the alliance between him and Liu Bei ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Earlier in 209 , Sun Quan Sun married Quan \u2019 s younger sister , Lady Sun , to strengthen the alliance between him and Liu Bei .\nOutput: ", "bloom_target": "Earlier in 209 , Sun Quan married Sun Quan 's younger sister Lady Sun to strengthen the alliance between him and Liu Bei .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Earlier in 209 , Sun Quan Sun married Quan \u2019 s younger sister , Lady Sun , to strengthen the alliance between him and Liu Bei .\nOutput:  Ear", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-b9234a56b4264f1badc45fb1e603b620", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-b9234a56b4264f1badc45fb1e603b620", "input": "Contempo Magazine is a monthly print and daily online American magazine published in McAllen , Texas .", "output": ["Contempo Magazine is a monthly American print and online magazine in McAllen , Texas ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Contempo Magazine is a monthly print and daily online American magazine published in McAllen , Texas .\nOutput: ", "bloom_target": "Contempo Magazine is a monthly American print and online magazine in McAllen , Texas .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Contempo Magazine is a monthly print and daily online American magazine published in McAllen , Texas .\nOutput:  Cont", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-b27718272dea455789acea0b7fb3957a", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-b27718272dea455789acea0b7fb3957a", "input": "The city sits at the confluence of the Snake River with the great Weiser River , which marks the border with Oregon .", "output": ["The city lies at the confluence of the Snake River and the Great Weiser River , which marks the border with Oregon ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The city sits at the confluence of the Snake River with the great Weiser River , which marks the border with Oregon .\nOutput: ", "bloom_target": "The city lies at the confluence of the Snake River and the Great Weiser River , which marks the border with Oregon .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The city sits at the confluence of the Snake River with the great Weiser River , which marks the border with Oregon .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-ca4cd486d83c4b7986fb081536b2ddb8", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-ca4cd486d83c4b7986fb081536b2ddb8", "input": "The Argentine government remained neutral until the last days of war , but quietly tolerated the entry of Nazi leaders who fled Germany , Belgium , and Vichy - France in 1945 .", "output": ["The Argentine government remained neutral until the last days of war , but quietly tolerated the entry of Nazi leaders fleeing from Vichy , France , Belgium , and Germany in 1945 ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The Argentine government remained neutral until the last days of war , but quietly tolerated the entry of Nazi leaders who fled Germany , Belgium , and Vichy - France in 1945 .\nOutput: ", "bloom_target": "The Argentine government remained neutral until the last days of war , but quietly tolerated the entry of Nazi leaders fleeing from Vichy , France , Belgium , and Germany in 1945 .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The Argentine government remained neutral until the last days of war , but quietly tolerated the entry of Nazi leaders who fled Germany , Belgium , and Vichy - France in 1945 .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-1dd35bea533e49fcb19abe78c146b409", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-1dd35bea533e49fcb19abe78c146b409", "input": "He moved to Quebec in 1685 and lived for some time in New - France .", "output": ["He moved to Quebec around 1685 and lived in New France for some time ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: He moved to Quebec in 1685 and lived for some time in New - France .\nOutput: ", "bloom_target": "He moved to Quebec around 1685 and lived in New France for some time .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: He moved to Quebec in 1685 and lived for some time in New - France .\nOutput:  He", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-87dde3c3b7a44e84b8f54cfc055f66d3", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-87dde3c3b7a44e84b8f54cfc055f66d3", "input": "The friendship between him and Duncan ended at a club meeting in 1951 when the two disagreed at an annual meeting and Duncan reported that Greaves said :", "output": ["The friendship between him and Duncan ended in 1951 at a club meeting , when the two did not agree at an annual meeting , and Duncan reported that Greaves said :"]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The friendship between him and Duncan ended at a club meeting in 1951 when the two disagreed at an annual meeting and Duncan reported that Greaves said :\nOutput: ", "bloom_target": "The friendship between him and Duncan ended in 1951 at a club meeting , when the two did not agree at an annual meeting , and Duncan reported that Greaves said :", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The friendship between him and Duncan ended at a club meeting in 1951 when the two disagreed at an annual meeting and Duncan reported that Greaves said :\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-27df9ba59ecf4f32ac1a5797338e73c9", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-27df9ba59ecf4f32ac1a5797338e73c9", "input": "The film was filmed in Red Rock Canyon State Park ( California ) in Cantil , California .", "output": ["The film was shot in California ( Red Rock Canyon State Park ) in Cantil , California ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The film was filmed in Red Rock Canyon State Park ( California ) in Cantil , California .\nOutput: ", "bloom_target": "The film was shot in California ( Red Rock Canyon State Park ) in Cantil , California .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The film was filmed in Red Rock Canyon State Park ( California ) in Cantil , California .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-29949bcf98d04aad94cfc5b7677dbcc5", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-29949bcf98d04aad94cfc5b7677dbcc5", "input": "Hector Crawford was the brother of 3DB manager and administrator Curteis Crawford , and also brother to Dorothy Crawford .", "output": ["Hector Crawford was brother of 3DB manager and administrator Curteis Crawford , and also brother of Dorothy Crawford ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Hector Crawford was the brother of 3DB manager and administrator Curteis Crawford , and also brother to Dorothy Crawford .\nOutput: ", "bloom_target": "Hector Crawford was brother of 3DB manager and administrator Curteis Crawford , and also brother of Dorothy Crawford .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Hector Crawford was the brother of 3DB manager and administrator Curteis Crawford , and also brother to Dorothy Crawford .\nOutput:  Hector", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-7e17f4c7ed2846c3acbce29cc6ba871d", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-7e17f4c7ed2846c3acbce29cc6ba871d", "input": "He was a scholar in Metaphysical Literature , Theology and Classical sciences .", "output": ["He was a scholar in metaphysical literature , theology , and classical science ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: He was a scholar in Metaphysical Literature , Theology and Classical sciences .\nOutput: ", "bloom_target": "He was a scholar in metaphysical literature , theology , and classical science .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: He was a scholar in Metaphysical Literature , Theology and Classical sciences .\nOutput:  He", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-020cfd10c5564cca96d85a5849a74597", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-020cfd10c5564cca96d85a5849a74597", "input": "Although five issues of the series were printed , the project was finished without any of them being aborted .", "output": ["Although five issues of the series were printed , the project was finished without any of them being cancelled ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Although five issues of the series were printed , the project was finished without any of them being aborted .\nOutput: ", "bloom_target": "Although five issues of the series were printed , the project was finished without any of them being cancelled .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Although five issues of the series were printed , the project was finished without any of them being aborted .\nOutput:  Although", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-6da78b67f9e1495cb87940c9286f656f", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-6da78b67f9e1495cb87940c9286f656f", "input": "Gimnasia y Esgrima ( LP ) won 3 -- 2 and stayed in the Primera Divisi\u00f3n .", "output": ["Gimnasia y Esgrima ( LP ) won with 3 -- 2 and stayed in the Primera Divisi\u00f3n ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Gimnasia y Esgrima ( LP ) won 3 -- 2 and stayed in the Primera Divisi\u00f3n .\nOutput: ", "bloom_target": "Gimnasia y Esgrima ( LP ) won with 3 -- 2 and stayed in the Primera Divisi\u00f3n .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Gimnasia y Esgrima ( LP ) won 3 -- 2 and stayed in the Primera Divisi\u00f3n .\nOutput:  Gimnas", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-61b7593cdc6f4cb497c105bddd6154f6", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-61b7593cdc6f4cb497c105bddd6154f6", "input": "There were also 58 liaison aircraft but 20 of these were only used for messengers .", "output": ["There were also 58 connecting aircraft , but 20 of these were used for messengers only ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: There were also 58 liaison aircraft but 20 of these were only used for messengers .\nOutput: ", "bloom_target": "There were also 58 connecting aircraft , but 20 of these were used for messengers only .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: There were also 58 liaison aircraft but 20 of these were only used for messengers .\nOutput:  There", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-0247e716ac0944a0ae15e076f446a383", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-0247e716ac0944a0ae15e076f446a383", "input": "Owen believed his utopian community would create a \" social environment \" based on his ideals of superior social , intellectual and physical reform .", "output": ["Owen believed that his utopian community would create a \" social environment , based on his ideals of superior social , intellectual , and physical reform ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Owen believed his utopian community would create a \" social environment \" based on his ideals of superior social , intellectual and physical reform .\nOutput: ", "bloom_target": "Owen believed that his utopian community would create a \" social environment , based on his ideals of superior social , intellectual , and physical reform .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Owen believed his utopian community would create a \" social environment \" based on his ideals of superior social , intellectual and physical reform .\nOutput:  Owen", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-242c516a0dd347ceb1b6660734636762", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-242c516a0dd347ceb1b6660734636762", "input": "Alycia Moulton defeated Billie Jean King at 6 -- 0 , 7 -- 5 .", "output": ["Alycia Moulton defeated Billie Jean King 6 -- 0 , 7 -- 5 ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Alycia Moulton defeated Billie Jean King at 6 -- 0 , 7 -- 5 .\nOutput: ", "bloom_target": "Alycia Moulton defeated Billie Jean King 6 -- 0 , 7 -- 5 .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Alycia Moulton defeated Billie Jean King at 6 -- 0 , 7 -- 5 .\nOutput:  A", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-06776c3d84074e91be13da8ee06d5609", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-06776c3d84074e91be13da8ee06d5609", "input": "Hastings Ndlovu was buried with Hector Pieterson at Avalon Cemetery in Johannesburg .", "output": ["Hastings Ndlovu , together with Hector Pieterson , was buried at the Avalon cemetery in Johannesburg ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Hastings Ndlovu was buried with Hector Pieterson at Avalon Cemetery in Johannesburg .\nOutput: ", "bloom_target": "Hastings Ndlovu , together with Hector Pieterson , was buried at the Avalon cemetery in Johannesburg .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Hastings Ndlovu was buried with Hector Pieterson at Avalon Cemetery in Johannesburg .\nOutput:  Hector", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-fe2c07b7882c48f7b18cad92bf572773", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-fe2c07b7882c48f7b18cad92bf572773", "input": "In the following year , Butcher returned and was eliminated in round by Ian Rotten .", "output": ["Butcher returned the following year and was eliminated in round two by Ian Rotten ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: In the following year , Butcher returned and was eliminated in round by Ian Rotten .\nOutput: ", "bloom_target": "Butcher returned the following year and was eliminated in round two by Ian Rotten .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: In the following year , Butcher returned and was eliminated in round by Ian Rotten .\nOutput:  In", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-85e06fe0cb144f3fa8af99919806cbda", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-85e06fe0cb144f3fa8af99919806cbda", "input": "These geographically correspond to the traditional Glasgow District , South , Edinburgh District , and North and Midlands districts .", "output": ["These geographically roughly correspond to the traditional districts of Glasgow District , South , Edinburgh District and North and Midlands ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: These geographically correspond to the traditional Glasgow District , South , Edinburgh District , and North and Midlands districts .\nOutput: ", "bloom_target": "These geographically roughly correspond to the traditional districts of Glasgow District , South , Edinburgh District and North and Midlands .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: These geographically correspond to the traditional Glasgow District , South , Edinburgh District , and North and Midlands districts .\nOutput:  These", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-2bb7d1c4a4ea43c6bdc5e95af0611b6b", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-2bb7d1c4a4ea43c6bdc5e95af0611b6b", "input": "Laetitia Pujol was a shadowy presence as Le Homme ; Karl Paquette was his strong , melancholy double ; and Mathieu Ganio portrayed La Femme .", "output": ["Laetitia Pujol was a shady presence as Le Homme , Karl Paquette was his strong , melancholy double , and Mathieu Ganio portrayed La Femme ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Laetitia Pujol was a shadowy presence as Le Homme ; Karl Paquette was his strong , melancholy double ; and Mathieu Ganio portrayed La Femme .\nOutput: ", "bloom_target": "Laetitia Pujol was a shady presence as Le Homme , Karl Paquette was his strong , melancholy double , and Mathieu Ganio portrayed La Femme .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Laetitia Pujol was a shadowy presence as Le Homme ; Karl Paquette was his strong , melancholy double ; and Mathieu Ganio portrayed La Femme .\nOutput:  La", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-e27fa70c4f274da1b12dedee6fbb1e5e", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-e27fa70c4f274da1b12dedee6fbb1e5e", "input": "Here we view pseudo-differential operators as a generalization of differential operators .", "output": ["We consider pseudo-differential operators here as a generalization of differential operators ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Here we view pseudo-differential operators as a generalization of differential operators .\nOutput: ", "bloom_target": "We consider pseudo-differential operators here as a generalization of differential operators .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Here we view pseudo-differential operators as a generalization of differential operators .\nOutput:  Here", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-ef06963eec2d42fda4edbebff1261a14", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-ef06963eec2d42fda4edbebff1261a14", "input": "The SR 164 was commissioned from Youngstown to Salineville in 1923 .", "output": ["SR 164 was commissioned in 1923 , routed from Youngstown to Salineville ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The SR 164 was commissioned from Youngstown to Salineville in 1923 .\nOutput: ", "bloom_target": "SR 164 was commissioned in 1923 , routed from Youngstown to Salineville .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The SR 164 was commissioned from Youngstown to Salineville in 1923 .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-a36ba40d67fe444dbfa1de3a3c22f05c", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-a36ba40d67fe444dbfa1de3a3c22f05c", "input": "Briggs later met Briggs at the 1967 Monterey Pop Festival , where Ravi Shankar was also performing , with Eric Burdon and The Animals .", "output": ["Briggs met Briggs later at the Monterey Pop Festival of 1967 , where Ravi Shankar also performed with Eric Burdon and The Animals ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Briggs later met Briggs at the 1967 Monterey Pop Festival , where Ravi Shankar was also performing , with Eric Burdon and The Animals .\nOutput: ", "bloom_target": "Briggs met Briggs later at the Monterey Pop Festival of 1967 , where Ravi Shankar also performed with Eric Burdon and The Animals .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Briggs later met Briggs at the 1967 Monterey Pop Festival , where Ravi Shankar was also performing , with Eric Burdon and The Animals .\nOutput:  Brig", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-ac71dd869f0541f0a0af515a2ecb16e6", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-ac71dd869f0541f0a0af515a2ecb16e6", "input": "She plays for Naisten Liiga of \u00c5land United .", "output": ["She plays for Naisten Liiga of the \u00c5land United ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: She plays for Naisten Liiga of \u00c5land United .\nOutput: ", "bloom_target": "She plays for Naisten Liiga of the \u00c5land United .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: She plays for Naisten Liiga of \u00c5land United .\nOutput:  She", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-2aa25b012f3e4c31a6a02ecc893aed38", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-2aa25b012f3e4c31a6a02ecc893aed38", "input": "The Tweenies consist of Bella , Milo , Fizz , Jake , Doodles , Izzles , Max , Judy , and are sometimes joined by Max 's sister Polly .", "output": ["The tweenies consist of Bella , Milo , Fizz , Jake , Scribbles , Izzles , Max , Judy and are sometimes joined by Max 'apos ; Sister Polly ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The Tweenies consist of Bella , Milo , Fizz , Jake , Doodles , Izzles , Max , Judy , and are sometimes joined by Max 's sister Polly .\nOutput: ", "bloom_target": "The tweenies consist of Bella , Milo , Fizz , Jake , Scribbles , Izzles , Max , Judy and are sometimes joined by Max 'apos ; Sister Polly .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The Tweenies consist of Bella , Milo , Fizz , Jake , Doodles , Izzles , Max , Judy , and are sometimes joined by Max 's sister Polly .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-fd27ecf55a44409ba5f2406dd6f67a1f", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-fd27ecf55a44409ba5f2406dd6f67a1f", "input": "The company was then acquired the St. Louis and Cairo Railroad , the narrow gauge .", "output": ["The company then was the St. Louis and Cairo Railroad , which acquired narrow gauge ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The company was then acquired the St. Louis and Cairo Railroad , the narrow gauge .\nOutput: ", "bloom_target": "The company then was the St. Louis and Cairo Railroad , which acquired narrow gauge .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The company was then acquired the St. Louis and Cairo Railroad , the narrow gauge .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-2f5f86e361f744909abe59032d9347e5", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-2f5f86e361f744909abe59032d9347e5", "input": "In February 2016 , Souray married the former WWE - professional - wrestler Barbara Blank , better known as Kelly Kelly , who separated in October 2017 .", "output": ["Souray married former WWE professional wrestler Barbara Blank , better known as Kelly Kelly in February 2016 . They have separated in October 2017 ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: In February 2016 , Souray married the former WWE - professional - wrestler Barbara Blank , better known as Kelly Kelly , who separated in October 2017 .\nOutput: ", "bloom_target": "Souray married former WWE professional wrestler Barbara Blank , better known as Kelly Kelly in February 2016 . They have separated in October 2017 .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: In February 2016 , Souray married the former WWE - professional - wrestler Barbara Blank , better known as Kelly Kelly , who separated in October 2017 .\nOutput:  In", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-6543cc7f918c4d708289579b1af1b196", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-6543cc7f918c4d708289579b1af1b196", "input": "The leaves are generally 1.5-4 mm long and 0.2-0.7 mm wide .", "output": ["The leaves are usually 1.5-4 mm long and 0.2-0.7 mm wide ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The leaves are generally 1.5-4 mm long and 0.2-0.7 mm wide .\nOutput: ", "bloom_target": "The leaves are usually 1.5-4 mm long and 0.2-0.7 mm wide .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: The leaves are generally 1.5-4 mm long and 0.2-0.7 mm wide .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-bdfef1c900cf43fcabdff47c026595f0", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-bdfef1c900cf43fcabdff47c026595f0", "input": "Amish Mennonites of Swiss descent from Galicia settled near Dubno in 1815 .", "output": ["Amish Mennonites from Galicia with Swiss descent settled in 1815 near Dubno ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Amish Mennonites of Swiss descent from Galicia settled near Dubno in 1815 .\nOutput: ", "bloom_target": "Amish Mennonites from Galicia with Swiss descent settled in 1815 near Dubno .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Amish Mennonites of Swiss descent from Galicia settled near Dubno in 1815 .\nOutput:  Am", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-7a616c1bf93f414ca5f372433ff4dfc5", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-7a616c1bf93f414ca5f372433ff4dfc5", "input": "His parents are Don Luis Toranzos , himself a prominent artist , and Angelina Miers from Argentina .", "output": ["His parents are Don Luis Toranzos , a prominent artist himself , and Angelina Miers , of Argentina ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: His parents are Don Luis Toranzos , himself a prominent artist , and Angelina Miers from Argentina .\nOutput: ", "bloom_target": "His parents are Don Luis Toranzos , a prominent artist himself , and Angelina Miers , of Argentina .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: His parents are Don Luis Toranzos , himself a prominent artist , and Angelina Miers from Argentina .\nOutput:  His", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-ab82db2c2e404bf1831ca95bd2791795", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-ab82db2c2e404bf1831ca95bd2791795", "input": "Several animatronic characters were also created ... a giant goose ( Galaga ) and an animatronic head for the puppeteered Cernos .", "output": ["There were also several animatronic characters created ... a giant goose ( Galaga ) and an animatronic head for the doll 's cernos ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Several animatronic characters were also created ... a giant goose ( Galaga ) and an animatronic head for the puppeteered Cernos .\nOutput: ", "bloom_target": "There were also several animatronic characters created ... a giant goose ( Galaga ) and an animatronic head for the doll 's cernos .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Several animatronic characters were also created ... a giant goose ( Galaga ) and an animatronic head for the puppeteered Cernos .\nOutput:  Several", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-965fab80e0d44f4997160804e03f91cf", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-965fab80e0d44f4997160804e03f91cf", "input": "WORHP , also referred to as eNLP ( European NLP Solver ) , is a mathematical software library for numerically solving continuous , nonlinear optimization problems on a large scale .", "output": ["WORHP , also referred to as eNLP ( European NLP solver ) by ESA , is a mathematical software library for solving continuous large scale nonlinear optimization problems numerically ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: WORHP , also referred to as eNLP ( European NLP Solver ) , is a mathematical software library for numerically solving continuous , nonlinear optimization problems on a large scale .\nOutput: ", "bloom_target": "WORHP , also referred to as eNLP ( European NLP solver ) by ESA , is a mathematical software library for solving continuous large scale nonlinear optimization problems numerically .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: WORHP , also referred to as eNLP ( European NLP Solver ) , is a mathematical software library for numerically solving continuous , nonlinear optimization problems on a large scale .\nOutput:  W", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-7dcdf152cc8946d98c32faf27aa60b9a", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-7dcdf152cc8946d98c32faf27aa60b9a", "input": "During their relationship the pair lived in Los Angeles , though Seymour spent more time in London and Los Angeles for her work .", "output": ["Throughout her relationship , the couple lived in Los Angeles , though Seymour spent more time in London and Los Angeles for their work ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: During their relationship the pair lived in Los Angeles , though Seymour spent more time in London and Los Angeles for her work .\nOutput: ", "bloom_target": "Throughout her relationship , the couple lived in Los Angeles , though Seymour spent more time in London and Los Angeles for their work .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: During their relationship the pair lived in Los Angeles , though Seymour spent more time in London and Los Angeles for her work .\nOutput:  The", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-fa516f2aff3640c88730cdc83f5f97d9", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-fa516f2aff3640c88730cdc83f5f97d9", "input": "He won the first Prix de Rome for painting in 1813 and the second Prix de Rome in 1814 for his painting of the \" Death of Diagoras \" .", "output": ["He won the first Prix de Rome for painting in 1813 and in 1814 the second Prix de Rome for his paintings of the \" death of the Diagoras \" ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: He won the first Prix de Rome for painting in 1813 and the second Prix de Rome in 1814 for his painting of the \" Death of Diagoras \" .\nOutput: ", "bloom_target": "He won the first Prix de Rome for painting in 1813 and in 1814 the second Prix de Rome for his paintings of the \" death of the Diagoras \" .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: He won the first Prix de Rome for painting in 1813 and the second Prix de Rome in 1814 for his painting of the \" Death of Diagoras \" .\nOutput:  He", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-aa130dced7ac4ae586e7a0fb96ad1960", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-aa130dced7ac4ae586e7a0fb96ad1960", "input": "Brockton is approximately 25 miles northeast of Providence , Rhode Island , and 30 miles south of Boston .", "output": ["Brockton is located approximately 25 miles northeast of Providence , Rhode Island and 30 miles south of Boston ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Brockton is approximately 25 miles northeast of Providence , Rhode Island , and 30 miles south of Boston .\nOutput: ", "bloom_target": "Brockton is located approximately 25 miles northeast of Providence , Rhode Island and 30 miles south of Boston .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Brockton is approximately 25 miles northeast of Providence , Rhode Island , and 30 miles south of Boston .\nOutput:  Brock", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-d07d7a6e73ac41a999d433bfc764157e", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-d07d7a6e73ac41a999d433bfc764157e", "input": "Lemmings , by contrast , are conspicuously colored and behave aggressively towards predators and even human observers .", "output": ["By contrast , the lemmings are strikingly colored and behave aggressively towards predators and even human observers ."]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Lemmings , by contrast , are conspicuously colored and behave aggressively towards predators and even human observers .\nOutput: ", "bloom_target": "By contrast , the lemmings are strikingly colored and behave aggressively towards predators and even human observers .", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Lemmings , by contrast , are conspicuously colored and behave aggressively towards predators and even human observers .\nOutput:  Lem", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task770-bac9e7761542439aaba6ff76e53797b6", "Task": "task770_pawsx_english_text_modification", "Contributors": "['Jacob Anderson']", "Source": ["paws_x"], "URL": ["https://huggingface.co/datasets/paws-x"], "Categories": ["Paraphrasing"], "Reasoning": [], "Definition": ["Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning."], "Positive Examples": [{"input": "The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .", "output": "The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .", "explanation": "The paraphrase of the original sentence in English is correct and accurate because even though sentence order has been changed, the denotion that the 30th season of the NBA occurred in the years 1975 - 1976 is retained."}, {"input": "When comparable rates of flow can be maintained , the results are high .", "output": "The results are high when comparable flow rates can be maintained .", "explanation": "This is a proper paraphrase of the original sentence in English because the main idea that results are high only in the condition that comparable flow rates is maintained."}], "Negative Examples": [{"input": "In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .", "output": "In October 1560 , he secretly met with the English ambassador , Nicolas Throckmorton , in Paris , and asked him for a passport to return to Scotland through England .", "explanation": "The example is not considered a proper paraphrase of the original sentence in English because the through and destination locations are flipped - Scotland and England, respectively."}, {"input": "There are also specific discussions , public profile debates and project discussions .", "output": "There are also public discussions , profile specific discussions , and project discussions .", "explanation": "This example does not constitute a proper paraphrase of the original sentence in English because the descriptors of the nouns (\"specific\", \"public\") are swapped, and the sentence is missing any reference to debates as it has been replaced with discussions."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task770-bac9e7761542439aaba6ff76e53797b6", "input": "Akshuat dendropark ( Russian : \u0410\u043a\u0448\u0443\u0430\u0442\u0441\u043a\u0438\u0439 \u0434\u0435\u043d\u0434\u0440\u043e\u043f\u0430\u0440\u043a ) is a natural monument ( Ulyanovsk Oblast protected areas )", "output": ["Akshuat dendropark ( Russian : \u0410\u043a\u0448\u0443\u0430\u0442\u0441\u043a\u0438\u0439 \u0434\u0435\u043d\u0434\u0440\u043e\u043f\u0430\u0440\u043a ) is a natural monument ( Protected areas of Ulyanovsk Oblast )"]}, "Instance License": ["Dedicated License (https://github.com/google-research-datasets/paws/blob/master/LICENSE)"], "bloom_input": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Akshuat dendropark ( Russian : \u0410\u043a\u0448\u0443\u0430\u0442\u0441\u043a\u0438\u0439 \u0434\u0435\u043d\u0434\u0440\u043e\u043f\u0430\u0440\u043a ) is a natural monument ( Ulyanovsk Oblast protected areas )\nOutput: ", "bloom_target": "Akshuat dendropark ( Russian : \u0410\u043a\u0448\u0443\u0430\u0442\u0441\u043a\u0438\u0439 \u0434\u0435\u043d\u0434\u0440\u043e\u043f\u0430\u0440\u043a ) is a natural monument ( Protected areas of Ulyanovsk Oblast )", "bloom_response": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning.\n\n Positive Example 1 -\nInput: The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\n Output: The 1975 -- 76 season of the National Basketball Association was the 30th season of the NBA .\n\n Positive Example 2 -\nInput: When comparable rates of flow can be maintained , the results are high .\n Output: The results are high when comparable flow rates can be maintained .\n\nNow complete the following example -\nInput: Akshuat dendropark ( Russian : \u0410\u043a\u0448\u0443\u0430\u0442\u0441\u043a\u0438\u0439 \u0434\u0435\u043d\u0434\u0440\u043e\u043f\u0430\u0440\u043a ) is a natural monument ( Ulyanovsk Oblast protected areas )\nOutput:  Ak", "prediction": "Definition: Given a sentence in English, provide an equivalent paraphrased version from the original that retains the same meaning"}
{"id": "task1608-a883ba3b97974293ba3f0884e4ec4612", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-a883ba3b97974293ba3f0884e4ec4612", "input": "Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: Who did the Broncos defeat in the AFC Championship game?", "output": ["New England Patriots"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: Who did the Broncos defeat in the AFC Championship game?\nOutput: ", "bloom_target": "New England Patriots", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: Who did the Broncos defeat in the AFC Championship game?\nOutput:  The", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-25f2ac698e6e48699f8310741f55a930", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-25f2ac698e6e48699f8310741f55a930", "input": "Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: Who had five sacks in nine games as a Carolina Panthers starter?", "output": ["Kony Ealy"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: Who had five sacks in nine games as a Carolina Panthers starter?\nOutput: ", "bloom_target": "Kony Ealy", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: Who had five sacks in nine games as a Carolina Panthers starter?\nOutput:  Kurt", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-964bcad2af2f44fb8ba40e4839a0b7c0", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-964bcad2af2f44fb8ba40e4839a0b7c0", "input": "Passage: Their local rivals, Polonia Warsaw, have significantly fewer supporters, yet they managed to win Ekstraklasa Championship in 2000. They also won the country\u2019s championship in 1946, and won the cup twice as well. Polonia's home venue is located at Konwiktorska Street, a ten-minute walk north from the Old Town. Polonia was relegated from the country's top flight in 2013 because of their disastrous financial situation. They are now playing in the 4th league (5th tier in Poland) -the bottom professional league in the National \u2013 Polish Football Association (PZPN) structure. Question: Why was Polonia relegated from the country's top flight in 2013?", "output": ["disastrous financial situation"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Their local rivals, Polonia Warsaw, have significantly fewer supporters, yet they managed to win Ekstraklasa Championship in 2000. They also won the country\u2019s championship in 1946, and won the cup twice as well. Polonia's home venue is located at Konwiktorska Street, a ten-minute walk north from the Old Town. Polonia was relegated from the country's top flight in 2013 because of their disastrous financial situation. They are now playing in the 4th league (5th tier in Poland) -the bottom professional league in the National \u2013 Polish Football Association (PZPN) structure. Question: Why was Polonia relegated from the country's top flight in 2013?\nOutput: ", "bloom_target": "disastrous financial situation", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Their local rivals, Polonia Warsaw, have significantly fewer supporters, yet they managed to win Ekstraklasa Championship in 2000. They also won the country\u2019s championship in 1946, and won the cup twice as well. Polonia's home venue is located at Konwiktorska Street, a ten-minute walk north from the Old Town. Polonia was relegated from the country's top flight in 2013 because of their disastrous financial situation. They are now playing in the 4th league (5th tier in Poland) -the bottom professional league in the National \u2013 Polish Football Association (PZPN) structure. Question: Why was Polonia relegated from the country's top flight in 2013?\nOutput:  Polonia", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-9348f66b88334998a75e7b12dacad724", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-9348f66b88334998a75e7b12dacad724", "input": "Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: How many interceptions did Josh Norman score touchdowns with in 2015?", "output": ["two."]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: How many interceptions did Josh Norman score touchdowns with in 2015?\nOutput: ", "bloom_target": "two.", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: How many interceptions did Josh Norman score touchdowns with in 2015?\nOutput:  8", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-1ddca988627b42b6a548ef31377109f3", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-1ddca988627b42b6a548ef31377109f3", "input": "Passage: The two most prominent Norman families to arrive in the Mediterranean were descendants of Tancred of Hauteville and the Drengot family, of whom Rainulf Drengot received the county of Aversa, the first Norman toehold in the south, from Duke Sergius IV of Naples in 1030. The Hauteville family achieved princely rank by proclaiming prince Guaimar IV of Salerno 'Duke of Apulia and Calabria'. He promptly awarded their elected leader, William Iron Arm, with the title of count in his capital of Melfi. The Drengot family thereafter attained the principality of Capua, and emperor Henry III legally ennobled the Hauteville leader, Drogo, as 'dux et magister Italiae comesque Normannorum totius Apuliae et Calabriae' ('Duke and Master of Italy and Count of the Normans of all Apulia and Calabria') in 1047. Question: Who was Count of Melfi", "output": ["William Iron Arm"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The two most prominent Norman families to arrive in the Mediterranean were descendants of Tancred of Hauteville and the Drengot family, of whom Rainulf Drengot received the county of Aversa, the first Norman toehold in the south, from Duke Sergius IV of Naples in 1030. The Hauteville family achieved princely rank by proclaiming prince Guaimar IV of Salerno 'Duke of Apulia and Calabria'. He promptly awarded their elected leader, William Iron Arm, with the title of count in his capital of Melfi. The Drengot family thereafter attained the principality of Capua, and emperor Henry III legally ennobled the Hauteville leader, Drogo, as 'dux et magister Italiae comesque Normannorum totius Apuliae et Calabriae' ('Duke and Master of Italy and Count of the Normans of all Apulia and Calabria') in 1047. Question: Who was Count of Melfi.\nOutput: ", "bloom_target": "William Iron Arm", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The two most prominent Norman families to arrive in the Mediterranean were descendants of Tancred of Hauteville and the Drengot family, of whom Rainulf Drengot received the county of Aversa, the first Norman toehold in the south, from Duke Sergius IV of Naples in 1030. The Hauteville family achieved princely rank by proclaiming prince Guaimar IV of Salerno 'Duke of Apulia and Calabria'. He promptly awarded their elected leader, William Iron Arm, with the title of count in his capital of Melfi. The Drengot family thereafter attained the principality of Capua, and emperor Henry III legally ennobled the Hauteville leader, Drogo, as 'dux et magister Italiae comesque Normannorum totius Apuliae et Calabriae' ('Duke and Master of Italy and Count of the Normans of all Apulia and Calabria') in 1047. Question: Who was Count of Melfi.\nOutput:  William", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-50dc9b1fa4b44944995841c7c49ba70c", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-50dc9b1fa4b44944995841c7c49ba70c", "input": "Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: How many 2015 season interceptions did the Panthers' defense get?", "output": ["24"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: How many 2015 season interceptions did the Panthers' defense get?\nOutput: ", "bloom_target": "24", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: How many 2015 season interceptions did the Panthers' defense get?\nOutput:  308", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-172201d9c5a54425af038cab558615d9", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-172201d9c5a54425af038cab558615d9", "input": "Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: How many points did the Panthers defense surrender?", "output": ["308"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: How many points did the Panthers defense surrender?\nOutput: ", "bloom_target": "308", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: How many points did the Panthers defense surrender?\nOutput:  308", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-0d04fb3ab2f44889b79f448f80c800fa", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-0d04fb3ab2f44889b79f448f80c800fa", "input": "Passage: Their local rivals, Polonia Warsaw, have significantly fewer supporters, yet they managed to win Ekstraklasa Championship in 2000. They also won the country\u2019s championship in 1946, and won the cup twice as well. Polonia's home venue is located at Konwiktorska Street, a ten-minute walk north from the Old Town. Polonia was relegated from the country's top flight in 2013 because of their disastrous financial situation. They are now playing in the 4th league (5th tier in Poland) -the bottom professional league in the National \u2013 Polish Football Association (PZPN) structure. Question: When did Polonia Warsaw win the country's championship prior to 2000?", "output": ["1946"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Their local rivals, Polonia Warsaw, have significantly fewer supporters, yet they managed to win Ekstraklasa Championship in 2000. They also won the country\u2019s championship in 1946, and won the cup twice as well. Polonia's home venue is located at Konwiktorska Street, a ten-minute walk north from the Old Town. Polonia was relegated from the country's top flight in 2013 because of their disastrous financial situation. They are now playing in the 4th league (5th tier in Poland) -the bottom professional league in the National \u2013 Polish Football Association (PZPN) structure. Question: When did Polonia Warsaw win the country's championship prior to 2000?\nOutput: ", "bloom_target": "1946", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Their local rivals, Polonia Warsaw, have significantly fewer supporters, yet they managed to win Ekstraklasa Championship in 2000. They also won the country\u2019s championship in 1946, and won the cup twice as well. Polonia's home venue is located at Konwiktorska Street, a ten-minute walk north from the Old Town. Polonia was relegated from the country's top flight in 2013 because of their disastrous financial situation. They are now playing in the 4th league (5th tier in Poland) -the bottom professional league in the National \u2013 Polish Football Association (PZPN) structure. Question: When did Polonia Warsaw win the country's championship prior to 2000?\nOutput:  Poland", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-7e4846a3aa0f4d3eb3a1c6ec46d2756a", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-7e4846a3aa0f4d3eb3a1c6ec46d2756a", "input": "Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: What team was the winner of Super Bowl XXXIII?", "output": ["Broncos"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: What team was the winner of Super Bowl XXXIII?\nOutput: ", "bloom_target": "Broncos", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: What team was the winner of Super Bowl XXXIII?\nOutput:  Denver", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-787ba3df0c054a4d92025bf7d319b012", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-787ba3df0c054a4d92025bf7d319b012", "input": "Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: How many points did the Broncos score in the final three minutes of the Pittsburgh game?", "output": ["11"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: How many points did the Broncos score in the final three minutes of the Pittsburgh game?\nOutput: ", "bloom_target": "11", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: How many points did the Broncos score in the final three minutes of the Pittsburgh game?\nOutput:  11", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-58e12f7499b543028f58ef375347bd6a", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-58e12f7499b543028f58ef375347bd6a", "input": "Passage: Warsaw's first stock exchange was established in 1817 and continued trading until World War II. It was re-established in April 1991, following the end of the post-war communist control of the country and the reintroduction of a free-market economy. Today, the Warsaw Stock Exchange (WSE) is, according to many indicators, the largest market in the region, with 374 companies listed and total capitalization of 162 584 mln EUR as of 31 August 2009. From 1991 until 2000, the stock exchange was, ironically, located in the building previously used as the headquarters of the Polish United Workers' Party (PZPR). Question: Whose former headquarters was the WSE located in until 2000?", "output": ["Polish United Workers' Party"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Warsaw's first stock exchange was established in 1817 and continued trading until World War II. It was re-established in April 1991, following the end of the post-war communist control of the country and the reintroduction of a free-market economy. Today, the Warsaw Stock Exchange (WSE) is, according to many indicators, the largest market in the region, with 374 companies listed and total capitalization of 162 584 mln EUR as of 31 August 2009. From 1991 until 2000, the stock exchange was, ironically, located in the building previously used as the headquarters of the Polish United Workers' Party (PZPR). Question: Whose former headquarters was the WSE located in until 2000?\nOutput: ", "bloom_target": "Polish United Workers' Party", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Warsaw's first stock exchange was established in 1817 and continued trading until World War II. It was re-established in April 1991, following the end of the post-war communist control of the country and the reintroduction of a free-market economy. Today, the Warsaw Stock Exchange (WSE) is, according to many indicators, the largest market in the region, with 374 companies listed and total capitalization of 162 584 mln EUR as of 31 August 2009. From 1991 until 2000, the stock exchange was, ironically, located in the building previously used as the headquarters of the Polish United Workers' Party (PZPR). Question: Whose former headquarters was the WSE located in until 2000?\nOutput:  The", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-f4754ab972a54741b14747f566c99205", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-f4754ab972a54741b14747f566c99205", "input": "Passage: Six-time Grammy winner and Academy Award nominee Lady Gaga performed the national anthem, while Academy Award winner Marlee Matlin provided American Sign Language (ASL) translation. Question: What actor did sign language for the National Anthem at Superbowl 50?", "output": ["Marlee Matlin"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Six-time Grammy winner and Academy Award nominee Lady Gaga performed the national anthem, while Academy Award winner Marlee Matlin provided American Sign Language (ASL) translation. Question: What actor did sign language for the National Anthem at Superbowl 50?\nOutput: ", "bloom_target": "Marlee Matlin", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Six-time Grammy winner and Academy Award nominee Lady Gaga performed the national anthem, while Academy Award winner Marlee Matlin provided American Sign Language (ASL) translation. Question: What actor did sign language for the National Anthem at Superbowl 50?\nOutput:  Lady", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-88d4f6c290b445a49b7ab1a678163e31", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-88d4f6c290b445a49b7ab1a678163e31", "input": "Passage: Six-time Grammy winner and Academy Award nominee Lady Gaga performed the national anthem, while Academy Award winner Marlee Matlin provided American Sign Language (ASL) translation. Question: How many Grammy Award's does the text say Lady Gaga has won?", "output": ["Six"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Six-time Grammy winner and Academy Award nominee Lady Gaga performed the national anthem, while Academy Award winner Marlee Matlin provided American Sign Language (ASL) translation. Question: How many Grammy Award's does the text say Lady Gaga has won?\nOutput: ", "bloom_target": "Six", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Six-time Grammy winner and Academy Award nominee Lady Gaga performed the national anthem, while Academy Award winner Marlee Matlin provided American Sign Language (ASL) translation. Question: How many Grammy Award's does the text say Lady Gaga has won?\nOutput:  6", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-bf93239afd2f46ce8fb16ec11d86c3a6", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-bf93239afd2f46ce8fb16ec11d86c3a6", "input": "Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: Who did John Elway play for in Super Bowl XXXIII?", "output": ["Broncos"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: Who did John Elway play for in Super Bowl XXXIII?\nOutput: ", "bloom_target": "Broncos", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: Who did John Elway play for in Super Bowl XXXIII?\nOutput:  Pey", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-dbb0ca0ec5e24d07b7e7744fc4906092", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-dbb0ca0ec5e24d07b7e7744fc4906092", "input": "Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: What was the time on the clock when Carolina got the ball to their 24-yard line in the fourth quarter?", "output": ["4:51"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: What was the time on the clock when Carolina got the ball to their 24-yard line in the fourth quarter?\nOutput: ", "bloom_target": "4:51", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: What was the time on the clock when Carolina got the ball to their 24-yard line in the fourth quarter?\nOutput:  4:", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-202f7062b1c146ecac6b05162e3f9c6c", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-202f7062b1c146ecac6b05162e3f9c6c", "input": "Passage: Nearby, in Ogr\u00f3d Saski (the Saxon Garden), the Summer Theatre was in operation from 1870 to 1939, and in the inter-war period, the theatre complex also included Momus, Warsaw's first literary cabaret, and Leon Schiller's musical theatre Melodram. The Wojciech Bogus\u0142awski Theatre (1922\u201326), was the best example of 'Polish monumental theatre'. From the mid-1930s, the Great Theatre building housed the Upati Institute of Dramatic Arts \u2013 the first state-run academy of dramatic art, with an acting department and a stage directing department. Question: How long was the Summer Theatre in operation?", "output": ["1870 to 1939"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Nearby, in Ogr\u00f3d Saski (the Saxon Garden), the Summer Theatre was in operation from 1870 to 1939, and in the inter-war period, the theatre complex also included Momus, Warsaw's first literary cabaret, and Leon Schiller's musical theatre Melodram. The Wojciech Bogus\u0142awski Theatre (1922\u201326), was the best example of 'Polish monumental theatre'. From the mid-1930s, the Great Theatre building housed the Upati Institute of Dramatic Arts \u2013 the first state-run academy of dramatic art, with an acting department and a stage directing department. Question: How long was the Summer Theatre in operation?\nOutput: ", "bloom_target": "1870 to 1939", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Nearby, in Ogr\u00f3d Saski (the Saxon Garden), the Summer Theatre was in operation from 1870 to 1939, and in the inter-war period, the theatre complex also included Momus, Warsaw's first literary cabaret, and Leon Schiller's musical theatre Melodram. The Wojciech Bogus\u0142awski Theatre (1922\u201326), was the best example of 'Polish monumental theatre'. From the mid-1930s, the Great Theatre building housed the Upati Institute of Dramatic Arts \u2013 the first state-run academy of dramatic art, with an acting department and a stage directing department. Question: How long was the Summer Theatre in operation?\nOutput:  Near", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-b81d7c57297c44e082ed90be0c5eb27f", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-b81d7c57297c44e082ed90be0c5eb27f", "input": "Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: How old was Elway during his Super Bowl XXXIII win?", "output": ["38"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: How old was Elway during his Super Bowl XXXIII win?\nOutput: ", "bloom_target": "38", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: How old was Elway during his Super Bowl XXXIII win?\nOutput:  Pey", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-5c65682bbe544eb19bfd10ac2580d7d5", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-5c65682bbe544eb19bfd10ac2580d7d5", "input": "Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: Who did the Broncos beat in the divisional game?", "output": ["Pittsburgh Steelers"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: Who did the Broncos beat in the divisional game?\nOutput: ", "bloom_target": "Pittsburgh Steelers", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Broncos defeated the Pittsburgh Steelers in the divisional round, 23\u201316, by scoring 11 points in the final three minutes of the game. They then beat the defending Super Bowl XLIX champion New England Patriots in the AFC Championship Game, 20\u201318, by intercepting a pass on New England's 2-point conversion attempt with 17 seconds left on the clock. Despite Manning's problems with interceptions during the season, he didn't throw any in their two playoff games. Question: Who did the Broncos beat in the divisional game?\nOutput:  The", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-a3918a74f5cf41168bcde5579b5951e0", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-a3918a74f5cf41168bcde5579b5951e0", "input": "Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: How old was Manning when he played Super Bowl 50?", "output": ["39"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: How old was Manning when he played Super Bowl 50?\nOutput: ", "bloom_target": "39", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver's Executive Vice President of Football Operations and General Manager. Question: How old was Manning when he played Super Bowl 50?\nOutput:  Pey", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-1794f6914a1e4f278d18e5455e4c08bc", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-1794f6914a1e4f278d18e5455e4c08bc", "input": "Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: What player punched the ball in from the 2?", "output": ["Anderson"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: What player punched the ball in from the 2?\nOutput: ", "bloom_target": "Anderson", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: With 4:51 left in regulation, Carolina got the ball on their own 24-yard line with a chance to mount a game-winning drive, and soon faced 3rd-and-9. On the next play, Miller stripped the ball away from Newton, and after several players dove for it, it took a long bounce backwards and was recovered by Ward, who returned it five yards to the Panthers 4-yard line. Although several players dove into the pile to attempt to recover it, Newton did not and his lack of aggression later earned him heavy criticism. Meanwhile, Denver's offense was kept out of the end zone for three plays, but a holding penalty on cornerback Josh Norman gave the Broncos a new set of downs. Then Anderson scored on a 2-yard touchdown run and Manning completed a pass to Bennie Fowler for a 2-point conversion, giving Denver a 24\u201310 lead with 3:08 left and essentially putting the game away. Carolina had two more drives, but failed to get a first down on each one. Question: What player punched the ball in from the 2?\nOutput:  Anderson", "prediction": "Definition: Answer the question from the given passage"}
{"id": "task1608-ab7e9813710d4588b3253fe5406fa17f", "Task": "task1608_xquad_en_answer_generation", "Contributors": "['Srija Macherla']", "Source": ["xquad"], "URL": ["https://huggingface.co/datasets/viewer/?dataset=xquad"], "Categories": ["Question Answering"], "Reasoning": [], "Definition": ["Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence."], "Positive Examples": [{"input": "Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?", "output": "the Lord", "explanation": "The paragraph clearly states that, Luther has credited the Lord for his union with Katharina, hence the Lord is correct answer."}, {"input": "Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?", "output": "Sunset", "explanation": "The task results Sunset as answer when asked for the time of the day the sky meets solar winds, which is stated in the paragraph. Hence, it is correctly answered."}, {"input": "Passage: On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before\u2014the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. Question: When did Apollo 11 spacecraft launch?", "output": "July 16, 1969", "explanation": "The paragraphs states that Apollo 11 was launched on July 16 1969 which has been answered right, hence it is a good example."}], "Negative Examples": [{"input": "Passage: Oceans and lakes have much in common, but they are also quite different. Both are bodies of water, but oceans are very large bodies of salt water, while lakes are much smaller bodies of fresh water. Question: What are large bodies of water?", "output": "lakes", "explanation": "The answer that is expected is Ocean according to the paragraph given but lakes is returned, hence this is a bad example."}, {"input": "Passage: First, move all of the items that do not have a proper place to the center of the room. Get rid of at least five things that you have not used within the last year. Take out all of the trash, and place all of the dirty dishes in the kitchen sink. Question: Where to place all the dirty dishes?", "output": "trash", "explanation": "The correct answer would be Kitchen Sink as per the context but trash is answered, therefore, this is a negative example."}], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Wikipedia"], "Instance": {"id": "task1608-ab7e9813710d4588b3253fe5406fa17f", "input": "Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: How many interceptions are the Panthers defense credited with in 2015?", "output": ["24"]}, "Instance License": ["CC BY-SA 4.0"], "bloom_input": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: How many interceptions are the Panthers defense credited with in 2015?\nOutput: ", "bloom_target": "24", "bloom_response": "Definition: Answer the question from the given passage. Your answer should be directly extracted from the passage, and it should be a single entity, name, or number, not a sentence.\n\n Positive Example 1 -\nInput: Passage: Martin Luther married Katharina von Bora, one of 12 nuns he had helped escape from the Nimbschen Cistercian convent in April 1523, when he arranged for them to be smuggled out in herring barrels. Suddenly, and while I was occupied with far different thoughts, he wrote to Wenceslaus Link, \u201cthe Lord has plunged me into marriage. At the time of their marriage, Katharina was 26 years old and Luther was 41 years old. Question: In a letter who did Luther credit for his union with Katharina?\n Output: the Lord.\n\n Positive Example 2 -\nInput: Passage: Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. Question: What time of the day does our sky meet outer space solar winds?\n Output: Sunset.\n\nNow complete the following example -\nInput: Passage: The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6\u00bd sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5\u00bd sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns. Question: How many interceptions are the Panthers defense credited with in 2015?\nOutput:  308", "prediction": "Definition: Answer the question from the given passage"}
