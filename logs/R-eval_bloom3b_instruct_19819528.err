Loaded module: cuda/12.1
  0%|          | 0/11810 [00:00<?, ?it/s]/dtu/p1/johlau/LMOps/minillm/transformers/src/transformers/tokenization_utils_base.py:3862: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/dtu/p1/johlau/LMOps/minillm/transformers/src/transformers/generation/utils.py:1365: UserWarning: Input length of input_ids is 959, but `max_length` is set to 128. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.
  warnings.warn(
  0%|          | 0/11810 [01:24<?, ?it/s]
Traceback (most recent call last):
  File "/dtu/p1/johlau/Tk-Instruct/src/run_bloom.py", line 80, in <module>
    example["bloom_input"] = tokenizer.decode(example["bloom_input"])
  File "/dtu/p1/johlau/LMOps/minillm/transformers/src/transformers/tokenization_utils_base.py", line 3752, in decode
    return self._decode(
  File "/dtu/p1/johlau/LMOps/minillm/transformers/src/transformers/tokenization_utils_fast.py", line 625, in _decode
    text = self._tokenizer.decode(token_ids, skip_special_tokens=skip_special_tokens)
TypeError: argument 'ids': 'list' object cannot be interpreted as an integer
Traceback (most recent call last):
  File "/dtu/p1/johlau/Tk-Instruct/src/compute_metrics.py", line 140, in <module>
    results = compute_metrics(predictions, references, xlingual=args.track == "xlingual")
  File "/dtu/p1/johlau/Tk-Instruct/src/compute_metrics.py", line 90, in compute_metrics
    exact_match = 100.0 * exact_match / len(references)
ZeroDivisionError: float division by zero
/dtu/p1/johlau/Tk-Instruct/scripts/run_bloom.sh: line 23: syntax error near unexpected token `done'
/dtu/p1/johlau/Tk-Instruct/scripts/run_bloom.sh: line 23: `done'
