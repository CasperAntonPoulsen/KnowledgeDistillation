Mon Dec 18 13:04:58 2023       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 PCIe               On  | 00000000:A1:00.0 Off |                    0 |
| N/A   25C    P0              45W / 350W |      4MiB / 81559MiB |      0%   E. Process |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
python3 -m torch.distributed.run --nproc_per_node 1 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 3002 /dtu/p1/johlau/LMOps/minillm/train_minillm.py --base-path /dtu/p1/johlau/LMOps/minillm --model-path bigscience/bloom-3b --teacher-model-path bigscience/bloom-7b1 --ckpt-name bloom-3b --teacher-ckpt-name bloom-7b1 --n-gpu 1 --n-nodes 1 --model-type bloom --teacher-model-fp16 --gradient-checkpointing --prompt-data-dir /dtu/p1/johlau/LMOps/minillm/processed_data/dolly_en/prompt/bloom/ --lm-data-dir /dtu/p1/johlau/LMOps/minillm/processed_data/roberta_en/bloom/256/20M/ --dev-num 1000 --num-workers 0 --epochs 10 --total-iters 5000 --kd-ratio 0.5 --batch-size 2 --eval-batch-size 4 --lr 5e-6 --lr-min 5e-6 --gradient-accumulation-steps 2 --max-length 256 --max-prompt-length 128 --warmup-iters 100 --scheduler-name cosine_trm --save /dtu/p1/johlau/LMOps/minillm/results/bloom/train/minillm_en/ --seed 10 --seed-ppo 42 --seed-lm 7 --save-interval 500 --eval-interval 100 --log-interval 16 --mid-log-num 1 --type minillm --ppo-epochs 4 --num-rollouts 32 --chunk-size 2 --length-norm --single-step-reg --teacher-mixed-alpha 0.2 --reward-scaling 0.5 --cliprange-reward 100 --do-sample --top-k 0 --top-p 1.0 --temperature 1.0 --deepspeed --deepspeed_config /dtu/p1/johlau/LMOps/minillm/configs/deepspeed/ds_config_zero2_offload.json /dtu/p1/johlau/LMOps/minillm 3002
PYTHONPATH=/dtu/p1/johlau/LMOps/minillm
[2023-12-18 13:05:50,464] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 1
[2023-12-18 13:06:12,060] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-12-18 13:06:12,060] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
arguments:
  model_path ................... bigscience/bloom-3b
  ckpt_name .................... bloom-3b
  model_type ................... bloom
  teacher_model_type ........... None
  n_gpu ........................ 1
  n_nodes ...................... 1
  teacher_model_path ........... bigscience/bloom-7b1
  teacher_ckpt_name ............ bloom-7b1
  teacher_model_fp16 ........... True
  model_parallel ............... False
  model_parallel_size .......... None
  no_value ..................... False
  dropout_path_rate ............ None
  fp32 ......................... False
  type ......................... minillm
  do_train ..................... False
  do_valid ..................... False
  do_eval ...................... False
  base_path .................... /dtu/p1/johlau/LMOps/minillm
  load ......................... None
  save ......................... /dtu/p1/johlau/LMOps/minillm/results/bloom/train/minillm_en/bs2-lr5e-06-G2-N1-NN1-lm1-len256/pe4_rs0.5_nr32_ln_sr_tm0.2
  log_interval ................. 16
  mid_log_num .................. 1
  save_interval ................ 500
  eval_interval ................ 100
  local_rank ................... 0
  save_additional_suffix ....... 
  save_rollout ................. False
  eb_sample_times .............. 3
  data_dir ..................... None
  processed_data_dir ........... None
  force_process ................ False
  force_process_demo ........... False
  data_process_workers ......... -1
  train_num .................... -1
  train_ratio .................. 1
  dev_num ...................... 1000
  dev_ratio .................... 1
  gen_num ...................... -1
  data_names ................... None
  prompt_type .................. None
  num_workers .................. 0
  max_prompt_length ............ 128
  min_prompt_length ............ 64
  json_data .................... False
  bin_data ..................... False
  txt_data ..................... False
  prompt_data_dir .............. /dtu/p1/johlau/LMOps/minillm/processed_data/dolly_en/prompt/bloom/
  lm_data_dir .................. /dtu/p1/johlau/LMOps/minillm/processed_data/roberta_en/bloom/256/20M/
  eval_ppl ..................... False
  eval_rw ...................... False
  eval_gen ..................... False
  only_prompt .................. False
  batch_size ................... 2
  eval_batch_size .............. 4
  clip_grad .................... 1.0
  total_iters .................. 5000
  train_iters_per_epoch ........ -1
  max_length ................... 256
  seed ......................... 10
  seed_order ................... 42
  seed_data .................... 42
  seed_ppo ..................... 42
  seed_lm ...................... 7
  epochs ....................... 10
  training_epochs .............. 10000
  gradient_accumulation_steps .. 2
  gradient_checkpointing ....... True
  attn_dtype ................... None
  lr ........................... 5e-06
  lr_min ....................... 5e-06
  weight_decay ................. 0.01
  loss_scale ................... 65536
  kd_ratio ..................... 0.5
  warmup_iters ................. 100
  lr_decay_iters ............... None
  lr_decay_style ............... noam
  scheduler_name ............... cosine_trm
  reward_scaling ............... 0.5
  cliprange_reward ............. 100.0
  ppo_epochs ................... 4
  num_rollouts ................. 32
  num_rollouts_per_device ...... 32
  cliprange .................... 0.2
  chunk_size ................... 2
  gamma ........................ 0.95
  length_norm .................. True
  single_step_reg .............. True
  teacher_mixed_alpha .......... 0.2
  lm_coef ...................... 1
  top_k ........................ 0
  top_p ........................ 1.0
  do_sample .................... True
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  num_beams .................... 1
  temperature .................. 1.0
  peft ......................... None
  peft_lora_r .................. 8
  peft_lora_alpha .............. 32
  peft_lora_dropout ............ 0.1
  peft_name .................... None
  peft_path .................... None
  teacher_peft_name ............ None
  teacher_peft_path ............ None
  deepspeed .................... True
  deepspeed_config ............. /dtu/p1/johlau/LMOps/minillm/configs/deepspeed/ds_config_zero2_offload.json
  deepscale .................... False
  deepscale_config ............. None
  deepspeed_mpi ................ False
  rank ......................... 0
  world_size ................... 1
